<!DOCTYPE html>
<html lang="en">
  <head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <meta charset="UTF-8">
    
    <meta http-equiv="refresh" content="0; url=https://arxiv.org/abs/2411.02124">
    
    <title>Adaptive Sparse Allocation with Mutual Choice &amp; Feature Choice Sparse Autoencoders</title>
  </head>
  <body>
    
    <h1>
      <a href="https://arxiv.org/abs/2411.02124" target="_blank" rel="noopener noreferrer">
        Adaptive Sparse Allocation with Mutual Choice &amp; Feature Choice Sparse Autoencoders
      </a>
    </h1>
    
    <p>
      Kola Ayonrinde (2024)<br>
      Under Review
    </p>
    <div>
<p>Sparse autoencoders (SAEs) are a promising approach to extracting features from neural networks, enabling model interpretability as well as causal interventions on model internals. SAEs generate sparse feature representations using a sparsifying activation function that implicitly defines a set of token-feature matches. We frame the token-feature matching as a resource allocation problem constrained by a total sparsity upper bound. For example, TopK SAEs solve this allocation problem with the additional constraint that each token matches with at most k features. In TopK SAEs, the k active features per token constraint is the same across tokens, despite some tokens being more difficult to reconstruct than others. To address this limitation, we propose two novel SAE variants, Feature Choice SAEs and Mutual Choice SAEs, which each allow for a variable number of active features per token. Feature Choice SAEs solve the sparsity allocation problem under the additional constraint that each feature matches with at most m tokens. Mutual Choice SAEs solve the unrestricted allocation problem where the total sparsity budget can be allocated freely between tokens and features. Additionally, we introduce a new auxiliary loss function, ğšŠğšğš¡_ğš£ğš’ğš™ğš_ğš•ğš˜ğšœğšœ, which generalises the ğšŠğšğš¡_ğš”_ğš•ğš˜ğšœğšœ to mitigate dead and underutilised features. Our methods result in SAEs with fewer dead features and improved reconstruction loss at equivalent sparsity levels as a result of the inherent adaptive computation. More accurate and scalable feature extraction methods provide a path towards better understanding and more precise control of foundation models.</p>
</div>
  </body>
</html>
