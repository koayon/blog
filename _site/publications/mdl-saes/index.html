<!DOCTYPE html>
<html lang="en">
  <head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <meta charset="UTF-8">
    
    <meta http-equiv="refresh" content="0; url=https://arxiv.org/abs/2410.11179">
    
    <title>Interpretability as Compression: Reconsidering SAE Explanations of Neural Activations with MDL-SAEs</title>
  </head>
  <body>
    
    <h1>
      <a href="https://arxiv.org/abs/2410.11179" target="_blank" rel="noopener noreferrer">
        Interpretability as Compression: Reconsidering SAE Explanations of Neural Activations with MDL-SAEs
      </a>
    </h1>
    
    <p>
      *Kola Ayonrinde, *Michael Pearce, Lee Sharkey (2024)<br>
      üåü NeurIPS 2024, Oral at InterpretableAI Workshop
    </p>
    <div>
<p>Sparse Autoencoders (SAEs) have emerged as a useful tool for interpreting the internal representations of neural networks. However, naively optimising SAEs for reconstruction loss and sparsity results in a preference for SAEs that are extremely wide and sparse. We present an information-theoretic framework for interpreting SAEs as lossy compression algorithms for communicating explanations of neural activations. We appeal to the Minimal Description Length (MDL) principle to motivate explanations of activations which are both accurate and concise. We further argue that interpretable SAEs require an additional property, ‚Äúindependent additivity‚Äù: features should be able to be understood separately. We demonstrate an example of applying our MDL-inspired framework by training SAEs on MNIST handwritten digits and find that SAE features representing significant line segments are optimal, as opposed to SAEs with features for memorised digits from the dataset or small digit fragments. We argue that using MDL rather than sparsity may avoid potential pitfalls with naively maximising sparsity such as undesirable feature splitting and that this framework naturally suggests new hierarchical SAE architectures which provide more concise explanations.</p>
</div>
  </body>
</html>
