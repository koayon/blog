<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.9.3">Jekyll</generator><link href="http://localhost:4000/blog/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/blog/" rel="alternate" type="text/html" /><updated>2023-10-23T01:33:12-04:00</updated><id>http://localhost:4000/blog/feed.xml</id><title type="html">Kola Ayonrinde</title><subtitle>The technical blog of Kola Ayonrinde: ML Engineer/Data Scientist</subtitle><entry><title type="html">An Analogy for Understanding Mixture of Expert Models</title><link href="http://localhost:4000/blog/2023/10/22/moe-analogy.html" rel="alternate" type="text/html" title="An Analogy for Understanding Mixture of Expert Models" /><published>2023-10-22T00:00:00-04:00</published><updated>2023-10-22T00:00:00-04:00</updated><id>http://localhost:4000/blog/2023/10/22/moe-analogy</id><content type="html" xml:base="http://localhost:4000/blog/2023/10/22/moe-analogy.html">&lt;!-- An Intuitive Way To Understand Mixture Of Expert Models (Sparse MoEs) --&gt;

&lt;h4 id=&quot;tldr-experts-are-doctors-routers-are-gps&quot;&gt;TL;DR: Experts are Doctors, Routers are GPs&lt;/h4&gt;

&lt;h2 id=&quot;motivation&quot;&gt;Motivation&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Foundation_models&quot;&gt;Foundation models&lt;/a&gt; aim to
solve a wide range of tasks. In the days of yore, we would build a supervised
model for every individual use case; foundation models promise a single unified
solution.&lt;/p&gt;

&lt;p&gt;There are challenges with this however. When two tasks need different skills,
trying to learn both can make you learn neither as well as if you had focused on
one&lt;sup id=&quot;fnref:neg&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:neg&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt;. Storing information for many tasks can also be a challenge, even for
large models.&lt;/p&gt;

&lt;p&gt;Moreover we might wonder if it make sense to use the same parameters for
computing the answer to a logic puzzle and for finding the perfect adjective to
describe the love interest in a romance fanfic.&lt;/p&gt;

&lt;p&gt;We would like our models to have modular functions. We could then select and
even combine abilities when needed.&lt;/p&gt;

&lt;h2 id=&quot;moes-for-scale&quot;&gt;MoEs For Scale&lt;/h2&gt;

&lt;p&gt;Scaling up models offers various advantages. There are three main quantities to
scale: the number of model &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;parameters&lt;/code&gt;, the amount of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;data&lt;/code&gt; and the amount of
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;compute&lt;/code&gt; applied at train time. With regular transformers, to scale up the
number of parameters, we must likewise scale the amount of compute applied.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Intuitively more parameters mean more &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;knowledge&lt;/code&gt;, and more compute represents
additional &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;intelligence&lt;/code&gt; &lt;sup id=&quot;fnref:1&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:1&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;2&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;There are some use cases where having more knowledge can be traded off with
being more cognitively able. For example, you may choose to memorise rather than
re-derive the laws of physics to use them in a specific problem. Similarly we
can trade off the opposite way as well - if you know you’ll have access to a
textbook or Wikipedia then you might not want to memorise certain historical
facts. All you need to know is when and how to look up the facts you need.&lt;/p&gt;

&lt;p&gt;So, dependent on whether we need more knowledge or more cognitive ability, we
also want to scale parameters and compute separately&lt;sup id=&quot;fnref:2&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:2&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;3&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;

&lt;h2 id=&quot;sparse-mixture-of-experts-models&quot;&gt;Sparse Mixture of Experts Models&lt;/h2&gt;

&lt;p&gt;In a vanilla transformer, each Transformer Block contains an attention layer for
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;communication&lt;/code&gt; between tokens and an MLP layer for &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;computation&lt;/code&gt; within
tokens. The MLP layer contains most of the parameters of a large transformer and
transforms the individual tokens.&lt;/p&gt;

&lt;p&gt;In &lt;a href=&quot;https://arxiv.org/pdf/2101.03961.pdf&quot;&gt;Sparse Mixture of Experts&lt;/a&gt; (MoEs), we
swap out the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;MLP layers&lt;/code&gt; of the vanilla transformer for an &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Expert Layer&lt;/code&gt;. The
Expert Layer is made up of multiple MLPs called “Experts”. For each input we
select one expert to send that input to. In this way, each token it has
different parameters applied to it. A dynamic routing mechanism decides how to
map tokens to Experts&lt;sup id=&quot;fnref:3&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:3&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;4&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;
  &lt;figure&gt;
    &lt;img src=&quot;/blog/images/softmoe/moe.png&quot; width=&quot;800&quot; alt=&quot;Sparse MoE&quot; /&gt;
    &lt;figcaption&gt;Sparse Expert Layer (Switch Transformer) &lt;/figcaption&gt;
    &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;Sparse MoEs solve the problems we noted earlier:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;MoEs allow their internal “Experts” to specialise in certain domains rather
than having to be all things to all tokens &lt;sup id=&quot;fnref:0&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:0&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;5&lt;/a&gt;&lt;/sup&gt; &lt;sup id=&quot;fnref:m&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:m&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;6&lt;/a&gt;&lt;/sup&gt;.&lt;/li&gt;
  &lt;li&gt;And with MoEs, we are able to increase the number of parameters of models
without increasing how much training compute or inference time latency. This
decouples parameter scaling from compute scaling (i.e. we decouple knowledge
from intelligence)&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;the-analogy&quot;&gt;The Analogy&lt;/h2&gt;

&lt;p&gt;Imagine you’re feeling fatigued and you have no idea what’s causing this.
Suppose the problem is with your eyes but you don’t know this yet. Since your
friend is a cardiologist (doctor specialising in the heart), you ask them for
advice, which they freely give. You might ask yourself if you should follow
their advice blindly or if you should:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Approach 1: Get a second opinion from another cardiologist.&lt;/strong&gt;&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;
  &lt;figure&gt;
    &lt;img src=&quot;/blog/images/analogy-moe/two_cardiologists.png&quot; width=&quot;600&quot; alt=&quot;Two Cardiologists&quot; /&gt;
    &lt;!-- &lt;figcaption&gt;Sparse Expert Layer (Switch Transformer) &lt;/figcaption&gt; --&gt;
    &lt;/figure&gt;
&lt;/div&gt;

&lt;blockquote&gt;
  &lt;p&gt;Averaging over multiple doctors who were trained in the same way increases
robustness by reducing variance (maybe the first doctor was tired that day or
something). But it doesn’t help with bias &lt;sup id=&quot;fnref:stat&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:stat&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;7&lt;/a&gt;&lt;/sup&gt; - all the cardiologists are
likely to be wrong in the same way, if they are wrong at all.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Approach 2: Go to a generalist doctor that has no specialism.&lt;/strong&gt;&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;
  &lt;figure&gt;
    &lt;img src=&quot;/blog/images/analogy-moe/no_specialist.png&quot; width=&quot;600&quot; alt=&quot;One cardiologist and one doctor with no specialism&quot; /&gt;
    &lt;!-- &lt;figcaption&gt;Sparse Expert Layer (Switch Transformer) &lt;/figcaption&gt; --&gt;
    &lt;/figure&gt;
&lt;/div&gt;

&lt;blockquote&gt;
  &lt;p&gt;It’s not clear whether this is better than asking another cardiologist. Sure
they might have different knowledge to the cardiologist which might be useful
if your problem isn’t about the heart. But there’s an awful lot of medical
knowledge out there and we can’t reasonably expect this one generalist to know
everything about all of them. They probably have cursory knowledge at best. We
need someone who specialises in the area that we’re struggling with. Problem
is we don’t know which area of specialism we need!&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Approach 3: Ask multiple doctors who all specialise in different areas and do
the thing most of them suggest.&lt;/strong&gt;&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;
  &lt;figure&gt;
    &lt;img src=&quot;/blog/images/analogy-moe/all_doctors.png&quot; width=&quot;600&quot; alt=&quot;Multiple Doctors with Different Specialisms&quot; /&gt;
    &lt;!-- &lt;figcaption&gt;Sparse Expert Layer (Switch Transformer) &lt;/figcaption&gt; --&gt;
    &lt;/figure&gt;
&lt;/div&gt;

&lt;blockquote&gt;
  &lt;p&gt;This is much better. If you have a problem with your eyes, you know that the
eye doctor is being consulted so you have a much better chance of getting the
right treatment. But there are downsides here. Most notably, asking multiple
doctors is probably pretty inefficient. Now we have to see 50 specialists for
every problem even though most of them have no idea about our problem. What we
would prefer is to know which one specialist (or possibly couple of
specialists) we should see and only get advice from them.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Approach 4: Go to your GP, tell them about your ailment and ask them which
doctor you should go and see.&lt;/strong&gt;&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;
  &lt;figure&gt;
    &lt;img src=&quot;/blog/images/analogy-moe/gp.png&quot; width=&quot;600&quot; alt=&quot;GP-Doctor System&quot; /&gt;
    &lt;!-- &lt;figcaption&gt;Sparse Expert Layer (Switch Transformer) &lt;/figcaption&gt; --&gt;
    &lt;/figure&gt;
&lt;/div&gt;

&lt;blockquote&gt;
  &lt;p&gt;Here we get the benefits of getting advice from the most relevant specialised
doctor without having to ask every other doctor. This is both more accurate
and time-efficient.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;In approach 4, the GP is the routing function. They know the strengths of the
different doctors and send you to one of them depending on your problem.&lt;/p&gt;

&lt;p&gt;The Doctors are the Experts. We allow them to specialise knowing that the GP can
route us to the correct doctor for our problem.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;The GP-doctor system is exactly a Mixture of Experts layer.&lt;/strong&gt;&lt;/p&gt;

&lt;h3 id=&quot;what-are-moes-good-for&quot;&gt;What Are MoEs Good For?&lt;/h3&gt;

&lt;p&gt;Viewed this way we see that Mixture of Expert models will be effective whenever
we want a model to have access to large amounts of information - more than a
single Expert could hope to learn alone. Another use case is when our task can
be decomposed into one of a number of tasks.&lt;/p&gt;

&lt;p&gt;In general we might imagine MoEs which when faced with more difficult problems
can send the input to a more powerful expert which has access to more resources.
This starts to move us increasingly towards
&lt;a href=&quot;https://github.com/koayon/awesome-adaptive-computation&quot;&gt;Adaptive Computation&lt;/a&gt;.&lt;/p&gt;

&lt;div class=&quot;footnotes&quot; role=&quot;doc-endnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:neg&quot; role=&quot;doc-endnote&quot;&gt;

      &lt;p&gt;This phenomena is known as negative interference in learning. Jack of All
Trades, Master of None. For other tasks we can see positive interference
however, also known as Transfer Learning. &lt;a href=&quot;#fnref:neg&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:1&quot; role=&quot;doc-endnote&quot;&gt;

      &lt;p&gt;For some vague definitions of “intelligence” and “knowledge”. This intuition
is courtesy of
&lt;a href=&quot;https://scholar.google.com/citations?user=wsGvgA8AAAAJ&amp;amp;hl=en&quot;&gt;Noam Shazeer&lt;/a&gt;. &lt;a href=&quot;#fnref:1&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:2&quot; role=&quot;doc-endnote&quot;&gt;

      &lt;p&gt;In reality both knowledge and cognitive ability are hard to separate this
cleanly but hopefully the intuition still remains useful. &lt;a href=&quot;#fnref:2&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:3&quot; role=&quot;doc-endnote&quot;&gt;

      &lt;p&gt;The experts “compete” to process the tokens and as in Natural Selection and
Economics, competition for niches makes them specialise. &lt;a href=&quot;#fnref:3&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:0&quot; role=&quot;doc-endnote&quot;&gt;

      &lt;p&gt;In actuality Expert might not necessarily specialise strictly by task. It
might be beneficial for an expert to specialise in syntactic rather than
semantic features or to combine two tasks which are different enough to not
inference with each other. &lt;a href=&quot;#fnref:0&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:m&quot; role=&quot;doc-endnote&quot;&gt;

      &lt;p&gt;This approach also has good biological precedent. Humans don’t use every
part of their brain for every stimulus they receive - when they receive, for
example a visual stimuli, they use only their visual cortex to process it. &lt;a href=&quot;#fnref:m&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:stat&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;In the statistical sense &lt;a href=&quot;#fnref:stat&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;</content><author><name></name></author><category term="machine-learning" /><category term="mixture-of-experts" /><category term="adaptive-computation" /><summary type="html"></summary></entry><entry><title type="html">From Sparse To Soft Mixtures of Experts</title><link href="http://localhost:4000/blog/2023/10/20/soft-moe.html" rel="alternate" type="text/html" title="From Sparse To Soft Mixtures of Experts" /><published>2023-10-20T00:00:00-04:00</published><updated>2023-10-20T00:00:00-04:00</updated><id>http://localhost:4000/blog/2023/10/20/soft-moe</id><content type="html" xml:base="http://localhost:4000/blog/2023/10/20/soft-moe.html">&lt;p&gt;Mixture of Expert (MoE) models have recently emerged as an ML architecture
offering efficient scaling and practicality in both training and inference &lt;sup id=&quot;fnref:1&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:1&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;

&lt;h3 id=&quot;what-are-sparse-moes&quot;&gt;What Are Sparse MoEs?&lt;/h3&gt;

&lt;p&gt;In traditional &lt;a href=&quot;https://arxiv.org/pdf/2101.03961.pdf&quot;&gt;Sparse MoEs&lt;/a&gt;, we swap out
the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;MLP layers&lt;/code&gt; of the vanilla transformer for an &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Expert Layer&lt;/code&gt;. The Expert
Layer is made up of multiple MLPs referred to as Experts. For each input one
expert is selected to send that input to. A dynamic routing mechanism decides
how to map tokens to Experts. Importantly, though this is less mentioned, MoEs
are more modular and hence more naturally interpretable than vanilla
transformers.&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;
  &lt;figure&gt;
    &lt;img src=&quot;/blog/images/softmoe/moe.png&quot; width=&quot;800&quot; alt=&quot;Sparse MoE&quot; /&gt;
    &lt;figcaption&gt;Sparse Expert Layer (Switch Transformer) &lt;/figcaption&gt;
    &lt;/figure&gt;
&lt;/div&gt;

&lt;h3 id=&quot;introducing-soft-moes&quot;&gt;Introducing Soft MoEs&lt;/h3&gt;

&lt;p&gt;The Soft MoE paradigm was introduced by Google researchers in the paper
&lt;a href=&quot;https://arxiv.org/pdf/2308.00951.pdf&quot;&gt;From Sparse To Soft Mixtures of Experts&lt;/a&gt;.
Unlike Sparse MoEs, Soft MoEs don’t send a &lt;em&gt;subset&lt;/em&gt; of the input tokens to
experts. Instead, each expert receives a &lt;em&gt;linear combination&lt;/em&gt; of all the input
tokens. The weights for these combinations are determined by the same dynamic
routing mechanism as in Sparse MoEs.&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;
  &lt;figure&gt;
    &lt;img src=&quot;/blog/images/softmoe/duck.png&quot; width=&quot;500&quot; alt=&quot;Soft MoE&quot; /&gt;
    &lt;figcaption&gt;In Soft MoEs each expert processes linear combinations of image patches. &lt;/figcaption&gt;
    &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;The discrete routing that makes Sparse MoEs so effective also makes them not
inherently fully differentiable and can cause training issues. The Soft MoE
approach solves these issues, are better suited to GPU hardware and in general
outperform Sparse MoEs.&lt;/p&gt;

&lt;p&gt;The paper abstract reads:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Sparse mixture of expert architectures (MoEs) scale model capacity without
large increases in training or inference costs. Despite their success, MoEs
suffer from a number of issues: training instability, token dropping,
inability to scale the number of experts, or ineffective finetuning. In this
work, we propose Soft MoE, a fully-differentiable sparse Transformer that
addresses these challenges, while maintaining the benefits of MoEs. Soft MoE
performs an implicit soft assignment by passing different weighted
combinations of all input tokens to each expert. As in other MoE works,
experts in Soft MoE only process a subset of the (combined) tokens, enabling
larger model capacity at lower inference cost. In the context of visual
recognition, Soft MoE greatly outperforms standard Transformers (ViTs) and
popular MoE variants (Tokens Choice and Experts Choice). For example, Soft
MoE-Base/16 requires 10.5× lower inference cost (5.7× lower wall-clock time)
than ViT-Huge/14 while matching its performance after similar training. Soft
MoE also scales well: Soft MoE Huge/14 with 128 experts in 16 MoE layers has
over 40× more parameters than ViT Huge/14, while inference time cost grows by
only 2%, and it performs substantially better.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h3 id=&quot;links-to-talk-and-slides&quot;&gt;Links to Talk and Slides&lt;/h3&gt;

&lt;p&gt;I recently gave a talk at &lt;a href=&quot;https://www.eleuther.ai&quot;&gt;EleutherAI&lt;/a&gt;, the open-source
AI research lab, about Soft MoEs.&lt;/p&gt;

&lt;p&gt;You can watch the talk back on YouTube
&lt;a href=&quot;https://youtu.be/xCKdBC5dh_g?si=uDH8vLVII7l_X8_L&quot;&gt;here&lt;/a&gt; &lt;sup id=&quot;fnref:2&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:2&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;2&lt;/a&gt;&lt;/sup&gt; or view the slides
&lt;a href=&quot;https://docs.google.com/presentation/d/12Sw4wRQJr3sxcJR91_UM_dlYgYxeAbf9t8es54bAYUM/edit#slide=id.p&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;I’m very excited about research ideas working on expanding the SoftMoE paradigm
to autoregressive (GPT-style) models, which is currently an open problem
described in the above talk. Feel free to reach out if you’re interested in or
are currently researching in this area. &lt;br /&gt; &lt;br /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;div class=&quot;footnotes&quot; role=&quot;doc-endnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:1&quot; role=&quot;doc-endnote&quot;&gt;

      &lt;p&gt;For more details on MoE models see the
&lt;a href=&quot;https://github.com/koayon/awesome-adaptive-computation&quot;&gt;Awesome Adaptive Computation&lt;/a&gt;
repo. &lt;a href=&quot;#fnref:1&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:2&quot; role=&quot;doc-endnote&quot;&gt;

      &lt;p&gt;Unfortunately the video’s audio quality isn’t as great as it could be, I may
look at cleaning this up. &lt;a href=&quot;#fnref:2&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;</content><author><name></name></author><category term="machine-learning" /><category term="mixture-of-experts" /><category term="adaptive-computation" /><summary type="html">Mixture of Expert (MoE) models have recently emerged as an ML architecture offering efficient scaling and practicality in both training and inference [^1].</summary></entry><entry><title type="html">DeepSpeed’s Bag of Tricks for Speed &amp;amp; Scale</title><link href="http://localhost:4000/blog/2023/07/14/deepspeed-train.html" rel="alternate" type="text/html" title="DeepSpeed’s Bag of Tricks for Speed &amp;amp; Scale" /><published>2023-07-14T00:00:00-04:00</published><updated>2023-07-14T00:00:00-04:00</updated><id>http://localhost:4000/blog/2023/07/14/deepspeed-train</id><content type="html" xml:base="http://localhost:4000/blog/2023/07/14/deepspeed-train.html">&lt;!-- # DeepSpeed&apos;s Bag of Tricks for Speed &amp; Scale --&gt;

&lt;h2 id=&quot;an-introduction-to-deepspeed-for-training&quot;&gt;An Introduction to DeepSpeed for Training&lt;/h2&gt;

&lt;p&gt;In the literature and the public conversation around Natural Language
Processing, lots has been made of the results of scaling up data, compute and
model size. For example we have the &lt;a href=&quot;https://arxiv.org/abs/2001.08361&quot;&gt;original&lt;/a&gt;
and &lt;a href=&quot;https://arxiv.org/abs/2203.15556&quot;&gt;updated&lt;/a&gt; transformer scaling laws.&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;
  &lt;figure&gt;
    &lt;img src=&quot;/blog/images/deepspeed/stack_more_layers.webp&quot; width=&quot;500&quot; alt=&quot;Layers&quot; /&gt;
    &lt;figcaption&gt;Keep it stacking&lt;/figcaption&gt;
    &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;One sometimes overlooked point is the vital role of engineering breakthroughs in
enabling large models to be trained and served on current hardware.&lt;/p&gt;

&lt;p&gt;This post is about the engineering tricks that bring the research to life.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;em&gt;Note: This post assumes some basic familiarity with PyTorch/Tensorflow and
transformers. If you’ve never used these before check out the
&lt;a href=&quot;https://pytorch.org/docs/stable/index.html&quot;&gt;PyTorch docs&lt;/a&gt; and the
&lt;a href=&quot;https://jalammar.github.io/illustrated-transformer/&quot;&gt;Illustrated Transformer&lt;/a&gt;.
Some background on backpropagation works will also be useful - check out
&lt;a href=&quot;https://www.youtube.com/watch?v=Ilg3gGewQ5U&quot;&gt;this video&lt;/a&gt; if you want a
refresher!&lt;/em&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;!-- {:toc}  --&gt;

&lt;hr /&gt;

&lt;details&gt;
  &lt;summary&gt;Table of Contents&lt;/summary&gt;
  &lt;ul&gt;
    &lt;li&gt;&lt;a href=&quot;#&quot;&gt;0. Introduction&lt;/a&gt;
      &lt;ul&gt;
        &lt;li&gt;&lt;a href=&quot;#01-deepspeed-s-three-innovation-pillars&quot;&gt;0.1 DeepSpeed&apos;s Three Innovation Pillars&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&quot;#02-problems-training-large-models&quot;&gt;0.2 Problems Training Large Models&lt;/a&gt;&lt;/li&gt;
      &lt;/ul&gt;
    &lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;#1-partial-solutions&quot;&gt;1. Partial Solutions&lt;/a&gt;
      &lt;ul&gt;
        &lt;li&gt;&lt;a href=&quot;#11-naive-data-parallelism&quot;&gt;1.1 Naive Data Parallelism&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&quot;#12-naive-model-parallelism&quot;&gt;1.2 Naive Model Parallelism&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&quot;#13-a-better-way--deepspeed&quot;&gt;1.3 A Better Way: DeepSpeed&lt;/a&gt;&lt;/li&gt;
      &lt;/ul&gt;
    &lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;#2-deepspeed-deep-dive--key-ideas&quot;&gt;2. DeepSpeed Deep Dive: Key Ideas&lt;/a&gt;
      &lt;ul&gt;
        &lt;li&gt;&lt;a href=&quot;#20-mixed-precision-training&quot;&gt;2.0 Mixed Precision Training&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&quot;#21-delaying-weight-updates&quot;&gt;2.1 Delaying Weight Updates&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&quot;#22-storing-optimiser-states-without-redundancy--zero-stage-1-&quot;&gt;2.2 Storing Optimiser States Without Redundancy (ZeRO stage 1)&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&quot;#23-storing-gradients-and-parameters-without-redundancy--zero-stages-2---3-&quot;&gt;2.3 Storing Gradients and Parameters Without Redundancy (ZeRO stages 2 &amp;amp; 3)&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&quot;#24-tensor-slicing&quot;&gt;2.4 Tensor Slicing&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&quot;#25-gradient-checkpointing&quot;&gt;2.5 Gradient Checkpointing&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&quot;#26-profiling-etc&quot;&gt;2.6 Profiling etc&lt;/a&gt;&lt;/li&gt;
      &lt;/ul&gt;
    &lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;#3-in-pictures&quot;&gt;3. In Pictures&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;#4-in-code&quot;&gt;4. In Code&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;#5-using-deepspeed&quot;&gt;5. Using DeepSpeed&lt;/a&gt;&lt;/li&gt;
  &lt;/ul&gt;
&lt;/details&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;01-deepspeeds-three-innovation-pillars&quot;&gt;0.1 DeepSpeed’s Three Innovation Pillars&lt;/h3&gt;

&lt;p&gt;&lt;a href=&quot;https://www.deepspeed.ai&quot;&gt;DeepSpeed&lt;/a&gt; has four main use cases: enabling large
training runs, decreasing inference latency, model compression and enabling ML
science.&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;
  &lt;figure&gt;
  &lt;img src=&quot;/blog/images/deepspeed/deepspeed-pillars.png&quot; width=&quot;700&quot; alt=&quot;&quot; /&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;This post covers training optimizations.&lt;/p&gt;

&lt;h3 id=&quot;02-problems-training-large-models&quot;&gt;0.2 Problems Training Large Models&lt;/h3&gt;

&lt;p&gt;Training large models (e.g. LLMs) on huge datasets can be can be prohibitively
slow, expensive, or even impossible with available hardware.&lt;/p&gt;

&lt;p&gt;In particular, very large models generally do not fit into the memory of a
single GPU/TPU node. Compared to CPUs, GPUs are generally higher throughput but
lower memory capacity. (A typical GPU may have 32GB memory versus 1TB+ for
CPUs).&lt;/p&gt;

&lt;p&gt;Our aims are:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;To train models too large for a single device&lt;/li&gt;
  &lt;li&gt;Efficiently distribute computation across devices&lt;/li&gt;
  &lt;li&gt;Fully utilize all devices as much as possible&lt;/li&gt;
  &lt;li&gt;Minimize communication bottlenecks &lt;em&gt;between&lt;/em&gt; devices&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;DeepSpeed reduces compute and time to train by &amp;gt;100x for large models.&lt;/p&gt;

&lt;p&gt;If you just want to see how to implement DeepSpeed in your code, see the
&lt;a href=&quot;#5-using-deepspeed&quot;&gt;Using DeepSpeed&lt;/a&gt; section below.&lt;/p&gt;

&lt;h2 id=&quot;1-partial-solutions&quot;&gt;1. Partial Solutions&lt;/h2&gt;

&lt;h3 id=&quot;11-naive-data-parallelism&quot;&gt;1.1 Naive Data Parallelism&lt;/h3&gt;

&lt;p&gt;Without any data parallelism, we get this sorry sight:&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;
  &lt;figure&gt;
  &lt;img src=&quot;/blog/images/deepspeed/gpu_unused.png&quot; width=&quot;700&quot; alt=&quot;Unused GPU potential&quot; /&gt;
  &lt;figcaption&gt;Oh dear&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;We’ve spent a lot of money on GPU cores for them all to sit there idle apart
from one! Unless you’re single-handedly trying to prop up the NVIDIA share
price, this is a terrible idea!&lt;/p&gt;

&lt;p&gt;One thing that we might try is splitting up the data, parallelising across
devices. Here we copy the entire model onto each worker, each of which process
different subsets of the training dataset.&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;
  &lt;figure&gt;
  &lt;img src=&quot;/blog/images/deepspeed/data_parallel.png&quot; width=&quot;700&quot; alt=&quot;Data Parallelisation&quot; /&gt;
  &lt;figcaption&gt;Data Parallelisation&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;Each device compute its own gradients and then we average out the gradients
across all the nodes to update our parameters with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;all_reduce&lt;/code&gt;. This approach
is pretty straightforward to implement and works for any model type.&lt;/p&gt;

&lt;p&gt;We’ve turned more GPUs into more speed - great!&lt;/p&gt;

&lt;p&gt;In addition we also increase effective batch size, reducing costly parameter
updates. Since with larger batch sizes there is more signal in each gradient
update, this also improves convergence (up to a point).&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;
  &lt;figure&gt;
  &lt;img src=&quot;/blog/images/deepspeed/whats_the_catch.gif&quot; alt=&quot;What&apos;s The Catch&quot; /&gt;
  &lt;figcaption&gt;I thought you&apos;d never ask&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;Unfortunately the memory bottleneck still remains. For Data Parallelism to work,
the entire model has to fit on every device, which just isn’t going to happen
for large models.&lt;/p&gt;

&lt;h3 id=&quot;12-naive-model-parallelism&quot;&gt;1.2 Naive Model Parallelism&lt;/h3&gt;

&lt;p&gt;Another thing we could try is splitting up the computation of the model itself,
putting different layers (transformer blocks) on different devices. With this
model parallelism approach we aren’t limited by the size of a memory of a single
GPU, but instead by all the GPUs that we have.&lt;/p&gt;

&lt;p&gt;However two problems remain. Firstly how to split up a model efficiently is very
dependant on the specific model architecture (for example the number of layers
and attention heads). And secondly communicating &lt;em&gt;between&lt;/em&gt; nodes now bottlenecks
training.&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;
  &lt;figure&gt;
  &lt;img src=&quot;/blog/images/deepspeed/model_parallel.png&quot; width=&quot;600&quot; alt=&quot;Model parallelisation&quot; /&gt;
  &lt;figcaption&gt;One batch moving through the parallelised model. In model parallelisation, one forward and backward pass requires all the devices, most of which are idle at any one time&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;Since each layer requires the input to the previous layer in each pass, workers
spend most of their time waiting. What a waste of GPU time! Here it looks like
the model takes the same amount of time as if we had a GPU to fit it on but it’s
even worse. The communication overhead of getting data between nodes makes it
even &lt;em&gt;slower&lt;/em&gt; than a single GPU.&lt;/p&gt;

&lt;p&gt;Can we do better than this?&lt;/p&gt;

&lt;h3 id=&quot;13-a-better-way-deepspeed&quot;&gt;1.3 A Better Way: DeepSpeed&lt;/h3&gt;

&lt;p&gt;Data Parallelism gave speedups but couldn’t handle models too large for a single
machine. Model Parallelism allowed us to train large models but it’s slow.&lt;/p&gt;

&lt;p&gt;We really want a marriage of the ideas of both data and model parallelism -
speed and scale together.&lt;/p&gt;

&lt;p&gt;We don’t always get what we want, but in this case we do. With DeepSpeed,
Microsoft packaged up a bag of tricks to allow ML engineers to train larger
models more efficiently. All in, DeepSpeed enables &amp;gt;100x lower training time and
cost with minimal code changes - just 4 changed lines of PyTorch code. Let’s
walk through how.&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;
  &lt;figure&gt;
  &lt;img src=&quot;/blog/images/deepspeed/dp_vs_mp.png&quot; width=&quot;700&quot; alt=&quot;DP vs MP&quot; /&gt;
  &lt;figcaption&gt;Data Parallelisation vs Model Parallelism&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h2 id=&quot;2-deepspeed-deep-dive-key-ideas&quot;&gt;2. DeepSpeed Deep Dive: Key Ideas&lt;/h2&gt;

&lt;p&gt;&lt;del&gt;One&lt;/del&gt; Seven Weird Tricks to Train Large Models:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Mixed precision training&lt;/li&gt;
  &lt;li&gt;Delaying Weight Updates&lt;/li&gt;
  &lt;li&gt;Storing the optimiser states without redundancy (ZeRO stage 1)&lt;/li&gt;
  &lt;li&gt;Storing gradients and parameters without redundancy (ZeRO stages 2 &amp;amp; 3)&lt;/li&gt;
  &lt;li&gt;Tensor Slicing&lt;/li&gt;
  &lt;li&gt;Gradient Checkpointing&lt;/li&gt;
  &lt;li&gt;Quality of Life Improvements and Profiling&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;20-mixed-precision-training&quot;&gt;2.0 Mixed Precision Training&lt;/h3&gt;

&lt;p&gt;Ordinarily mathematical operations are performed with 32 bit floats (fp32).
Using half precision (fp16) vs full precision (fp32) halves memory and speeds up
computation.&lt;/p&gt;

&lt;p&gt;We forward/backward pass in fp16 for speed, keeping copies of fp32 optimizer
states (momentum, first order gradient etc.) for accuracy. The high precision
fp32 maintains the high dynamic range so that we can still represent very slight
updates.&lt;/p&gt;

&lt;h3 id=&quot;21-delaying-weight-updates&quot;&gt;2.1 Delaying Weight Updates&lt;/h3&gt;

&lt;p&gt;A simple training loop might contain something like:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;batch&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;enumerate&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train_loader&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;

    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;j&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;minibatch&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;enumerate&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;batch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;

        &lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;minibatch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;local_gradients&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;gradients&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;batch_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;average_gradients&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;distributed&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;all_reduce&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;local_gradients&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# reduce INSIDE inner loop
&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;optimizer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;step&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;average_gradients&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Note here that within every loop we’re calculating not only the local gradients
but also synchronizing gradients which requires communicating with all the other
nodes.&lt;/p&gt;

&lt;p&gt;Delaying synchronization improves throughput e.g:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;batch&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;enumerate&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train_loader&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;

    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;j&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;minibatch&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;enumerate&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;batch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;

        &lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;minibatch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;local_gradients&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;gradients&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;batch_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

    &lt;span class=&quot;n&quot;&gt;average_gradients&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;distributed&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;all_reduce&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;local_gradients&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# reduce OUTSIDE inner loop
&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;optimizer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;step&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;average_gradients&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;22-storing-optimiser-states-without-redundancy-zero-stage-1&quot;&gt;2.2 Storing Optimiser States Without Redundancy (ZeRO stage 1)&lt;/h3&gt;

&lt;p&gt;Suppose we have a GPU with 50GB of memory and our model weights are 10GB of
memory. That’s all great right?&lt;/p&gt;

&lt;p&gt;For inference we feed in our input data and get out activations at each step.
Then once we pass each layer, we can throw away activations from prior layers.
Our model fits on the single GPU.&lt;/p&gt;

&lt;p&gt;For training however, it’s a different story. Each GPU needs its intermediate
activations, gradients and the fp32 optimiser states for backpropagation. Pretty
soon we’re overflowing the GPU with our model’s memory footprint 😞&lt;/p&gt;

&lt;p&gt;The biggest memory drain on our memory is the optimisation states.&lt;/p&gt;

&lt;p&gt;We know that we’re going to need to get multiple GPUs and do some model
parallelisation here. Eventually we want to partition the whole model but a good
first move would be to at least remove optimisation state redundancy.&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;
  &lt;figure&gt;
  &lt;img src=&quot;/blog/images/deepspeed/zero_stages.png&quot; width=&quot;800&quot; alt=&quot;The Stages of ZeRO&quot; /&gt;
  &lt;figcaption&gt;The Stages of Zero Redundancy Optimisation (ZeRO)&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;For ZeRO stage 1, in the backward pass, each device calculates the (first order)
gradients for the final section of the model. The final device &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;gathers&lt;/code&gt; all
these gradients, averages them and then computes the Adam optimised gradient
with the optimisation states. It then &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;broadcasts&lt;/code&gt; back the new parameter states
for the final section of the model to all devices. Then the penultimate device
will do the same and so on until we reach the first device.&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;
  &lt;figure&gt;
  &lt;img src=&quot;/blog/images/deepspeed/zero1-t1.gif&quot; width=&quot;800&quot; alt=&quot;ZeRO Stage 1&quot; /&gt;
  &lt;figcaption&gt;
    ZeRO Stage 1
&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;
&lt;p&gt;We can think of this as a 5 step process:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;All nodes calculate gradients from their loss (note they all did a forward
pass on different data so their losses will be different!)&lt;/li&gt;
  &lt;li&gt;Final node collects and averages the gradients from all nodes via &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;reduce&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;Final node calculates gradient update using optimiser states&lt;/li&gt;
  &lt;li&gt;Final node &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;broadcasts&lt;/code&gt; the new gradients to all of the nodes.&lt;/li&gt;
  &lt;li&gt;Repeat for penultimate section and so on to complete the gradient updates.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;ZeRO stage 1 typically reduces our memory footprint by ~4x.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;🔄 Fun Fact: The name DeepSpeed is a palindrome! How cute 🤗
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;23-storing-gradients-and-parameters-without-redundancy-zero-stages-2--3&quot;&gt;2.3 Storing Gradients and Parameters Without Redundancy (ZeRO stages 2 &amp;amp; 3)&lt;/h3&gt;

&lt;p&gt;We can take the partitioning idea further and do it for parameters and gradients
as well as optimisation states.&lt;/p&gt;

&lt;h4 id=&quot;in-the-forward-pass&quot;&gt;In the forward pass:&lt;/h4&gt;

&lt;div align=&quot;center&quot;&gt;
  &lt;figure&gt;
  &lt;img src=&quot;/blog/images/deepspeed/zero3_forward.gif&quot; width=&quot;800&quot; alt=&quot;ZeRO Stage 3 (Forward)&quot; /&gt;
  &lt;figcaption&gt;
    ZeRO Stage 3: forward pass
&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;
&lt;ol&gt;
  &lt;li&gt;The first node &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;broadcasts&lt;/code&gt; the parameters for the first section of the model.&lt;/li&gt;
  &lt;li&gt;All nodes complete the forward pass for their data for the first section of the model.&lt;/li&gt;
  &lt;li&gt;They then throw away the parameters for first section of the model.&lt;/li&gt;
  &lt;li&gt;Repeat for second section and so on to get the loss.&lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;and-the-backward-pass&quot;&gt;And the backward pass:&lt;/h4&gt;

&lt;div align=&quot;center&quot;&gt;
  &lt;figure&gt;
  &lt;img src=&quot;/blog/images/deepspeed/zero3_backward.gif&quot; width=&quot;800&quot; alt=&quot;ZeRO Stage 3 (Backward)&quot; /&gt;
  &lt;figcaption&gt;
    Zero Stage 3: backward pass
&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;
&lt;ol&gt;
  &lt;li&gt;The final node &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;broadcasts&lt;/code&gt; its section gradients.&lt;/li&gt;
  &lt;li&gt;Each backpropagate their own loss to get the next gradients.&lt;/li&gt;
  &lt;li&gt;As before, final node accumulates and averages all gradients (&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;reduce&lt;/code&gt;), calculates gradient update with optimiser and then &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;broadcasts&lt;/code&gt; the results, which can be used for the next section.&lt;/li&gt;
  &lt;li&gt;Once used, all gradients are thrown away by nodes which are not responsible for that section.&lt;/li&gt;
  &lt;li&gt;Repeat for penultimate section and so on to complete the gradient updates.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;If we have &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;N&lt;/code&gt; cores, we now have an &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;N&lt;/code&gt;x memory footprint reduction from ZeRO.&lt;/p&gt;

&lt;h4 id=&quot;a-breather&quot;&gt;A breather&lt;/h4&gt;

&lt;p&gt;That was the most complex part so feel free to check out these resources to make
sure you understand what’s going on:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=y4_bCiAsIAk&amp;amp;list=PLDEUW02OCkqGZ5_8jVQUK0dRJx8Um-hpc&amp;amp;index=1&amp;amp;t=20s&quot;&gt;DeepSpeed founder at MLOps community&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.microsoft.com/en-us/research/blog/ZeRO-deepspeed-new-system-optimizations-enable-training-models-with-over-100-billion-parameters/&quot;&gt;Microsoft blog post&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;It’s all downhill from here!&lt;/p&gt;

&lt;h4 id=&quot;benefits-of-zero&quot;&gt;Benefits of ZeRO&lt;/h4&gt;

&lt;p&gt;Overall, ZeRO removes the redundancy across data parallel process by
partitioning optimizer states, gradients and parameters across nodes. Look at
how much memory footprint we’ve saved!&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;
  &lt;figure&gt;
  &lt;img src=&quot;/blog/images/deepspeed/deepspeed_benefits.png&quot; width=&quot;800&quot; alt=&quot;DeepSpeed Benefits&quot; /&gt;
  &lt;figcaption&gt;Benefits of DeepSpeed&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;One surprising thing about this approach is that it scales superlinearly. That
is, when we double the number of GPUs that we’re using, we &lt;em&gt;more than&lt;/em&gt; double
the throughput of the system! In splitting up the model across more GPUs, we
leave more space per node for activations which allows for higher batch sizes.&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;
  &lt;figure&gt;
  &lt;img src=&quot;/blog/images/deepspeed/superlinear_scale.png&quot; alt=&quot;Superlinear Scale&quot; /&gt;
  &lt;figcaption&gt;Superlinear Scale of DeepSpeed vs Perfect Scaling&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h3 id=&quot;24-tensor-slicing&quot;&gt;2.4 Tensor Slicing&lt;/h3&gt;

&lt;p&gt;Most of the operations in a large ML model are matrix multiplications followed
by non-linearities. Matrix multiplication can be thought of as dot products
between pairs of matrix rows and columns. So we can compute independent dot
products on different GPUs and then combine the results afterwards.&lt;/p&gt;

&lt;p&gt;Another way to think about this is that if we want to parallelise matrix
multiplication across GPUs, we can slice up huge tensors into smaller ones and
then combine the results at the end.&lt;/p&gt;

&lt;p&gt;For matrices $$ X = \begin{bmatrix} X_1 &amp;amp; X_2 \end{bmatrix} $$ and $$ A =
\begin{bmatrix} A_1 \\ A_2 \end{bmatrix} $$, we note that:&lt;/p&gt;

&lt;p&gt;$$
XA = \begin{bmatrix} X_1 &amp;amp; X_2 \end{bmatrix} \begin{bmatrix} A_1 \\ A_2 \end{bmatrix}
$$&lt;/p&gt;

&lt;p&gt;For example:&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;
  &lt;figure&gt;
  &lt;img src=&quot;/blog/images/deepspeed/row_slicing_numbers.png&quot; width=&quot;700&quot; alt=&quot;Row Slicing&quot; /&gt;
  &lt;figcaption&gt;Row Slicing&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;However if there is a non-linear map after the M e.g. if $$ Y = \text{ReLU}(XA)
$$, this slicing isn’t going to work. $$ \text{ReLU}(X_1A_1 + X_2A_2) \neq
\text{ReLU}(X_1A_1) + \text{ReLU}(X_2A_2) $$ in general by non-linearity. So we
should instead split up X by columns and duplicate M across both nodes such that
we have:&lt;/p&gt;

&lt;p&gt;$$ Y = [Y_1, Y_2] = [\text{ReLU}(X A_1), \text{ReLU}(X A_2)] = XA $$&lt;/p&gt;

&lt;p&gt;For example:&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;
  &lt;figure&gt;
  &lt;img src=&quot;/blog/images/deepspeed/column_slicing_numbers.png&quot; width=&quot;700&quot; alt=&quot;Column Slicing&quot; /&gt;
  &lt;figcaption&gt;Column Slicing&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;Note: normally we think of A acting on X by left multiplication. In this case X
is our data and A is the weights which we want to parallelise. Through taking
transposes we can swap the order of the geometric interpretation so we can think
of the above as linear map A acting on our data X and still retain the slicing.&lt;/p&gt;

&lt;h3 id=&quot;25-gradient-checkpointing&quot;&gt;2.5 Gradient Checkpointing&lt;/h3&gt;

&lt;p&gt;In our description of ZeRO each core cached (held in memory) the activations for
it’s part of the model.&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;
  &lt;figure&gt;
  &lt;img src=&quot;https://github.com/cybertronai/gradient-checkpointing/raw/master/img/output.gif&quot; alt=&quot;Regular backprop&quot; /&gt;
  &lt;figcaption&gt;The top layer represents the activations in the model populating during the forward pass and the lower layer, the gradients populated in the backward pass. The first circle is the input data and the bottom right is the loss.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;Suppose we had extremely limited memory but were flush with compute. An
alternative approach to storing all the activations would be to simply recompute
them when we need in the backward pass. We can always recompute the activations
by running the same input data through a forward pass.&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;
  &lt;figure&gt;
  &lt;img src=&quot;https://github.com/cybertronai/gradient-checkpointing/raw/master/img/output_poor.gif&quot; alt=&quot;Memory poor backprop&quot; /&gt;
  &lt;figcaption&gt;Here each activation is computed just before it&apos;s needed using forward passes.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;This recomputing approach saves lots of memory but is quite compute wasteful,
incurring &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;m&lt;/code&gt; extra forward passes for an &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;m-layer&lt;/code&gt; transformer.&lt;/p&gt;

&lt;p&gt;A middle ground approach to trading off compute and memory is
&lt;a href=&quot;https://github.com/cybertronai/gradient-checkpointing&quot;&gt;gradient checkpointing&lt;/a&gt;
(sometimes known as activation checkpointing). Here we store some intermediate
activations with $$\sqrt m$$ of the memory for the cost of one forward pass.&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;
  &lt;figure&gt;
  &lt;img src=&quot;https://github.com/cybertronai/gradient-checkpointing/raw/master/img/output2.gif&quot; alt=&quot;Gradient Checkpointing&quot; /&gt;
  &lt;figcaption&gt;Here the only the second layer activations are cached as a &quot;checkpoint&quot;. Now for activations after the checkpoint instead of computing from the input data, we can compute from the checkpoint. This approach trades off memory and compute.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h3 id=&quot;26-profiling-etc&quot;&gt;2.6 Profiling etc&lt;/h3&gt;

&lt;p&gt;While not strictly causing any code optimisations, DeepSpeed provides developer
friendly features like convenient profiling and monitoring to track latency and
performance. We also have model checkpointing so you can recover a model from
different points in training. Developer happiness matters almost as much as
loss!&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;
  &lt;figure&gt;
  &lt;img src=&quot;https://i.imgflip.com/7s8ojc.jpg&quot; width=&quot;500&quot; alt=&quot;Happy&quot; /&gt;
  &lt;figcaption&gt;Happy engineers write happy code&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;Check out the &lt;a href=&quot;https://deepspeed.readthedocs.io/en/latest/&quot;&gt;docs&lt;/a&gt; for more info!&lt;/p&gt;

&lt;h2 id=&quot;3-in-pictures&quot;&gt;3. In Pictures&lt;/h2&gt;

&lt;video controls=&quot;&quot; width=&quot;700&quot;&gt;
  &lt;source src=&quot;/blog/images/deepspeed/Turing-Animation.mp4&quot; type=&quot;video/mp4&quot; /&gt;
&lt;/video&gt;

&lt;p&gt;&lt;em&gt;Animated Video from Microsoft: warning, it’s a little slow.&lt;/em&gt;&lt;/p&gt;

&lt;h2 id=&quot;4-in-code&quot;&gt;4. In Code&lt;/h2&gt;

&lt;p&gt;The full DeepSpeed library, with all the hardware level optimisations, is
open-sourced. See the &lt;a href=&quot;https://github.com/microsoft/DeepSpeed/&quot;&gt;core library&lt;/a&gt;,
the &lt;a href=&quot;https://www.deepspeed.ai/training/&quot;&gt;docs&lt;/a&gt; and
&lt;a href=&quot;https://github.com/microsoft/DeepSpeedExamples&quot;&gt;examples&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;For an annotated and easier to follow implementation see
&lt;a href=&quot;https://nn.labml.ai/scaling/zero3/index.html&quot;&gt;Lab ML’s version&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;5-using-deepspeed&quot;&gt;5. Using DeepSpeed&lt;/h2&gt;

&lt;p&gt;DeepSpeed integrates with PyTorch and TensorFlow to optimize training.&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;
  &lt;figure&gt;
  &lt;img src=&quot;/blog/images/deepspeed/stack.png&quot; width=&quot;600&quot; alt=&quot;Stack&quot; /&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;In PyTorch we only need to change 4 lines of code to apply DeepSpeed such that
our code is optimised for training on a single GPU machine, a single machine
with multiple GPUs, or on multiple machines in a distributed fashion.&lt;/p&gt;

&lt;p&gt;First we swap out:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;to&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;device&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;optimizer&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;optim&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Adam&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;parameters&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;lr&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;1e-4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;with initialising DeepSpeed by writing:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;ds_config&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
  &lt;span class=&quot;s&quot;&gt;&quot;train_micro_batch_size_per_gpu&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;batch_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
  &lt;span class=&quot;s&quot;&gt;&quot;optimizer&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
      &lt;span class=&quot;s&quot;&gt;&quot;type&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;Adam&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
      &lt;span class=&quot;s&quot;&gt;&quot;params&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
          &lt;span class=&quot;s&quot;&gt;&quot;lr&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;1e-4&lt;/span&gt;
      &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
  &lt;span class=&quot;p&quot;&gt;},&lt;/span&gt;
  &lt;span class=&quot;s&quot;&gt;&quot;fp16&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
      &lt;span class=&quot;s&quot;&gt;&quot;enabled&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;
  &lt;span class=&quot;p&quot;&gt;},&lt;/span&gt;
  &lt;span class=&quot;s&quot;&gt;&quot;zero_optimization&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
      &lt;span class=&quot;s&quot;&gt;&quot;stage&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
      &lt;span class=&quot;s&quot;&gt;&quot;offload_optimizer&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
         &lt;span class=&quot;s&quot;&gt;&quot;device&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;cpu&quot;&lt;/span&gt;
      &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
  &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;model_engine&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;initialize&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;model_architecture&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                       &lt;span class=&quot;n&quot;&gt;model_parameters&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;params&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                       &lt;span class=&quot;n&quot;&gt;config&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ds_config&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Then in our training loop we change out the original PyTorch…&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;step&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;batch&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;enumerate&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data_loader&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;# Calculate loss using model e.g.
&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;output&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;batch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;criterion&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;output&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;target&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

    &lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;backward&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;optimizer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;step&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;for:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;step&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;batch&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;enumerate&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data_loader&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;# Forward propagation method to get loss
&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;...&lt;/span&gt;

    &lt;span class=&quot;c1&quot;&gt;# Runs backpropagation
&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;model_engine&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;backward&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

    &lt;span class=&quot;c1&quot;&gt;# Weights update
&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;model_engine&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;step&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;That’s all it takes! In addition, DeepSpeed’s backend has also been integrated
with HuggingFace via the
&lt;a href=&quot;https://huggingface.co/docs/accelerate/index&quot;&gt;Accelerate library&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;thats-all-folks&quot;&gt;That’s All Folks!&lt;/h2&gt;

&lt;p&gt;There’s a lot of clever improvements that go into the special sauce for training
large models. And for users, with just a few simple code changes, DeepSpeed
works its magic to unleash the power of all your hardware for fast, efficient
model training.&lt;/p&gt;

&lt;p&gt;Happy training!&lt;/p&gt;</content><author><name></name></author><category term="machine-learning" /><category term="deepspeed" /><category term="training" /><summary type="html"></summary></entry></feed>