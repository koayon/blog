<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.9.3">Jekyll</generator><link href="http://localhost:4000/blog/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/blog/" rel="alternate" type="text/html" /><updated>2024-04-03T18:01:21+01:00</updated><id>http://localhost:4000/blog/feed.xml</id><title type="html">Kola Ayonrinde</title><subtitle>The technical blog of Kola Ayonrinde: Research Scientist/ML Engineer</subtitle><entry><title type="html">Mamba Explained</title><link href="http://localhost:4000/blog/2024/02/11/mamba.html" rel="alternate" type="text/html" title="Mamba Explained" /><published>2024-02-11T00:00:00+00:00</published><updated>2024-02-11T00:00:00+00:00</updated><id>http://localhost:4000/blog/2024/02/11/mamba</id><content type="html" xml:base="http://localhost:4000/blog/2024/02/11/mamba.html">&lt;h3 id=&quot;the-state-space-model-taking-on-transformers&quot;&gt;The State Space Model taking on Transformers&lt;/h3&gt;

&lt;div align=&quot;center&quot;&gt;
  &lt;figure&gt;
    &lt;img src=&quot;/blog/images/mamba/snake.png&quot; width=&quot;400&quot; alt=&quot;Mamba vs Transformer&quot; /&gt;
    &lt;figcaption&gt;&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;
&lt;p&gt;&lt;br /&gt;
&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;Right now, AI is eating the world.&lt;/p&gt;

&lt;p&gt;And by AI, I mean Transformers. Practically all the big breakthroughs in AI over
the last few years are due to Transformers.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Mamba&lt;/strong&gt;, however, is one of an alternative class of models called &lt;strong&gt;State
Space Models&lt;/strong&gt; (&lt;strong&gt;SSMs&lt;/strong&gt;). Importantly, for the first time, Mamba promises
similar performance (and crucially similar
&lt;a href=&quot;https://arxiv.org/pdf/2203.15556.pdf&quot;&gt;&lt;em&gt;scaling laws&lt;/em&gt;&lt;/a&gt;) as the Transformer
whilst being feasible at long sequence lengths (say 1 million tokens). We
achieve this long context by removing the ‚Äúquadratic bottleneck‚Äù in the
Attention Mechanism. Mamba also runs &lt;em&gt;fast&lt;/em&gt; - like ‚Äúup to 5x faster than
Transformer fast‚Äù&lt;sup id=&quot;fnref:figure&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:figure&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;
  &lt;figure&gt;
    &lt;img src=&quot;/blog/images/mamba/mamba_scaling.png&quot; width=&quot;800&quot; alt=&quot;Scaling Laws for Mamba vs other Language Models&quot; /&gt;
    &lt;figcaption&gt;Mamba performs similarly (or slightly better than) other Language Models on The Pile&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;Gu and Dao, the Mamba authors write:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Mamba enjoys fast inference and linear scaling in sequence length, and its
performance improves on real data up to million-length sequences. As a general
sequence model backbone, Mamba achieves state-of-the-art performance across
several modalities such as language, audio, and genomics. On language
modeling, our Mamba-3B model outperforms Transformers of the same size and
matches Transformers twice its size, both in pretraining and downstream
evaluation.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;Here we‚Äôll discuss:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;The advantages (and disadvantages) of Mamba (üêç) vs Transformers (ü§ñ),&lt;/li&gt;
  &lt;li&gt;Analogies and intuitions for thinking about Mamba, and&lt;/li&gt;
  &lt;li&gt;What Mamba means for Interpretability, AI Safety and Applications.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;problems-with-transformers---maybe-attention-isnt-all-you-need&quot;&gt;Problems with Transformers - Maybe Attention &lt;em&gt;Isn‚Äôt&lt;/em&gt; All You Need&lt;/h2&gt;

&lt;p&gt;We‚Äôre very much in the Transformer-era of history. ML used to be about detecting
cats and dogs. Now, with Transformers, we‚Äôre
&lt;a href=&quot;https://openai.com/research/gpt-4&quot;&gt;generating human-like poetry&lt;/a&gt;,
&lt;a href=&quot;https://storage.googleapis.com/deepmind-media/AlphaCode2/AlphaCode2_Tech_Report.pdf&quot;&gt;coding better than the median competitive programmer&lt;/a&gt;,
and
&lt;a href=&quot;https://www.nature.com/articles/s41586-021-03819-2&quot;&gt;solving the protein folding problem&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;But Transformers have one core problem. In a transformer, every token can look
back at every previous token when making predictions. For this lookback, we
cache detailed information about each token in the so-called KV cache.&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;
  &lt;figure&gt;
    &lt;img src=&quot;/blog/images/mamba/attention.png&quot; width=&quot;800&quot; alt=&quot;attention&quot; /&gt;
    &lt;figcaption&gt;When using the Attention Mechanism, information from all previous tokens can be passed to the current token&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;This pairwise communication means a forward pass is O(n¬≤) time complexity in
training (the dreaded &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;quadratic bottleneck&lt;/code&gt;) and each new token generated
autoregressively takes O(n) time. That is to say, as the context gets larger,
the model gets &lt;em&gt;slower&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;To add insult to injury, storing this KV cache requires O(n) space. The fateful
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;CUDA OOM&lt;/code&gt; error looms large as the memory footprint balloons. If space were the
only issue, we might just add more GPUs but with latency growing
quadratically‚Ä¶ perhaps not.&lt;/p&gt;

&lt;p&gt;On the margin, we can mitigate the quadratic bottleneck with techniques like
&lt;a href=&quot;https://paperswithcode.com/method/sliding-window-attention&quot;&gt;Sliding Window Attention&lt;/a&gt;
or clever CUDA optimisations like
&lt;a href=&quot;https://arxiv.org/pdf/2205.14135.pdf&quot;&gt;FlashAttention&lt;/a&gt;. But ultimately, for
super long context windows (like a chatbot which remembers every conversation
you‚Äôve shared), we need a different approach.&lt;/p&gt;

&lt;h3 id=&quot;foundation-model-backbones&quot;&gt;Foundation Model Backbones&lt;/h3&gt;

&lt;p&gt;Fundamentally, all good ML architecture backbones have components for two
important operations:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;strong&gt;Communication&lt;/strong&gt; &lt;em&gt;between&lt;/em&gt; tokens&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Computation&lt;/strong&gt; &lt;em&gt;within&lt;/em&gt; a token&lt;/li&gt;
&lt;/ol&gt;

&lt;div align=&quot;center&quot;&gt;
  &lt;figure&gt;
    &lt;img src=&quot;/blog/images/mamba/transformer_block.png&quot; width=&quot;800&quot; alt=&quot;Transformer Block&quot; /&gt;
    &lt;figcaption&gt;The Transformer Block&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;In transformers, this is &lt;strong&gt;Attention&lt;/strong&gt; (&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;communication&lt;/code&gt;) and &lt;strong&gt;MLPs&lt;/strong&gt;
(&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;computation&lt;/code&gt;). We improve transformers by optimising these two
operations&lt;sup id=&quot;fnref:scale&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:scale&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;2&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;

&lt;p&gt;We would like to replace the Attention component &lt;sup id=&quot;fnref:attention&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:attention&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;3&lt;/a&gt;&lt;/sup&gt; with some other
method for communicating between tokens. &lt;strong&gt;Mamba&lt;/strong&gt; uses the Control Theory
inspired &lt;strong&gt;SSM&lt;/strong&gt; for &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Communication&lt;/code&gt; and keeps MLP-style projections for
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Computation&lt;/code&gt;.&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;
  &lt;figure&gt;
    &lt;img src=&quot;/blog/images/mamba/mamba_block.png&quot; width=&quot;800&quot; alt=&quot;Mamba Block&quot; /&gt;
    &lt;figcaption&gt;The Mamba Block&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;Like a Transformer made up of stacked transformer blocks, Mamba is made up of
stacked Mamba blocks as above.&lt;/p&gt;

&lt;p&gt;We would like to understand and motivate the choice of the SSM for sequence
transformations.&lt;/p&gt;

&lt;h2 id=&quot;motivating-mamba---a-throwback-to-temple-run&quot;&gt;Motivating Mamba - A Throwback to Temple Run&lt;/h2&gt;

&lt;p&gt;Imagine we‚Äôre building a Temple Run agent &lt;sup id=&quot;fnref:temple-run&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:temple-run&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;4&lt;/a&gt;&lt;/sup&gt;. It chooses if the
runner should move left or right at any time.&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;
  &lt;figure&gt;
    &lt;img src=&quot;/blog/images/mamba/temple_run.png&quot; width=&quot;400&quot; alt=&quot;Temple Run&quot; /&gt;
    &lt;figcaption&gt;&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;To successfully pick the correct direction, we need information about our
surroundings. Let‚Äôs call the collection of relevant information the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;state&lt;/code&gt;.
Here the state likely includes your current position and velocity, the position
of the nearest obstacle, weather conditions, etc.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Claim 1: if you know the current state of the world and how the world is
evolving, then you can use this to determine the direction to move.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Note that you don‚Äôt need to look at the whole screen all the time. You can
figure out what will happen to most of the screen by noting that as you run, the
obstacles move down the screen. You only need to look at the top of the screen
to understand the new information and then simulate the rest.&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;
  &lt;figure&gt;
    &lt;img src=&quot;/blog/images/mamba/temple_run_annotated.png&quot; width=&quot;800&quot; alt=&quot;Temple Run&quot; /&gt;
    &lt;figcaption&gt;&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;This lends itself to a natural formulation. Let h be the hidden state, relevant
knowledge about the world. Also let x be the input, the observation that you get
each time. h‚Äô then represents the derivative of the hidden state, i.e. how the
state is evolving. We‚Äôre trying to predict y, the optimal next move (right or
left).&lt;/p&gt;

&lt;p&gt;Now, Claim 1 states that
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;from the hidden state h, h‚Äô, and the new observation x, you can figure out y&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;More concretely, h, the state, can be represented as a differential equation (Eq
1a):&lt;/p&gt;

&lt;p&gt;$$
h‚Äô(t) = \mathbf{A}h(t) + \mathbf{B}x(t)
$$&lt;/p&gt;

&lt;p&gt;Knowing h allows you to determine your next move y (Eq 1b):&lt;/p&gt;

&lt;p&gt;$$
y(t) = \mathbf{C}h(t) + \mathbf{D}x(t)
$$&lt;/p&gt;

&lt;p&gt;The system evolves as a function of the current state and new observations. A
small new observation is enough because we can determine most of the state by
applying known state dynamics to the previous state. That is, most of the screen
isn‚Äôt new, it‚Äôs just the natural downward movement of the previous state. Fully
knowing the state would allow us to pick the best next move, y.&lt;/p&gt;

&lt;p&gt;You can learn a lot about the system dynamics by observing the top of the
screen - if it‚Äôs moving faster, we can infer the whole screen is and the game is
speeding up&lt;sup id=&quot;fnref:smooth&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:smooth&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;5&lt;/a&gt;&lt;/sup&gt;. In this way, even if we start off knowing nothing about
the game except our limited observation, pretty soon we could understand the
whole screen.&lt;/p&gt;

&lt;h3 id=&quot;whats-the-state&quot;&gt;What‚Äôs the State?&lt;/h3&gt;

&lt;p&gt;Here, &lt;strong&gt;state&lt;/strong&gt; refers to the variables that, when combined with the input
variables, fully determine the future system behaviour. In theory, once we have
the state, there‚Äôs nothing else we need to know about the past to predict the
future. With this choice of state, the system is converted to a &lt;strong&gt;Markov
Decision Process&lt;/strong&gt;. Ideally, the state is a fairly small amount of information
which captures the essential properties of the system. That is, &lt;strong&gt;the state is a
compression of the past&lt;/strong&gt; &lt;sup id=&quot;fnref:diagonal&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:diagonal&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;6&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;

&lt;h2 id=&quot;discretisation---how-to-deal-with-living-in-a-quantised-world&quot;&gt;Discretisation - How To Deal With Living in a Quantised World&lt;/h2&gt;

&lt;p&gt;Okay, great! So, given some state and input observation, we have an
autoregressive-style system to determine the next action. Amazing!&lt;/p&gt;

&lt;p&gt;In practice though, there‚Äôs a little snag here. We‚Äôre modelling time as
continuous. But in real life, we get new inputs and take new actions at discrete
time steps &lt;sup id=&quot;fnref:discrete&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:discrete&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;7&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;
  &lt;figure&gt;
    &lt;img src=&quot;/blog/images/mamba/quantised.png&quot; width=&quot;600&quot; alt=&quot;Reality is Quantised&quot; /&gt;
    &lt;figcaption&gt;&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;We would like to convert this &lt;em&gt;continuous-time differential equation&lt;/em&gt; into a
&lt;em&gt;discrete-time difference equation&lt;/em&gt;. This conversion process is known as
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;discretisation&lt;/code&gt;. Discretisation is a well-studied problem in the literature.
Mamba uses the &lt;a href=&quot;https://en.wikipedia.org/wiki/Zero-order_hold&quot;&gt;Zero-Order Hold&lt;/a&gt;
(ZOH) discretisation&lt;sup id=&quot;fnref:zoh&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:zoh&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;8&lt;/a&gt;&lt;/sup&gt;. To give an idea of what‚Äôs happening morally,
consider a naive first-order approximation&lt;sup id=&quot;fnref:Euler&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:Euler&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;9&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;

&lt;p&gt;From Equation 1a, we have&lt;/p&gt;

&lt;p&gt;$$
h‚Äô(t) = \mathbf{A}h(t) + \mathbf{B}x(t)
$$&lt;/p&gt;

&lt;p&gt;And for small ‚àÜ,&lt;/p&gt;

&lt;p&gt;$$
h‚Äô(t) \approx \frac{h(t+\Delta) - h(t)}{\Delta}
$$&lt;/p&gt;

&lt;p&gt;by the definition of the derivative.&lt;/p&gt;

&lt;p&gt;We let:&lt;/p&gt;

&lt;p&gt;$$
h_t = h(t)
$$&lt;/p&gt;

&lt;p&gt;and&lt;/p&gt;

&lt;p&gt;$$
h_{t+1} = h(t + \Delta)
$$&lt;/p&gt;

&lt;p&gt;and substitute into Equation 1a giving:&lt;/p&gt;

&lt;p&gt;$$
h_{t+1} - h_t \approx \Delta (\mathbf{A}h_t + \mathbf{B}x_t)
$$&lt;/p&gt;

&lt;p&gt;$$
\Rightarrow h_{t+1} \approx (I + \Delta \mathbf{A})h_t + (\Delta
\mathbf{B})x_t
$$&lt;/p&gt;

&lt;p&gt;Hence, after renaming the coefficients and relabelling indices, we have the
discrete representations:&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;
  &lt;figure&gt;
    &lt;img src=&quot;/blog/images/mamba/equation_2.png&quot; width=&quot;800&quot; alt=&quot;Equation 2&quot; /&gt;
    &lt;figcaption&gt;The Discretised Version of the SSM Equation&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;If you‚Äôve ever looked at an RNN before &lt;sup id=&quot;fnref:rnn&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:rnn&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;10&lt;/a&gt;&lt;/sup&gt; and this feels familiar - &lt;em&gt;trust
your instincts&lt;/em&gt;:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;We have some input x, which is combined with the previous hidden state by some
transform to give the new hidden state. Then we use the hidden state to
calculate the output at each time step.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;understanding-the-ssm-matrices&quot;&gt;Understanding the SSM Matrices&lt;/h2&gt;

&lt;p&gt;Now, we can interpret the A, B, C, D matrices more intuitively:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;A is the transition state matrix. It shows how you transition the current
state into the next state. It asks ‚ÄúHow should I forget the less relevant
parts of the state over time?‚Äù&lt;/li&gt;
  &lt;li&gt;B is mapping the new input into the state, asking ‚ÄúWhat part of my new input
should I remember?‚Äù. &lt;sup id=&quot;fnref:B&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:B&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;11&lt;/a&gt;&lt;/sup&gt;&lt;/li&gt;
  &lt;li&gt;C is mapping the state to the output of the SSM. It asks, ‚ÄúHow can I use the
state to make a good next prediction?‚Äù. &lt;sup id=&quot;fnref:C&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:C&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;12&lt;/a&gt;&lt;/sup&gt;&lt;/li&gt;
  &lt;li&gt;D is how the new input passes through to the output. It‚Äôs a kind of modified
skip connection that asks ‚ÄúHow can I use the new input in my prediction?‚Äù&lt;/li&gt;
&lt;/ul&gt;

&lt;div align=&quot;center&quot;&gt;
  &lt;figure&gt;
    &lt;img src=&quot;/blog/images/mamba/graphical_matmuls.png&quot; width=&quot;600&quot; alt=&quot;Visual SSM Equations&quot; /&gt;
    &lt;figcaption&gt;Visual Representation of The SSM Equations&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;Additionally, ‚àÜ has a nice interpretation - it‚Äôs the step size, or what we might
call the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;linger time&lt;/code&gt; or the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;dwell time&lt;/code&gt;. For large ‚àÜ, you focus more on that
token; for small ‚àÜ, you skip past the token immediately and don‚Äôt include it
much in the next state.&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;
  &lt;figure&gt;
    &lt;img src=&quot;/blog/images/mamba/mamba_hardware_diagram.png&quot; width=&quot;800&quot; alt=&quot;Hardware Diagram&quot; /&gt;
    &lt;figcaption&gt;&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;And that‚Äôs it! That‚Äôs the SSM, our ~drop-in replacement for Attention
(&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Communication&lt;/code&gt;) in the Mamba block. The &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Computation&lt;/code&gt; in the Mamba
architecture comes from regular linear projections, non-linearities, and local
convolutions - the regular ML building blocks we know and love!&lt;/p&gt;

&lt;p&gt;Okay great, that‚Äôs the theory - but does this work? Well‚Ä¶&lt;/p&gt;

&lt;h2 id=&quot;effectiveness-vs-efficiency-attention-is-focus-selectivity-is-prioritisation&quot;&gt;Effectiveness vs Efficiency: Attention is Focus, Selectivity is Prioritisation&lt;/h2&gt;

&lt;p&gt;At WWDC ‚Äò97, Steve Jobs famously noted that
‚Äú&lt;a href=&quot;https://www.youtube.com/watch?v=H8eP99neOVs&amp;amp;t=98s&quot;&gt;focusing is about saying no&lt;/a&gt;‚Äù.
Focus is ruthless prioritisation. It‚Äôs common to think about Attention
&lt;em&gt;positively&lt;/em&gt; as choosing what to &lt;em&gt;notice&lt;/em&gt;. In the Steve Jobs sense, we might
instead frame Attention &lt;em&gt;negatively&lt;/em&gt; as choosing what to &lt;em&gt;discard&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;There‚Äôs a classic intuition pump in Machine Learning known as the
&lt;a href=&quot;https://ieeexplore.ieee.org/document/8555495&quot;&gt;Cocktail Party Problem&lt;/a&gt;
&lt;sup id=&quot;fnref:alcohol&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:alcohol&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;13&lt;/a&gt;&lt;/sup&gt;. Imagine a party with dozens of simultaneous loud conversations:&lt;/p&gt;

&lt;p&gt;Question:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;How do we recognise what one person is saying when others are talking at the
same time? &lt;sup id=&quot;fnref:frequency&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:frequency&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;14&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Answer:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;The brain solves this problem by focusing your ‚Äúattention‚Äù on a particular
stimulus &lt;em&gt;and hence&lt;/em&gt; drowning out all other sounds as much as possible.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;div align=&quot;center&quot;&gt;
  &lt;figure&gt;
    &lt;img src=&quot;/blog/images/mamba/cocktail_party.png&quot; width=&quot;450&quot; alt=&quot;Cocktail Party&quot; /&gt;
    &lt;figcaption&gt;&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;Transformers use Dot-Product Attention to focus in on the most relevant tokens.
A big reason Attention is so great is that you have the potential to look back
at everything that ever happened in its context. This is like photographic
memory when done right. &lt;sup id=&quot;fnref:photo&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:photo&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;15&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;

&lt;p&gt;Transformers (ü§ñ) are extremely &lt;strong&gt;effective&lt;/strong&gt;. But they aren‚Äôt very
&lt;strong&gt;efficient&lt;/strong&gt;. They store everything from the past so that they can look back at
tokens with theoretically perfect recall.&lt;/p&gt;

&lt;p&gt;Traditional RNNs (üîÅ) are the opposite - they forget a lot, only recalling a
small amount in their hidden state and discarding the rest. They are very
&lt;strong&gt;efficient&lt;/strong&gt; - their state is small. Yet they are less &lt;strong&gt;effective&lt;/strong&gt; as
discarded information cannot be recovered.&lt;/p&gt;

&lt;p&gt;We‚Äôd like something closer to the Pareto frontier of the
effectiveness/efficiency tradeoff. Something that‚Äôs more effective than
traditional RNNs and more efficient than transformers.&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;
  &lt;figure&gt;
    &lt;img src=&quot;/blog/images/mamba/pareto_frontier.png&quot; width=&quot;800&quot; alt=&quot;Pareto Frontier&quot; /&gt;
    &lt;figcaption&gt;The Mamba Architecture seems to offer a solution which pushes out the Pareto frontier of effectiveness/efficiency.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;SSMs are as &lt;strong&gt;efficient&lt;/strong&gt; as RNNs, but we might wonder how &lt;strong&gt;effective&lt;/strong&gt; they
are. After all, it seems like they would have a hard time discarding only
&lt;em&gt;unnecessary&lt;/em&gt; information and keeping everything relevant. If each token is
being processed the same way, applying the same A and B matrices as if in a
factory assembly line for tokens, there is no context-dependence. We would like
the forgetting and remembering matrices (A and B respectively) to vary and
dynamically adapt to inputs.&lt;/p&gt;

&lt;h3 id=&quot;the-selection-mechanism&quot;&gt;The Selection Mechanism&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;Selectivity&lt;/strong&gt; allows each token to be transformed into the state in a way that
is unique to its own needs. Selectivity is what takes us from vanilla SSM models
(applying the same A (forgetting) and B (remembering) matrices to every input)
to Mamba, the &lt;strong&gt;&lt;em&gt;Selective&lt;/em&gt;&lt;/strong&gt; &lt;em&gt;State Space Model&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;In regular SSMs, A, B, C and D are learned matrices - that is&lt;/p&gt;

&lt;p&gt;$$
\mathbf{A} =
\mathbf{A}_{\theta}$$ etc. (where Œ∏ represents the learned
parameters)&lt;/p&gt;

&lt;p&gt;With the Selection Mechanism in Mamba, A, B, C and D are also functions of x.
That is $$\mathbf{A} = \mathbf{A}_{\theta(x)}$$ etc; the matrices are context
dependent rather than static.&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;
  &lt;figure&gt;
    &lt;img src=&quot;/blog/images/mamba/ssm_algorithm.png&quot; width=&quot;800&quot; alt=&quot;SSM Algorithm&quot; /&gt;
    &lt;figcaption&gt;Mamba (right) differs from traditional SSMs by allowing A,B,C matrices to be &lt;b&gt; selective &lt;/b&gt; i.e. context dependent &lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;Making A and B functions of x allows us to get the best of both worlds:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;We‚Äôre selective about what we include in the state, which improves
&lt;strong&gt;effectiveness&lt;/strong&gt; vs traditional SSMs.&lt;/li&gt;
  &lt;li&gt;Yet, since the state size is bounded, we improve on &lt;strong&gt;efficiency&lt;/strong&gt; relative to
the Transformer. We have O(1), not O(n) space and O(n) not O(n¬≤) time
requirements.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The Mamba paper authors write:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;The efficiency vs. effectiveness tradeoff of sequence models is characterized
by how well they compress their state: efficient models must have a small
state, while effective models must have a state that contains all necessary
information from the context. In turn, we propose that a fundamental principle
for building sequence models is selectivity: or the context-aware ability to
focus on or filter out inputs into a sequential state. In particular, a
selection mechanism controls how information propagates or interacts along the
sequence dimension.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;Humans (mostly) don‚Äôt have photographic memory for everything they experience
within a lifetime - or even within a day! There‚Äôs just way too much information
to retain it all. Subconsciously, we select what to remember by choosing to
forget, throwing away most information as we encounter it. Transformers (ü§ñ)
decide what to focus on at &lt;strong&gt;recall time&lt;/strong&gt;. Humans (üßë) also decide what to
throw away at &lt;strong&gt;memory-making time&lt;/strong&gt;. Humans filter out information early and
often.&lt;/p&gt;

&lt;p&gt;If we had infinite capacity for memorisation, it‚Äôs clear the transformer
approach is better than the human approach - it truly is more effective. But
it‚Äôs less efficient - transformers have to store so much information about the
past that might not be relevant. Transformers (ü§ñ) only decide what‚Äôs relevant
at &lt;strong&gt;recall time&lt;/strong&gt;. The innovation of Mamba (üêç) is allowing the model better
ways of forgetting earlier - it‚Äôs focusing by choosing what to &lt;em&gt;discard&lt;/em&gt; using
&lt;strong&gt;Selectivity&lt;/strong&gt;, throwing away less relevant information at &lt;strong&gt;memory-making
time&lt;/strong&gt;&lt;sup id=&quot;fnref:seq_len&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:seq_len&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;16&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;

&lt;h3 id=&quot;the-problems-of-selectivity&quot;&gt;The Problems of Selectivity&lt;/h3&gt;

&lt;p&gt;Applying the Selection Mechanism does have its gotchas though. Non-selective
SSMs (i.e. A,B not dependent on x) are fast to compute in training. This is
because the component of $$ y_t $$ which depends on $$ x_i $$ can be expressed
as a linear map, i.e. a single matrix that can be precomputed!&lt;/p&gt;

&lt;p&gt;For example (ignoring the D component, the skip connection):&lt;/p&gt;

&lt;p&gt;$$
y_2 = \mathbf{C}\mathbf{B}x_2 + \mathbf{C}\mathbf{A}\mathbf{B}x_1 +
\mathbf{C}\mathbf{A}\mathbf{A}\mathbf{B}x_0
$$&lt;/p&gt;

&lt;p&gt;If we‚Äôre paying attention, we might spot something even better here - this
expression can be written as a convolution. Hence we can apply the Fast Fourier
Transform and the Convolution Theorem to compute this &lt;em&gt;very&lt;/em&gt; efficiently on
hardware as in Equation 3 below.&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;
  &lt;figure&gt;
    &lt;img src=&quot;/blog/images/mamba/equations_2_3.png&quot; width=&quot;800&quot; alt=&quot;Equations 2 and 3&quot; /&gt;
    &lt;figcaption&gt;We can calculate Equation 2, the SSM equations, efficiently in the Convolutional Form, Equation 3. &lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;Unfortunately, with the Selection Mechanism, we lose the convolutional form.
Much attention is given to making Mamba efficient on modern GPU hardware using
similar hardware optimisation tricks to Tri Dao‚Äôs Flash Attention &lt;sup id=&quot;fnref:cuda&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:cuda&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;17&lt;/a&gt;&lt;/sup&gt;. With
the hardware optimisations, Mamba is able to run faster than comparably sized
Transformers.&lt;/p&gt;

&lt;h3 id=&quot;machine-learning-for-political-economists---how-large-should-the-state-be&quot;&gt;Machine Learning for Political Economists - How Large Should The State Be?&lt;/h3&gt;

&lt;p&gt;The Mamba authors write, ‚Äúthe efficiency vs. effectiveness tradeoff of sequence
models is characterised by how well they compress their state‚Äù. In other words,
like in political economy&lt;sup id=&quot;fnref:oop&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:oop&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;18&lt;/a&gt;&lt;/sup&gt;, the fundamental problem is how to manage the
state.&lt;/p&gt;

&lt;p&gt;üîÅ &lt;strong&gt;Traditional RNNs are anarchic&lt;/strong&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;They have a small, minimal state. The size of the state is bounded. The
compression of state is poor.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;ü§ñ &lt;strong&gt;Transformers are communist&lt;/strong&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;They have a maximally large state. The ‚Äústate‚Äù is just a cache of the entire
history with no compression. Every context token is treated equally until
recall time.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;üêç&lt;strong&gt;Mamba has a compressed state&lt;/strong&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;‚Ä¶but it‚Äôs selective about what goes in. Mamba says we can get away with a
small state if the state is well focused and effective &lt;sup id=&quot;fnref:politik&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:politik&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;19&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;div align=&quot;center&quot;&gt;
  &lt;figure&gt;
    &lt;img src=&quot;/blog/images/mamba/political_spectrum.png&quot; width=&quot;800&quot; alt=&quot;Language Models and State Size&quot; /&gt;
    &lt;figcaption&gt;Language Models and State Size&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;The upshot is &lt;strong&gt;state representation is critical&lt;/strong&gt;. A smaller state is more
efficient; a larger state is more effective. The key is to &lt;strong&gt;selectively&lt;/strong&gt; and
&lt;strong&gt;dynamically&lt;/strong&gt; compress data into the state. Mamba‚Äôs Selection Mechanism allows
for context-dependent reasoning, focusing and ignoring. For both performance and
interpretability, understanding the state seems to be very useful.&lt;/p&gt;

&lt;h2 id=&quot;information-flow-in-transformer-vs-mamba&quot;&gt;Information Flow in Transformer vs Mamba&lt;/h2&gt;

&lt;p&gt;How do Transformers know anything? At initialisation, a transformer isn‚Äôt very
smart. It learns in two ways:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Training data (Pretraining, SFT, RLHF etc)&lt;/li&gt;
  &lt;li&gt;In context-data&lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;training-data&quot;&gt;Training Data&lt;/h4&gt;

&lt;p&gt;Models learn from their training data. This is a kind of lossy compression of
input data into the weights. We can think of the effect of pretraining data on
the transformer kinda like the effect of your ancestor‚Äôs experiences on your
genetics - you can‚Äôt recall their experiences, you just have vague instincts
about them &lt;sup id=&quot;fnref:analogy&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:analogy&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;20&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;

&lt;h4 id=&quot;in-context-data&quot;&gt;In Context-Data&lt;/h4&gt;

&lt;p&gt;Transformers use their context as short-term memory, which they can recall with
~perfect fidelity. So we get
&lt;a href=&quot;https://thegradient.pub/in-context-learning-in-context/&quot;&gt;In-Context Learning&lt;/a&gt;,
e.g. using induction heads to solve the
&lt;a href=&quot;https://arxiv.org/pdf/2211.00593.pdf&quot;&gt;Indirect Object Identification&lt;/a&gt; task, or
&lt;a href=&quot;https://proceedings.neurips.cc/paper_files/paper/2022/file/c529dba08a146ea8d6cf715ae8930cbe-Paper-Conference.pdf&quot;&gt;computing Linear Regression&lt;/a&gt;.&lt;/p&gt;

&lt;h4 id=&quot;retrieval&quot;&gt;Retrieval&lt;/h4&gt;

&lt;p&gt;Note that Transformers don‚Äôt filter their context at all until recall time. So
if we have a bunch of information we think &lt;em&gt;might&lt;/em&gt; be useful to the Transformer,
we filter it &lt;em&gt;outside&lt;/em&gt; the Transformer (using Information Retrieval strategies)
and then stuff the results into the prompt. This process is known as Retrieval
Augmented Generation (RAG). RAG determines relevant information for the context
window of a transformer. A human with the internet is kinda like a RAG system -
you still have to know what to search but whatever you retrieve is as salient as
short-term memory to you.&lt;/p&gt;

&lt;h4 id=&quot;information-flow-for-mamba&quot;&gt;Information Flow for Mamba&lt;/h4&gt;

&lt;p&gt;Training Data acts similarly for Mamba. However, the lines are slightly blurred
for in-context data and retrieval. In-context data for Mamba &lt;em&gt;is&lt;/em&gt;
compressed/filtered similar to retrieval data for transformers. This in-context
data is also accessible for look-up like for transformers (although with
somewhat lower fidelity).&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;
  &lt;figure&gt;
    &lt;img src=&quot;/blog/images/mamba/information_flow.png&quot; width=&quot;800&quot; alt=&quot;The Information Flow in Mamba&quot; /&gt;
    &lt;figcaption&gt;&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;Transformer context is to Mamba states what short-term is to long-term memory.
Mamba doesn‚Äôt just have ‚ÄúRAM‚Äù, it has a hard drive&lt;sup id=&quot;fnref:hard-drive&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:hard-drive&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;21&lt;/a&gt;&lt;/sup&gt; &lt;sup id=&quot;fnref:ssd&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:ssd&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;22&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;

&lt;h3 id=&quot;swapping-states-as-a-new-prompting-paradigm&quot;&gt;Swapping States as a New Prompting Paradigm&lt;/h3&gt;

&lt;p&gt;Currently, we often use RAG to give a transformer contextual information.&lt;/p&gt;

&lt;p&gt;With Mamba-like models, you could instead imagine having a library of states
created by running the model over specialised data. States could be shared kinda
like
&lt;a href=&quot;https://paperswithcode.com/paper/lora-low-rank-adaptation-of-large-language&quot;&gt;LoRAs&lt;/a&gt;
for image models.&lt;/p&gt;

&lt;p&gt;For example, I could do inference on 20 physics textbooks and, say, 100 physics
questions and answers. Then I have a state which I can give to you. Now you
don‚Äôt need to add any few-shot examples; you just simply ask your question.
&lt;strong&gt;The in-context learning is in the state&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;In other words, you can drag and drop downloaded states into your model, like
literal plug-in cartridges. And note that ‚Äútraining‚Äù a state doesn‚Äôt require any
backprop. It‚Äôs more like a highly specialised one-pass fixed-size compression
algorithm. This is unlimited in-context learning applied at inference time for
zero-compute or latency. &lt;sup id=&quot;fnref:steering&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:steering&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;23&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;

&lt;p&gt;The structure of an effective LLM call goes from‚Ä¶&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;System Prompt&lt;/li&gt;
  &lt;li&gt;Preamble&lt;/li&gt;
  &lt;li&gt;Few shot-examples&lt;/li&gt;
  &lt;li&gt;Question&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;‚Ä¶for Transformers, to simply‚Ä¶&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Inputted state (with problem context, initial instructions, textbooks, and
few-shot examples)&lt;/li&gt;
  &lt;li&gt;Short question&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;‚Ä¶for Mamba.&lt;/p&gt;

&lt;p&gt;This is cheaper and faster than few-shot prompting (as the state is infinitely
reusable without inference cost). It‚Äôs also MUCH cheaper than finetuning and
doesn‚Äôt require any gradient updates. We could imagine retrieving states in
addition to context.&lt;/p&gt;

&lt;h2 id=&quot;mamba--mechanistic-interpretability&quot;&gt;Mamba &amp;amp; Mechanistic Interpretability&lt;/h2&gt;

&lt;p&gt;Transformer interpretability typically involves:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;understanding token relationships via attention,&lt;/li&gt;
  &lt;li&gt;understanding circuits, and&lt;/li&gt;
  &lt;li&gt;using
&lt;a href=&quot;https://www.kolaayonrinde.com/blog/2023/11/03/dictionary-learning.html&quot;&gt;Dictionary Learning&lt;/a&gt;
for unfolding MLPs.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Most of the ablations that we would like to do for Mamba are still valid, but
understanding token communication (1) is now more nuanced. All information moves
between tokens via hidden states instead of the Attention Mechanism which can
‚Äúteleport‚Äù information from one sequence position to another.&lt;/p&gt;

&lt;p&gt;For understanding in-context learning (ICL) tasks with Mamba, we will look to
intervene on the SSM state. A classic task in-context learning task is
&lt;a href=&quot;https://arxiv.org/pdf/2211.00593.pdf&quot;&gt;Indirect Object Identification&lt;/a&gt; in which
a model has to finish a paragraph like:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;em&gt;Then, Shelby and Emma had a lot of fun at the school. [Shelby/Emma] gave an
apple to [BLANK]&lt;/em&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;The model is expected to fill in the blank with the name that is not repeated in
the paragraph. In the chart below we can see that information is passed from the
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;[Shelby/Emma]&lt;/code&gt; position to the final position via the hidden state (see the two
blue lines in the top chart).&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;
  &lt;figure&gt;
    &lt;img src=&quot;/blog/images/mamba/patching_state.png&quot; width=&quot;800&quot; alt=&quot;Patching State&quot; /&gt;
    &lt;figcaption&gt;&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;
  &lt;figure&gt;
    &lt;img src=&quot;/blog/images/mamba/patching_residual_stream.png&quot; width=&quot;800&quot; alt=&quot;Patching Residual Stream&quot; /&gt;
    &lt;figcaption&gt;&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;Since it‚Äôs hypothesised that much of In-Context Learning in Transformers is
downstream of more primitive sequence position operations (like
&lt;a href=&quot;https://transformer-circuits.pub/2022/in-context-learning-and-induction-heads/index.html&quot;&gt;Induction Heads&lt;/a&gt;),
Mamba being able to complete this task suggests a more general In-Context
Learning ability.&lt;/p&gt;

&lt;h2 id=&quot;whats-next-for-mamba--ssms&quot;&gt;What‚Äôs Next for Mamba &amp;amp; SSMs?&lt;/h2&gt;

&lt;p&gt;Mamba-like models are likely to excel in scenarios requiring extremely long
context and long-term memory. Examples include:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Processing DNA&lt;/li&gt;
  &lt;li&gt;Generating (or reasoning over) video&lt;/li&gt;
  &lt;li&gt;Writing novels&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;An illustrative example is agents with long-term goals.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Suppose you have an agent interacting with the world. Eventually, its
experiences become too much for the context window of a transformer. The agent
then has to compress or summarise its experiences into some more compact
representation.&lt;/p&gt;

  &lt;p&gt;But how do you decide what information is the most useful as a summary? If the
task is language, LLMs are actually fairly good at summaries - okay, yeah,
you‚Äôll lose some information, but the most important stuff can be retained.&lt;/p&gt;

  &lt;p&gt;However, for other disciplines, it might not be clear how to summarise. For
example, what‚Äôs the best way to summarise a 2 hour movie? &lt;sup id=&quot;fnref:movie&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:movie&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;24&lt;/a&gt;&lt;/sup&gt;. Could the
model itself learn to do this naturally rather than a hacky workaround like
trying to describe the aesthetics of the movie in text?&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;This is what Mamba allows. Actual long-term memory. A real state where the model
learns to keep what‚Äôs important.
&lt;a href=&quot;https://arxiv.org/pdf/2309.10668.pdf&quot;&gt;Prediction is compression&lt;/a&gt; - learning
what‚Äôs useful to predict what‚Äôs coming next inevitably leads to building a
useful compression of the previous tokens.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;The implications for Assistants are clear:&lt;/p&gt;

&lt;p&gt;Your chatbot co-evolves with you. It remembers.&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;
  &lt;figure&gt;
    &lt;img src=&quot;/blog/images/mamba/her.png&quot; width=&quot;800&quot; alt=&quot;Her&quot; /&gt;
    &lt;figcaption&gt;The film HER is looking better and better as time goes on üò≥&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;agents--ai-safety&quot;&gt;Agents &amp;amp; AI Safety&lt;/h3&gt;

&lt;p&gt;One reason for positive updates in existential risk from AGI is Language Models.
Previously, Deep-RL agents trained via self-play looked set to be the first
AGIs. Language models are inherently much safer since they aren‚Äôt trained with
long-term goals. &lt;sup id=&quot;fnref:safe&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:safe&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;25&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;

&lt;p&gt;The potential for long-term sequence reasoning here brings back the importance
of agent-based AI safety. Few agent worries are relevant to Transformers with an
8k context window. Many are relevant to systems with impressive long-term
memories and possible instrumental goals.&lt;/p&gt;

&lt;h3 id=&quot;the-best-collab-since-taco-bell--kfc--x-&quot;&gt;The Best Collab Since Taco Bell &amp;amp; KFC: ü§ñ x üêç&lt;/h3&gt;

&lt;p&gt;The Mamba authors show that there‚Äôs value in combining Mamba‚Äôs long context with
the Transformer‚Äôs high fidelity over short sequences. For example, if you‚Äôre
making long videos, you likely can‚Äôt fit a whole movie into a Transformer‚Äôs
context for attention &lt;sup id=&quot;fnref:image&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:image&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;26&lt;/a&gt;&lt;/sup&gt;. You could imagine having Attention look at the
most recent frames for short-term fluidity and an SSM for long-term narrative
consistency &lt;sup id=&quot;fnref:optimisation&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:optimisation&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;27&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;This isn‚Äôt the end for Transformers. Their high effectiveness is exactly what‚Äôs
needed for many tasks. But now Transformers aren‚Äôt the only option. Other
architectures are genuinely feasible.&lt;/p&gt;

&lt;p&gt;So we‚Äôre not in the post-&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Transformer&lt;/code&gt; era. But for the first time, we‚Äôre living
in the post-&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;only-Transformers&lt;/code&gt; era &lt;sup id=&quot;fnref:other-models&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:other-models&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;28&lt;/a&gt;&lt;/sup&gt;. And this blows the
possibilities wide open for sequence modelling with extreme context lengths and
native long-term memory.&lt;/p&gt;

&lt;p&gt;Two ML researchers, Sasha Rush (HuggingFace, Annotated Transformer, Cornell
Professor) and Jonathan Frankle (Lottery Ticket Hypothesis, MosaicML, Harvard
Professor), currently have a bet &lt;a href=&quot;http://www.isattentionallyouneed.com/&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;
  &lt;figure&gt;
    &lt;img src=&quot;/blog/images/mamba/attention_wager.png&quot; width=&quot;800&quot; alt=&quot;Attention Wager&quot; /&gt;
    &lt;figcaption&gt;&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;Currently Transformers are far and away in the lead. With 3 years left, there‚Äôs
now a research direction with a fighting chance.&lt;/p&gt;

&lt;p&gt;All that remains to ask is: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Is Attention All We Need?&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;Join the discussion on Hacker News &lt;a href=&quot;https://news.ycombinator.com/item?id=39501982&quot;&gt;here&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Thanks to Gon√ßalo for reading an early draft, Jaden for the nnsight library
used for the Interpretability analysis and Tessa for Mamba patching
visualisations.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Also see: &lt;a href=&quot;https://arxiv.org/pdf/2312.00752.pdf&quot;&gt;Mamba paper&lt;/a&gt;, Mamba Python
code, &lt;a href=&quot;https://srush.github.io/annotated-s4/&quot;&gt;Annotated S4&lt;/a&gt;,
&lt;a href=&quot;https://www.cognitiverevolution.ai/emergency-pod-mamba-memory-and-the-ssm-moment/&quot;&gt;Nathan Labenz podcast&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;br /&gt;&lt;/p&gt;
&lt;div class=&quot;footnotes&quot; role=&quot;doc-endnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:figure&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;see Figure 8 in the Mamba paper.¬†&lt;a href=&quot;#fnref:figure&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:scale&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;And scaling up with massive compute.¬†&lt;a href=&quot;#fnref:scale&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:attention&quot; role=&quot;doc-endnote&quot;&gt;

      &lt;p&gt;More specifically the scaled dot-product Attention popularised by
Transformers¬†&lt;a href=&quot;#fnref:attention&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:temple-run&quot; role=&quot;doc-endnote&quot;&gt;

      &lt;p&gt;For people who don‚Äôt see Temple Run as the cultural cornerstone it is ü§£
Temple Run was an iPhone game from 2011 similar to Subway Surfer¬†&lt;a href=&quot;#fnref:temple-run&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:smooth&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Here we assume the environment is sufficiently smooth.¬†&lt;a href=&quot;#fnref:smooth&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:diagonal&quot; role=&quot;doc-endnote&quot;&gt;

      &lt;p&gt;One pretty important constraint for this to be efficient is that we don‚Äôt
allow the individual elements of the state vector to interact with each
other directly. We‚Äôll use a combination of the state dimensions to determine
the output but we don‚Äôt e.g. allow the velocity of the runner and the
direction of the closest obstacle (or whatever else was in our state) to
directly interact. This helps with efficient computation and we achieve this
practically by constraining A to be a diagonal matrix.¬†&lt;a href=&quot;#fnref:diagonal&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:discrete&quot; role=&quot;doc-endnote&quot;&gt;

      &lt;p&gt;Concretely consider the case of Language Models - each token is a discrete
step¬†&lt;a href=&quot;#fnref:discrete&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:zoh&quot; role=&quot;doc-endnote&quot;&gt;

      &lt;p&gt;ZOH also has nice properties for the initialisations - we want A_bar to be
close to the identity so that the state can be mostly maintained from
timestep to timestep if desired. ZOH gives A_bar as an exponential so any
diagonal element initialisations close to zero give values close to 1¬†&lt;a href=&quot;#fnref:zoh&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:Euler&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;This is known as the Euler discretisation in the literature¬†&lt;a href=&quot;#fnref:Euler&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:rnn&quot; role=&quot;doc-endnote&quot;&gt;

      &lt;p&gt;It‚Äôs wild to note that some readers might not have, we‚Äôre so far into the
age of Attention that RNNs have been forgotten!¬†&lt;a href=&quot;#fnref:rnn&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:B&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;B is like the Query (Q) matrix for Transformers.¬†&lt;a href=&quot;#fnref:B&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:C&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;C is like the Output (O) matrix for Transformers.¬†&lt;a href=&quot;#fnref:C&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:alcohol&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Non-alcoholic options also available!¬†&lt;a href=&quot;#fnref:alcohol&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:frequency&quot; role=&quot;doc-endnote&quot;&gt;

      &lt;p&gt;Especially as all voices roughly occupy the same space on the audio
frequency spectrum Intuitively this seems really hard!¬†&lt;a href=&quot;#fnref:frequency&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:photo&quot; role=&quot;doc-endnote&quot;&gt;

      &lt;p&gt;Note that photographic memory doesn‚Äôt necessarily imply perfect inferences
from that memory!¬†&lt;a href=&quot;#fnref:photo&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:seq_len&quot; role=&quot;doc-endnote&quot;&gt;

      &lt;p&gt;To be clear, if you have a short sequence, then a transformer should
theoretically be a better approach. If you &lt;em&gt;can&lt;/em&gt; store the whole context,
then why not!? If you have enough memory for a high-resolution image, why
compress it into a JPEG? But Mamba-style architectures are likely to hugely
outperform with long-range sequences.¬†&lt;a href=&quot;#fnref:seq_len&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:cuda&quot; role=&quot;doc-endnote&quot;&gt;

      &lt;p&gt;More details are available for engineers interested in CUDA programming -
&lt;a href=&quot;https://www.youtube.com/watch?v=foG0ebzuw34&amp;amp;list=PLDEUW02OCkqGFMLHEpET24ArjE0By8JwS&amp;amp;index=9&amp;amp;pp=gAQBiAQB&quot;&gt;Tri‚Äôs talk&lt;/a&gt;,
Mamba paper section &lt;strong&gt;3.3.2&lt;/strong&gt;, and the
&lt;a href=&quot;https://github.com/state-spaces/mamba/tree/main/csrc/selective_scan&quot;&gt;official CUDA code&lt;/a&gt;
are good resources for understanding the Hardware-Aware Scan¬†&lt;a href=&quot;#fnref:cuda&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:oop&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;or in Object Oriented Programming¬†&lt;a href=&quot;#fnref:oop&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:politik&quot; role=&quot;doc-endnote&quot;&gt;

      &lt;p&gt;Implications to actual Political Economy are left to the reader but maybe Gu
and Dao accidentally solved politics!?¬†&lt;a href=&quot;#fnref:politik&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:analogy&quot; role=&quot;doc-endnote&quot;&gt;

      &lt;p&gt;This isn‚Äôt a perfect analogy as human evolution follows a genetic algorithm
rather than SGD.¬†&lt;a href=&quot;#fnref:analogy&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:hard-drive&quot; role=&quot;doc-endnote&quot;&gt;

      &lt;p&gt;Albeit a pretty weird hard drive at that - it morphs over time rather than
being a fixed representation.¬†&lt;a href=&quot;#fnref:hard-drive&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:ssd&quot; role=&quot;doc-endnote&quot;&gt;

      &lt;p&gt;As a backronym, I‚Äôve started calling the hidden_state the state space
dimension (or selective state dimension) which shortens to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;SSD&lt;/code&gt;, a nice
reminder for what this object represents - the long-term memory of the
system.¬†&lt;a href=&quot;#fnref:ssd&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:steering&quot; role=&quot;doc-endnote&quot;&gt;

      &lt;p&gt;I‚Äôm thinking about this similarly to the relationship between harmlessness
finetuning and activation steering. State swapping, like activation
steering, is an inference time intervention giving comparable results to its
train time analogue.¬†&lt;a href=&quot;#fnref:steering&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:movie&quot; role=&quot;doc-endnote&quot;&gt;

      &lt;p&gt;This is a very non-trivial problem! How do human brains represent a movie
internally? It‚Äôs not a series of the most salient frames, nor is it a text
summary of the colours, nor is it a purely vibes-based summary if you can
memorise some lines of the film.¬†&lt;a href=&quot;#fnref:movie&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:safe&quot; role=&quot;doc-endnote&quot;&gt;

      &lt;p&gt;They‚Äôre also safer since they inherently understand (though don‚Äôt
necessarily embody) human values. It‚Äôs not all clear that how to teach an RL
agent human morality.¬†&lt;a href=&quot;#fnref:safe&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:image&quot; role=&quot;doc-endnote&quot;&gt;

      &lt;p&gt;Note that typically an image (i.e. a single frame) counts as &amp;gt;196 tokens,
and movies are typically 24 fps so you‚Äôll fill a 32k context window in 7
seconds ü§Ø¬†&lt;a href=&quot;#fnref:image&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:optimisation&quot; role=&quot;doc-endnote&quot;&gt;

      &lt;p&gt;Another possibility that I‚Äôm excited about is applying optimisation pressure
to the state itself as well as the output to have models that respect
particular use cases.¬†&lt;a href=&quot;#fnref:optimisation&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:other-models&quot; role=&quot;doc-endnote&quot;&gt;

      &lt;p&gt;This is slightly hyperbolic, the TS-Mixer for time series, Gradient Boosting
Trees for tabular data and Graph Neural Networks for weather prediction
exist and are currently used, but these aren‚Äôt at the core of AI¬†&lt;a href=&quot;#fnref:other-models&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;</content><author><name></name></author><category term="machine-learning" /><category term="state-space-models" /><category term="mamba" /><category term="ssm" /><summary type="html">The State Space Model taking on Transformers</summary></entry><entry><title type="html">The Impact of Mixtral</title><link href="http://localhost:4000/blog/2024/01/14/mixtral.html" rel="alternate" type="text/html" title="The Impact of Mixtral" /><published>2024-01-14T00:00:00+00:00</published><updated>2024-01-14T00:00:00+00:00</updated><id>http://localhost:4000/blog/2024/01/14/mixtral</id><content type="html" xml:base="http://localhost:4000/blog/2024/01/14/mixtral.html">&lt;h3 id=&quot;can-you-feel-the-moe&quot;&gt;Can You Feel The MoE?&lt;/h3&gt;

&lt;div align=&quot;center&quot;&gt;
  &lt;figure&gt;
    &lt;img src=&quot;/blog/images/mixtral/mistral_logo.png&quot; width=&quot;800&quot; alt=&quot;Mistral Logo&quot; /&gt;
    &lt;figcaption&gt;Love a little WordArt throwback&lt;/figcaption&gt;
    &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;Since the infamous
&lt;a href=&quot;https://x.com/MistralAI/status/1706877320844509405?s=20&quot;&gt;BitTorrent link launch&lt;/a&gt;
of Mixtral, Mistral‚Äôs Mixture of Expert (MoE) model, there‚Äôs been renewed
attention&lt;sup id=&quot;fnref:1&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:1&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt; paid to MoE models.&lt;/p&gt;

&lt;p&gt;This week, Mistral released the &lt;a href=&quot;https://arxiv.org/pdf/2401.04088.pdf&quot;&gt;paper&lt;/a&gt;
accompanying the &lt;a href=&quot;https://huggingface.co/mistralai/Mixtral-8x7B-v0.1&quot;&gt;model&lt;/a&gt;.
This feels like a great time to dig into the details of the Mixtral model and
the impact that it‚Äôs having on the MoE and LLM communities so far.&lt;/p&gt;

&lt;h2 id=&quot;mixtral-and-the-moe-paradigm&quot;&gt;Mixtral and the MoE paradigm&lt;/h2&gt;

&lt;p&gt;We discussed the intuition behind MoE models in
&lt;a href=&quot;https://www.kolaayonrinde.com/blog/2023/10/22/moe-analogy.html&quot;&gt;An Analogy for Understanding Mixture of Expert Models&lt;/a&gt;:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;In &lt;a href=&quot;https://arxiv.org/pdf/2101.03961.pdf&quot;&gt;Sparse Mixture of Experts&lt;/a&gt; (MoEs),
we swap out the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;MLP layers&lt;/code&gt; of the vanilla transformer for an &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Expert Layer&lt;/code&gt;.
The Expert Layer is made up of multiple MLPs called ‚ÄúExperts‚Äù. For each input
we select one expert to send that input to. In this way, each token has
different parameters applied to it. A dynamic routing mechanism decides how to
map tokens to Experts.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;div align=&quot;center&quot;&gt;
  &lt;figure&gt;
    &lt;img src=&quot;/blog/images/mixtral/switch_transformers.png&quot; width=&quot;800&quot; alt=&quot;Switch Transformer&quot; /&gt;
    &lt;figcaption&gt;Sparse Expert Layer
&lt;a href=&quot;https://arxiv.org/pdf/2101.03961.pdf&quot;&gt; Switch Transformer&lt;/a&gt; &lt;/figcaption&gt;
    &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;This approach gives models more parameters &lt;sup id=&quot;fnref:2&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:2&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;2&lt;/a&gt;&lt;/sup&gt; without requiring more compute
or latency for each forward pass. MoE models also typically have better sample
efficiency - that is, their performance improves much faster than dense
transformers in training, when given the same amount of compute. This isn‚Äôt
quite a free lunch because it requires more memory to store the model for
inference, but, if you have enough memory, it‚Äôs pretty great.&lt;/p&gt;

&lt;p&gt;Mixtral 8x7B has the backbone of Mistral-7B (their previous model). As with
Mistral-7B, Mixtral uses Group Query Attention and Sliding Window Attention. The
main changes are a 32k context window out of the box and replacing the Feed
Forward Networks (FFNs) with Mixture of Expert (MoE) layers.&lt;/p&gt;

&lt;p&gt;Mixtral opts for an MoE layer with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;8 FFN experts&lt;/code&gt; which are sparsely activated
by choosing the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;top 2&lt;/code&gt; at each layer. &lt;sup id=&quot;fnref:3&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:3&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;3&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;

&lt;p&gt;Having &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;8 experts&lt;/code&gt; means that where the original Mistral had a single FFN per
transformer block, Mixtral has 8 separate FFNs. &lt;sup id=&quot;fnref:4&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:4&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;4&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;

&lt;p&gt;Rather than each token rather than being processed by all the parameters, a
routing network dynamically selects the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;top 2&lt;/code&gt; experts for each token depending
on the content of the token itself. Hence, though the total parameter count is
47B, the ‚Äúactive‚Äù parameter count (i.e. the number of parameters used for each
forward pass) comes in at 13B. &lt;sup id=&quot;fnref:params&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:params&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;5&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;

&lt;p&gt;Succinctly an MoE layer is given as:&lt;/p&gt;

&lt;p&gt;$$
\displaystyle \sum_{i=0}^{n-1}G(x)_i \cdot E_i(x),
$$&lt;/p&gt;

&lt;p&gt;where G is a gating function which is 0 everywhere except at 2 indices and where
each $$E_i$$ is a single expert FFN. Note that in the above formula since most
of the entries of the sum are zeros (as G(x) is zero for most i), we only have
to compute some of the $$E_is$$ rather than all of them. This is where MoEs have
computational efficiency advantages over using bigger models or using an
ensemble of models.&lt;/p&gt;

&lt;p&gt;There is an MoE layer in each of the transformer blocks (32 in this case) and
hence we do this routing procedure 32 times for each forward pass. In a
traditional ensemble model, N (8 in this case) models have their predictions
averaged, so there are 8 token paths. We can compare the number of possible
paths that each token could take in an MoE to these ensemble methods:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;At each layer we choose 2 of the 8 experts to process our token. There are
$$\binom{8}{2}$$ = 28 ways to do this. And this happens at each of the 32
layers giving $$28^{32}$$ possible paths overall, which is huge&lt;sup id=&quot;fnref:5&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:5&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;6&lt;/a&gt;&lt;/sup&gt;! ü§Ø The
variety of possible paths here points towards increasingly Adaptive
Computation in models. In Adaptive Computation, we consider models which
handle different tokens with different parameters and different amounts of
compute.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;div align=&quot;center&quot;&gt;
  &lt;figure&gt;
    &lt;img src=&quot;/blog/images/mixtral/moe_layers_sketch.png&quot; width=&quot;800&quot; alt=&quot;MoE layers&quot; /&gt;
    &lt;figcaption&gt;A diagram showing an example path that a token could take for the first two layers&lt;/figcaption&gt;
    &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;Up until now there have been a two barriers to truly performant and stably
trainable MoEs:&lt;/p&gt;

&lt;h4 id=&quot;problem-1-training-moes-properly-from-a-mathematical-perspective&quot;&gt;Problem 1: Training MoEs properly from a mathematical perspective&lt;/h4&gt;

&lt;p&gt;MoE models have an inherently discrete step, the hard routing, and this
typically harms the gradient flow. Typically we want fully differentiable
functions for backprop and MoEs aren‚Äôt even continuous! Considering
mathematically plausible approximations to the true gradient can hugely improve
MoE training. Recent approaches like
&lt;a href=&quot;https://arxiv.org/pdf/2310.00811.pdf&quot;&gt;Sparse Backpropagation&lt;/a&gt; and
&lt;a href=&quot;https://arxiv.org/abs/2308.00951&quot;&gt;Soft MoE for encoders&lt;/a&gt; provide better
gradient flow and hence more performant models.&lt;/p&gt;

&lt;h4 id=&quot;problem-2-training-moes-efficiently&quot;&gt;Problem 2: Training MoEs efficiently&lt;/h4&gt;

&lt;p&gt;Compared to their FLOP-class, MoEs are larger models. Their size means that
there are real benefits to effective parallelisation and minimising
communication costs. Many frameworks such as
&lt;a href=&quot;https://arxiv.org/pdf/2201.05596.pdf&quot;&gt;DeepSpeed MoE&lt;/a&gt; now support MoE training
in a fairly hardware efficient way.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;Having overcome both of these issues, we‚Äôre now ready to use MoEs more in
practise.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;notes-on-mixtrals-paper&quot;&gt;Notes on Mixtral‚Äôs paper&lt;/h2&gt;

&lt;h3 id=&quot;evals&quot;&gt;Evals&lt;/h3&gt;

&lt;p&gt;The Mixtral base model outperforms popular (and larger) models like Llama 2 70B,
Gemini Pro and GPT-3.5 on most benchmarks. Note that these models are not only
larger in total parameter count but are also larger in active parameter count
too!&lt;/p&gt;

&lt;p&gt;At the time of writing, Mixtral is the best open-source model and the 3rd best
Chat model, only beaten by GPT-4 and Claude 2.0 variants.&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;
  &lt;figure&gt;
    &lt;img src=&quot;/blog/images/mixtral/llm_leaderboard.png&quot; width=&quot;800&quot; alt=&quot;LLM Leaderboard&quot; /&gt;
    &lt;figcaption&gt;&lt;/figcaption&gt;
    &lt;/figure&gt;
&lt;/div&gt;

&lt;h3 id=&quot;context-window&quot;&gt;Context Window&lt;/h3&gt;

&lt;p&gt;Mixtral shows impressive use of its whole 32k context window. The model has
relatively good recall even for mid-context tokens.&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;
  &lt;figure&gt;
    &lt;img src=&quot;/blog/images/mixtral/context_window.png&quot; width=&quot;800&quot; alt=&quot;Graph of Long Context Performance&quot; /&gt;
    &lt;!-- &lt;figcaption&gt;Mixtral maintains good performance across its context window and seems to effectively use all of the context&lt;/figcaption&gt; --&gt;
    &lt;/figure&gt;
&lt;/div&gt;

&lt;h3 id=&quot;instruction-fine-tuning&quot;&gt;Instruction Fine-Tuning&lt;/h3&gt;

&lt;p&gt;Along with the base model, Mistral also released Instruction Fine-Tuned Chat and
Assistant models. For alignment, they opted for Direct Preference Optimisation
(DPO) which is proving to be a powerful and less finicky alternative to the
traditional RLHF. &lt;sup id=&quot;fnref:lambert&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:lambert&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;7&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;

&lt;h3 id=&quot;interpretability&quot;&gt;Interpretability&lt;/h3&gt;

&lt;p&gt;One hypothesis about MoEs is that some experts might specialise in a particular
domain (e.g. mathematics, biology, code, poetry etc.). This hypothesis is an old
one which has consistently been shown to be mistaken in the literature. Here,
the authors confirm, as in previous MoE papers, there is little difference in
the distribution of experts used for different domains (although they report
being surprised by this finding!). Often experts seem to specialise
syntactically (e.g. an expert for punctuation or whitespace), rather than
semantically (an expert for neuroscience). &lt;sup id=&quot;fnref:specialisation&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:specialisation&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;8&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;

&lt;p&gt;Although the distribution of experts is fairly uniform overall, interestingly
two adjacent tokens are much more likely to be processed by the same expert,
than we might naively predict. In other words, once an expert sees one token,
it‚Äôs quite likely to also see the next one - experts like to alley-oop
themselves! üèÄ&lt;/p&gt;

&lt;p&gt;This recent &lt;a href=&quot;https://arxiv.org/pdf/2312.17238.pdf&quot;&gt;paper&lt;/a&gt; details ways to
exploit this alley-oop property by caching the recently used expert weights in
fast memory.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;As I‚Äôve noted
&lt;a href=&quot;https://www.kolaayonrinde.com/blog/2023/11/03/dictionary-learning.html#whatsnext:~:text=%F0%9F%94%B3-,Modularity,-As%20mentioned%20above&quot;&gt;previously&lt;/a&gt;,
I‚Äôm excited about the explicit modularity in MoE models for increased
interpretability&lt;/em&gt;.&lt;/p&gt;

&lt;h3 id=&quot;notable-omissions&quot;&gt;Notable omissions&lt;/h3&gt;

&lt;p&gt;There‚Äôs little information in the paper about expert balancing techniques. Many
different auxiliary losses have been proposed for expert balancing and it would
be cool to see which loss function Mistral found to work well at this scale.&lt;/p&gt;

&lt;p&gt;The authors are also quite hush about the pretrain, instruction or feedback
datasets used to train the model. Given the impressive performance, it‚Äôs quite
likely that there‚Äôs some secret sauce in the dataset compilation and filtering.
It seems increasingly likely that data will be a moat for Foundation Model
providers &lt;sup id=&quot;fnref:moat&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:moat&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;9&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;mixtral-in-the-wild&quot;&gt;Mixtral in the wild&lt;/h2&gt;

&lt;h3 id=&quot;impact-for-on-device-llms&quot;&gt;Impact for On Device LLMs&lt;/h3&gt;

&lt;p&gt;MoEs win by having increased performance with faster inference. Founder Sharif
Shameem &lt;a href=&quot;https://x.com/sharifshameem/status/1734470299314459108?s=20&quot;&gt;writes&lt;/a&gt;,
‚ÄúThe Mixtral MoE model genuinely feels like an inflection point ‚Äî a true GPT-3.5
level model that can run at 30 tokens/sec on an M1 MacBook Pro. Imagine all the
products now possible when inference is 100% free and your data stays on your
device!‚Äù&lt;/p&gt;

&lt;p&gt;Indeed since the launch of Mixtral, it‚Äôs been used in many applications from
&lt;em&gt;the enterprise&lt;/em&gt; to &lt;em&gt;local chatbots&lt;/em&gt; to &lt;em&gt;DIY Home Assistants √† la Siri&lt;/em&gt;.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;As many people use MoE models on-device for the first time, I expect that we
will start to see more methods which speed up MoE inference. The
&lt;a href=&quot;https://arxiv.org/pdf/2312.17238.pdf&quot;&gt;Fast MoE Inference paper&lt;/a&gt; and MoE
specific quantisation like &lt;a href=&quot;https://arxiv.org/pdf/2310.16795.pdf&quot;&gt;QMoE&lt;/a&gt; are all
great steps in this direction.&lt;/p&gt;

&lt;p&gt;In particular, Quantization can be thought of as storing a model compressed like
we do for audio in MP3s. We degrade the quality model slightly and get massive
decreases in the memory that it requires. We can typically quantise MoEs even
more aggressively than dense models and retain strong performance.&lt;/p&gt;

&lt;h3 id=&quot;impact-for-foundation-model-companies&quot;&gt;Impact for Foundation Model Companies&lt;/h3&gt;

&lt;p&gt;Mistral was only started a matter of months ago with a super lean team and is
already SOTA for Open Source models. This is impressive from their team but it
may also suggest that Foundation Models are being commodified real quick.&lt;/p&gt;

&lt;p&gt;Originally Mistral were offering Mixtral behind their API for \$1.96 per
million tokens. Considering GPT-4 is $10-30 at the time of writing, this seemed
fair for a hosted API. Within days different inference providers undercut
Mistral
&lt;a href=&quot;https://twitter.com/JosephJacks_/status/1735756308496667101&quot;&gt;significantly&lt;/a&gt;:&lt;/p&gt;

&lt;blockquote class=&quot;twitter-tweet&quot;&gt;&lt;p lang=&quot;en&quot; dir=&quot;ltr&quot;&gt;Last week &lt;a href=&quot;https://twitter.com/MistralAI?ref_src=twsrc%5Etfw&quot;&gt;@MistralAI&lt;/a&gt; launched pricing for the Mixtral MoE: $2.00~ / 1M tokens.&lt;br /&gt;&lt;br /&gt;Hours later &lt;a href=&quot;https://twitter.com/togethercompute?ref_src=twsrc%5Etfw&quot;&gt;@togethercompute&lt;/a&gt; took the weights and dropped pricing by 70% to $0.60 / 1M.&lt;br /&gt;&lt;br /&gt;Days later &lt;a href=&quot;https://twitter.com/abacusai?ref_src=twsrc%5Etfw&quot;&gt;@abacusai&lt;/a&gt; cut 50% deeper to $0.30 / 1M.&lt;br /&gt;&lt;br /&gt;Yesterday &lt;a href=&quot;https://twitter.com/DeepInfra?ref_src=twsrc%5Etfw&quot;&gt;@DeepInfra&lt;/a&gt; went to $0.27 / 1M.&lt;br /&gt;&lt;br /&gt;Who‚Äôs next ??? üìâ&lt;/p&gt;&amp;mdash; JJ ‚Äî oss/acc (@JosephJacks_) &lt;a href=&quot;https://twitter.com/JosephJacks_/status/1735756308496667101?ref_src=twsrc%5Etfw&quot;&gt;December 15, 2023&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async=&quot;&quot; src=&quot;https://platform.twitter.com/widgets.js&quot; charset=&quot;utf-8&quot;&gt;&lt;/script&gt;

&lt;p&gt;There was even one provider who was giving away tokens &lt;em&gt;for free&lt;/em&gt;. I know a race
to the bottom when I see one‚Ä¶&lt;/p&gt;

&lt;p&gt;The consumer/developer is truly winning here but it reiterates the point that
Foundation Model companies should expect the value of tokens to fall
&lt;em&gt;dramatically&lt;/em&gt;.
&lt;a href=&quot;https://www.amazon.co.uk/Zero-One-Notes-Start-Future/dp/0753555204&quot;&gt;Competition is for Losers&lt;/a&gt;,
as Peter Thiel might say; it‚Äôs very possible to compete away all the profits to
zero. &lt;sup id=&quot;fnref:altman&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:altman&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;10&lt;/a&gt;&lt;/sup&gt;. It increasingly looks like most of the value captured from an
LLM business perspective will likely be in the application layer (e.g.
Perplexity, Copilot) and the infrastructure layer (e.g. AWS/Azure).&lt;/p&gt;

&lt;h3 id=&quot;impact-for-the-scientific-community&quot;&gt;Impact for the Scientific Community&lt;/h3&gt;

&lt;p&gt;Mixtral is a huge win for the scientific and interpretability communities. We
now finally have a model which is comfortably better than GPT3.5 and whose
weights are freely available to researchers.&lt;/p&gt;

&lt;p&gt;In addition, given Mixtral shares the same backbone as the previous Mistral 7B,
it seems plausible some weights were re-used as initialisations for Mixtral.
This approach is known in the literature as
&lt;a href=&quot;https://arxiv.org/pdf/2212.05055.pdf&quot;&gt;Sparse Upcycling&lt;/a&gt;. If Sparse Upcycling
works, this suggests that the compute required to make great MoE models might be
much less than previous thought. Researchers can take advantage of existing
models like Llama 2 etc. rather than having to pretrain entirely from scratch,
which completely changes which projects are feasible for academics and the
GPU-poor.&lt;/p&gt;

&lt;h4 id=&quot;open-source-ml-in-the-age-of-adaptive-computation&quot;&gt;Open Source ML in the Age of Adaptive Computation&lt;/h4&gt;

&lt;p&gt;‚ÄúIn 2012 we were detecting cats and dogs and in 2022 we were writing human-like
poetry, generating beautiful and novel imagery, solving the protein folding
problem and writing code. Why is that?‚Äù&lt;/p&gt;

&lt;p&gt;Arthur Mensch, Mistral co-founder, suggests most of the reason is ‚Äúthe free flow
of information. You had academic labs [and] very big industry labs communicating
all the time about their results and building on top of others‚Äô results. That‚Äôs
the way we [significantly improved] the architecture and training techniques. We
made everything work as a community‚Äù.&lt;/p&gt;

&lt;p&gt;We‚Äôre not at the end of the ML story just yet. There‚Äôs still science to be done
and inventions to be discovered so we still need the free flow of information.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;In this house we love Open Source models and papers. ü§ó&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Expect MoEs to become even more important for 2024. The age of
&lt;a href=&quot;https://github.com/koayon/awesome-adaptive-computation&quot;&gt;Adaptive Computation&lt;/a&gt;
is here.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;br /&gt;&lt;/p&gt;
&lt;div class=&quot;footnotes&quot; role=&quot;doc-endnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:1&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;If I may¬†&lt;a href=&quot;#fnref:1&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:2&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;which is more knowledge in some sense¬†&lt;a href=&quot;#fnref:2&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:3&quot; role=&quot;doc-endnote&quot;&gt;

      &lt;p&gt;This is a slight reversal of recent work as many papers had followed the
Switch Transformer in only choosing the top 1 expert per layer. We expect
that choosing 2 experts allows for more expressivity, more stable training
and better gradient flow which is traded off against increased parallel
computation in each forward pass.¬†&lt;a href=&quot;#fnref:3&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:4&quot; role=&quot;doc-endnote&quot;&gt;

      &lt;p&gt;Early MoEs like the Switch Transformer were using 100s of experts per layer.
This always seemed a little excessive and working on these models, a good
heuristic for choosing the expert number hyperparameter is either the number
of experts that will fit into your single GPU memory if you‚Äôre mostly doing
single batch inference or the number of GPUs that you could do expert
parallelism on at inference time if you‚Äôre running a high bandwidth API.
With this in mind 8 experts seems like a nice middle ground right now for
users of an open-source product.¬†&lt;a href=&quot;#fnref:4&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:params&quot; role=&quot;doc-endnote&quot;&gt;

      &lt;p&gt;This is slightly less than 8x7 =56 total parameters as because attention and
embedding parameters are &lt;em&gt;not&lt;/em&gt; duplicated).¬†&lt;a href=&quot;#fnref:params&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:5&quot; role=&quot;doc-endnote&quot;&gt;

      &lt;p&gt;In fact this is quite the understatement, all of these paths are weighted
according to the router logits so there‚Äôs even more nuance than this in the
possible paths that tokens can take.¬†&lt;a href=&quot;#fnref:5&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:lambert&quot; role=&quot;doc-endnote&quot;&gt;

      &lt;p&gt;Nathan Lambert has a great explainer on DPO
&lt;a href=&quot;https://www.interconnects.ai/p/the-dpo-debate&quot;&gt;here&lt;/a&gt;¬†&lt;a href=&quot;#fnref:lambert&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:specialisation&quot; role=&quot;doc-endnote&quot;&gt;

      &lt;p&gt;It may also be the case that experts indeed do specialise semantically but
that their natural semantic specialisation is not very clear to human
researchers¬†&lt;a href=&quot;#fnref:specialisation&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:moat&quot; role=&quot;doc-endnote&quot;&gt;

      &lt;p&gt;at least for companies that don‚Äôt produce applications built on top of the
models.¬†&lt;a href=&quot;#fnref:moat&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:altman&quot; role=&quot;doc-endnote&quot;&gt;

      &lt;p&gt;Sam Altman has colourfully referred to this as the marginal cost of
intelligence going to zero¬†&lt;a href=&quot;#fnref:altman&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;</content><author><name></name></author><category term="machine-learning" /><category term="mixture-of-experts" /><category term="adaptive-computation" /><summary type="html">Can You Feel The MoE?</summary></entry><entry><title type="html">Descriptive Matrix Operations with Einops</title><link href="http://localhost:4000/blog/2024/01/08/einops.html" rel="alternate" type="text/html" title="Descriptive Matrix Operations with Einops" /><published>2024-01-08T00:00:00+00:00</published><updated>2024-01-08T00:00:00+00:00</updated><id>http://localhost:4000/blog/2024/01/08/einops</id><content type="html" xml:base="http://localhost:4000/blog/2024/01/08/einops.html">&lt;h4 id=&quot;tldr-use-einopseinsum&quot;&gt;tldr; use einops.einsum&lt;/h4&gt;

&lt;p&gt;Machine learning is built of matrix algebra. Einstein summation notation (or
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;einsum&lt;/code&gt; for short) makes matrix operations more intuitive and readable.&lt;/p&gt;

&lt;p&gt;As you may know, the matrix multiplication that you learned in high school‚Ä¶&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;
  &lt;figure&gt;
    &lt;img src=&quot;/blog/images/einops/matmul.png&quot; width=&quot;500&quot; alt=&quot;3x3 matrix multiplication&quot; /&gt;
    &lt;figcaption&gt;Calculating the 0,0th element of a matrix multiplication &lt;/figcaption&gt;
    &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;Can be written algebraically as:&lt;/p&gt;

&lt;p&gt;$$A_{ik} = \sum_j B_{ij} C_{jk}$$&lt;/p&gt;

&lt;p&gt;In other words in order to get the (1,2) element of A we calculate:&lt;/p&gt;

&lt;p&gt;$$A_{1,2} = \sum_j B_{1j} C_{j2}$$&lt;/p&gt;

&lt;p&gt;i.e. take the dot product of the 1st row of B with the 2nd column of C.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;In Einsum notation, to avoid having so many sigmas ($$\sum$$) flying around we
adopt the convention that any indices that appear more than once are being
summed over. Hence:&lt;/p&gt;

&lt;p&gt;$$A_{ik} = \sum_j B_{ij} C_{jk}$$&lt;/p&gt;

&lt;p&gt;can be written more simply as‚Ä¶&lt;/p&gt;

&lt;p&gt;$$A_{ik} = B_{ij} C_{jk}$$&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;Both torch and numpy have einsum packages to allow you to use einsum notation
for matrix operations. For example, we can write the above matrix multiplication
in torch as:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;torch&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;t&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;A&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;t&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;einsum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;ij,jk-&amp;gt;ik&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;B&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;C&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The convention is that if a dimension only appears on the left side of the
einsum then it‚Äôs summed over. So in the above we‚Äôre summing over the j dimension
and keeping the i and k dimensions. That‚Äôs our classic matrix multiplication
written in torch einsum notation&lt;sup id=&quot;fnref:chef&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:chef&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;

&lt;p&gt;Great!&lt;/p&gt;

&lt;p&gt;One issue when using torch.einsum though is that it‚Äôs not necessarily super
clear what each letter means:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Was &lt;strong&gt;i&lt;/strong&gt; a horizontal index (as in x,y coordinates) or is it a vertical index
(as in tensor indexing?)&lt;/li&gt;
  &lt;li&gt;Was &lt;strong&gt;e&lt;/strong&gt; embedding dimension or expert number?&lt;/li&gt;
  &lt;li&gt;Was &lt;strong&gt;h&lt;/strong&gt; height, head dimension or hidden dimension?&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;To get around this ambiguity, it‚Äôs common to see PyTorch code where in the
docstring each of the letters is defined. This isn‚Äôt a very natural pattern -
it‚Äôs like if all of your variable names in code had to be single letters and you
had another file which would act as a dictionary for what each letter actually
meant! &lt;em&gt;shudders&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;One of the most useful lines of the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Zen of Python&lt;/code&gt; is
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Explicit is better than Implicit&lt;/code&gt;. Following this principle, we would like to
be able to write the variable names in the einsum string itself. Without this,
it‚Äôs harder to read and means you‚Äôre always looking back when trying to
understand or debug the code.&lt;/p&gt;

&lt;h3 id=&quot;enter-einops&quot;&gt;Enter einops&lt;/h3&gt;

&lt;p&gt;Einops is a tensor manipulation package that can be used with PyTorch, NumPy,
Tensorflow and Jax. It offers a nice API but we‚Äôll focus on einsums which we can
now use with full variable names rather than single letters! It makes your ML
code so much clearer instantly.&lt;/p&gt;

&lt;p&gt;For example let‚Äôs write the
&lt;a href=&quot;https://paperswithcode.com/method/multi-query-attention&quot;&gt;multi-query attention&lt;/a&gt;
operation.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;
  &lt;figure&gt;
    &lt;img src=&quot;/blog/images/einops/multi-query-attention.png&quot; width=&quot;800&quot; alt=&quot;Multi Query Attention&quot; /&gt;
    &lt;figcaption&gt;Multi-Query Attention is a type of attention where we have multiple query heads (like in Multi-Head Attention) but only a single key and value head per layer&lt;/figcaption&gt;
    &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;torch&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;t&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;einops&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;einsum&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;multi_query_attention&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Q&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;t&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Tensor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;K&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;t&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Tensor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;V&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;t&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Tensor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;t&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Tensor&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;head_dim&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;K&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;attn_scores&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;einsum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Q&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;K&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
        &lt;span class=&quot;s&quot;&gt;&quot;batch head seq1 head_dim, batch seq2 head_dim -&amp;gt; batch head seq1 seq2&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;attn_matrix&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;t&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;softmax&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;attn_scores&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;head_dim&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;**&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;out&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;  &lt;span class=&quot;n&quot;&gt;einsum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;attn_matrix&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;V&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
        &lt;span class=&quot;s&quot;&gt;&quot;batch head seq1 seq2, batch seq2 head_dim -&amp;gt; batch head seq1 head_dim&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;out&lt;/span&gt;

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;em&gt;One catch here is that we want to have the sequence length represented twice
for&lt;/em&gt; $$QK^T$$ &lt;em&gt;but we don‚Äôt want to sum over it. To solve this we give them two
different names like &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;seq1&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;seq2&lt;/code&gt;&lt;/em&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;The nice things about this are that we didn‚Äôt need to write a glossary for what
random variables &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;b&lt;/code&gt; or &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;h&lt;/code&gt; were supposed to mean, we can just read it off.&lt;/p&gt;

&lt;p&gt;Also note that typically when computing attention, we need to calculate
$$QK^T$$. Here we didn‚Äôt need to worry about how exactly to take the transpose -
we just give the dimension names and the correct transposes are done for the
multiplication to make sense!&lt;/p&gt;

&lt;p&gt;Einops also offers great functions for rearranging, reducing and repeating
tensors which are also very useful.&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;
  &lt;figure&gt;
    &lt;img src=&quot;/blog/images/einops/the_world_if_einops.jpg&quot; width=&quot;500&quot; alt=&quot;The World If Everyone Used Einops&quot; /&gt;
    &lt;figcaption&gt;&lt;/figcaption&gt;
    &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;That‚Äôs all! Just trying to make those inscrutable matrix multiplications, a
little more scrutable. Ôøº&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;br /&gt;&lt;/p&gt;
&lt;div class=&quot;footnotes&quot; role=&quot;doc-endnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:chef&quot; role=&quot;doc-endnote&quot;&gt;

      &lt;p&gt;I feel like a fancy chef here. For our appetiser we have &lt;em&gt;Matrix
Multiplication Done Four Ways&lt;/em&gt; and so on‚Ä¶¬†&lt;a href=&quot;#fnref:chef&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;</content><author><name></name></author><category term="machine-learning" /><category term="pytorch" /><summary type="html">tldr; use einops.einsum</summary></entry><entry><title type="html">Dictionary Learning with Sparse AutoEncoders</title><link href="http://localhost:4000/blog/2023/11/03/dictionary-learning.html" rel="alternate" type="text/html" title="Dictionary Learning with Sparse AutoEncoders" /><published>2023-11-03T00:00:00+00:00</published><updated>2023-11-03T00:00:00+00:00</updated><id>http://localhost:4000/blog/2023/11/03/dictionary-learning</id><content type="html" xml:base="http://localhost:4000/blog/2023/11/03/dictionary-learning.html">&lt;h4 id=&quot;taking-features-out-of-superposition&quot;&gt;Taking Features Out of Superposition&lt;/h4&gt;

&lt;h2 id=&quot;mechanistic-interpretability&quot;&gt;Mechanistic Interpretability&lt;/h2&gt;

&lt;p&gt;Given a task that we don‚Äôt know how to solve directly in code (e.g. recognising
a cat or writing a unique sonnet), we often write programs which in turn, (via
&lt;a href=&quot;https://en.wikipedia.org/wiki/Stochastic_gradient_descent&quot;&gt;SGD&lt;/a&gt;), write
second-order programs. These second-order programs (i.e. neural network weights)
can solve the task, given lots of data.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;Suppose we have some neural network weights which describe how to do a task. We
might want to know how the network solved the task. This is useful either to (1)
understand the algorithm better for ourselves or (2) check if the algorithm
follows some guidelines we might like e.g. not being deceptive, not invoking
harmful bias etc.&lt;/p&gt;

&lt;p&gt;The field of Mechanistic Interpretability aims to do just that - given a neural
network&lt;sup id=&quot;fnref:1&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:1&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt;, return a &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;correct&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;parsimonious&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;faithful&lt;/code&gt; and
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;human-understandable&lt;/code&gt; explanation of the inner workings of the network when
solving a given task. This is analogous to the problem of
&lt;a href=&quot;https://www.neelnanda.io/mechanistic-interpretability/reverse-engineering&quot;&gt;reverse engineering software from machine code&lt;/a&gt;
or the problem of a
&lt;a href=&quot;https://colah.github.io/notes/interp-v-neuro/&quot;&gt;neuroscientist trying to understand the human brain&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;-the-dream&quot;&gt;üí≠ The Dream&lt;/h3&gt;

&lt;p&gt;How are we to translate giant inscrutable matrices into neat explanations and
high-level stories?&lt;/p&gt;

&lt;p&gt;In order for a neural network to make some prediction, it uses internal neuron
activations as ‚Äúvariables‚Äù. The neuron activations build up high-level,
semantically rich concepts in later layers using lower-level concepts in earlier
layers.&lt;/p&gt;

&lt;p&gt;A dream of Mechanistic Interpretability would be this:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Suppose we had some idea that each neuron corresponded to a single feature.
For example, we could point to one neuron and say ‚Äúif that neuron activates
(or ‚Äúfires‚Äù) then the network is thinking about cats!‚Äù. Then we point to
another and say ‚Äúthe network is thinking about the colour blue‚Äù. Now we could
give a neural network some inputs, look at which internal neurons activate (or
‚Äúfire‚Äù) and use this to piece together a story about how the network came up
with its eventual prediction. This story would involve knowing the concepts
(‚Äúfeatures‚Äù) the network was ‚Äúthinking‚Äù about together with the weights
(‚Äúcircuits‚Äù) which connect them.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;This would be great! Unfortunately, there are a couple of problems here‚Ä¶&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;-the-nightmare&quot;&gt;üëª The Nightmare&lt;/h3&gt;

&lt;p&gt;Firstly, neural networks are freaking huge. There can be literally billions of
weights and activations relevant for processing a single sentence in a language
model. So with the naive approach above, it would be an incredibly difficult
practical undertaking to actually tell a good story about the network‚Äôs internal
workings&lt;sup id=&quot;fnref:2&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:2&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;2&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;

&lt;p&gt;But, secondly, and more importantly, when we look at the neurons of a neural
network we don‚Äôt see the concepts that it sees. We see a huge mess of concepts
all enmeshed together because it‚Äôs more efficient for the network to process
information in this way. Neurons that don‚Äôt activate on a single concept but
instead activate on many distinct concepts are known as &lt;strong&gt;polysemantic
neurons&lt;/strong&gt;. It turns out that basically all neurons are highly polysemantic&lt;sup id=&quot;fnref:3&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:3&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;3&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;

&lt;p&gt;In essence, neural networks have lots of &lt;strong&gt;features&lt;/strong&gt;, which are the
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;fundamental&lt;/code&gt; units (‚Äúvariables‚Äù) in neural networks. We might think of features
as directions in neuron space corresponding to the concepts. And neurons are
&lt;em&gt;linear combinations&lt;/em&gt; of these features in a way that makes sense to the network
but looks very entangled to us - we can‚Äôt just read off the features from
looking at the activations.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;sparse-dictionary-learning-as-a-solution&quot;&gt;Sparse Dictionary Learning As A Solution&lt;/h2&gt;

&lt;p&gt;So we‚Äôre given a network and we know that all the neurons are linear
combinations of the underlying features but we don‚Äôt know what the features are.
That is, we hypothesise that there is some linear map &lt;strong&gt;g&lt;/strong&gt; from the feature
space to neuron space. Generally, feature space is much bigger than neuron
space. That is to say, there are more useful concepts in language than the
number of neurons that a network has. So our map &lt;strong&gt;g&lt;/strong&gt; is a very rectangular
matrix: it takes in a large vector and outputs a smaller one with the number of
neurons as the number of dimensions.&lt;/p&gt;

&lt;p&gt;We want to recover the features. To do this we could try to find a linear
function which can map from neuron space ‚Üí feature space and acts as the inverse
of &lt;strong&gt;g&lt;/strong&gt;. We go to our Linear Algebra textbook (or ask ChatGPT) how to invert a
long rectangular matrix and it says‚Ä¶ oh wait, yeah this actually isn‚Äôt
possible&lt;sup id=&quot;fnref:4&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:4&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;4&lt;/a&gt;&lt;/sup&gt;. A general linear map from feature space ‚Üí neuron space loses
information and so cannot be inverted - we can‚Äôt recover the features given only
the neurons.&lt;/p&gt;

&lt;p&gt;This seems bad but let‚Äôs soldier on. Instead of giving up, we instead ask, ‚Äúokay
well we can‚Äôt invert a &lt;em&gt;general&lt;/em&gt; linear map &lt;strong&gt;g&lt;/strong&gt; but what constraints could we
put on &lt;strong&gt;g&lt;/strong&gt; such that it might be invertible?‚Äù As it turns out, if most of the
numbers in the matrix corresponding to &lt;strong&gt;g&lt;/strong&gt; are 0 (that is if &lt;strong&gt;g&lt;/strong&gt; is
sufficiently &lt;strong&gt;sparse&lt;/strong&gt;) then we &lt;em&gt;can&lt;/em&gt; invert &lt;strong&gt;g&lt;/strong&gt;.&lt;sup id=&quot;fnref:5&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:5&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;5&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;Q: Hold on, is this reasonable? Why might we expect &lt;strong&gt;g&lt;/strong&gt; to be (approximately)
sparse?&lt;/p&gt;

&lt;p&gt;In predicting the next token there will be some relevant features of the
previous tokens which are useful. If the neural network has tens of thousands of
features per layer (or perhaps even more), then we would expect &lt;em&gt;some&lt;/em&gt; of them
to be useful for each prediction. But if the prediction function uses all of the
features it would be super complex; most features should be irrelevant for each
prediction.&lt;/p&gt;

&lt;p&gt;As an example consider if you‚Äôre deciding if a picture of an animal is a dog -
you might ask ‚Äúdoes it have 4 legs?‚Äù - 4 legged-ness is a useful feature. The
texture of its fur is also relevant. The question ‚Äúwould a rider sit within or
on top‚Äù is probably not relevant, though it might be relevant in other
situations for example distinguishing a motorbike from a car. In this way, not
all of the features are needed at once&lt;sup id=&quot;fnref:6&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:6&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;6&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;To recap, so far we‚Äôve said:&lt;/p&gt;

  &lt;ol&gt;
    &lt;li&gt;Language models use features in order to predict the next token.&lt;/li&gt;
    &lt;li&gt;There are potentially a lot more features than there are neurons.&lt;/li&gt;
    &lt;li&gt;If the linear map &lt;strong&gt;g&lt;/strong&gt;: features ‚Üí neurons was sparse then we might be
able to find an inverse.&lt;/li&gt;
    &lt;li&gt;Sparse maps are relatively good approximations to the real linear map
&lt;strong&gt;g&lt;/strong&gt;.&lt;/li&gt;
  &lt;/ol&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;Sparse Dictionary Learning is a method which exploits these facts to numerically
find the inverse of g. Intuitively what we have is a lookup table (or a
‚Äúdictionary‚Äù) which tells us how much of each feature goes into each neuron. And
if these features look monosemantic and human-understandable then we‚Äôre getting
very close to the dream of Mechanistic Interpretability outlined above. We could
run a model, read off the features it used for the prediction and build a story
of how it works!&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;
  &lt;figure&gt;
    &lt;img src=&quot;/blog/images/dictionary-learning/projection_g.png&quot; width=&quot;800&quot; alt=&quot;Dictionary Learning as an Inverse Problem&quot; /&gt;
    &lt;figcaption&gt;Dictionary Learning can be seen as trying to find the inverse map to g. The map g is analogous to PCA - it‚Äôs the network‚Äôs way of trying to fit as much
information as possible into a lower-dimensional space. &lt;/figcaption&gt;
    &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;dictionary-learning-set-up&quot;&gt;Dictionary Learning Set-up&lt;/h3&gt;

&lt;p&gt;We‚Äôll focus here on
&lt;a href=&quot;https://transformer-circuits.pub/2023/monosemantic-features/index.html&quot;&gt;Anthropic‚Äôs set-up&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;We start with a small 1-Layer transformer which has an embedding dimension
of 128. Here the &lt;strong&gt;MLP&lt;/strong&gt; hidden dimension is 512.&lt;sup id=&quot;fnref:7&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:7&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;7&lt;/a&gt;&lt;/sup&gt; The MLP contains:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;An up_projection to MLP neuron space (512d),&lt;/li&gt;
  &lt;li&gt;A ReLU activation which produces activations and then&lt;/li&gt;
  &lt;li&gt;A down_projection back to embedding space (128d)&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;We capture the MLP neuron activations and send those through our sparse
autoencoder which has N dimensions for some N ‚â• 512.&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;
  &lt;figure&gt;
    &lt;img src=&quot;/blog/images/dictionary-learning/transformer.png&quot; width=&quot;800&quot; alt=&quot;Transformer with an AutoEncoder attached&quot; /&gt;
    &lt;figcaption&gt;The AutoEncoder set-up on a 1-Layer Transformer. The MLP activations are
captured and sent through the AutoEncoder. &lt;/figcaption&gt;
    &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;An &lt;strong&gt;AutoEncoder&lt;/strong&gt; is a model which tries to reconstruct some data after putting
it through a &lt;strong&gt;bottleneck&lt;/strong&gt;. In
&lt;a href=&quot;https://en.wikipedia.org/wiki/Autoencoder&quot;&gt;traditional autoencoders&lt;/a&gt;, the
bottleneck might be mapping to a smaller dimensional space or including noise
that the representation should be robust to. AutoEncoders aim to recreate the
original data as closely as possible despite the bottleneck. To achieve the
reconstruction, we use a reconstruction loss which penalises outputs by how much
they differ from the MLP activations (the inputs to the AutoEncoder).&lt;/p&gt;

&lt;p&gt;In the &lt;strong&gt;Sparse AutoEncoder&lt;/strong&gt; setting, our ‚Äúbottleneck‚Äù is actually a higher
dimensional space than neuron space (N ‚â• 512), but the constraint is that the
autoencoder features are &lt;strong&gt;sparse&lt;/strong&gt;. That is, for any given set of MLP neuron
activations, only a small fraction of the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;features&lt;/code&gt; should be activated.&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;
  &lt;figure&gt;
    &lt;img src=&quot;/blog/images/dictionary-learning/autoencoder.png&quot; width=&quot;800&quot; alt=&quot;Autoencoder representation&quot; /&gt;
    &lt;figcaption&gt;The AutoEncoder passes some inputs through a hidden layer (which acts as a
bottleneck) and tries to reconstruct the inputs. For a well-trained AutoEncoder,
the output vector should be approximately the input vector.&lt;/figcaption&gt;
    &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;In order to make the hidden feature activations sparse, we add an L1 loss over
the feature activations to the reconstruction loss for the AutoEncoder‚Äôs loss
function. Since the L1 loss gives the absolute value of the vector, minimising
L1 loss pushes as many as possible of the feature activations towards zero
(whilst still being able to reconstruct the MLP neurons to get low
reconstruction loss).&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;To recap:&lt;/p&gt;

  &lt;ul&gt;
    &lt;li&gt;The input of the AutoEncoder is the MLP activations.&lt;/li&gt;
    &lt;li&gt;
      &lt;p&gt;The goal is for the output of the AutoEncoder to be as close to the input as
possible - the reconstruction loss penalises outputs by how much they differ
from the MLP activation inputs.&lt;/p&gt;
    &lt;/li&gt;
    &lt;li&gt;The bottleneck is the sparsity in the hidden layer which is induced by
pressure from the L1 loss to minimise feature activations.&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;p&gt;In summary, the set-up Anthropic uses is:&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;
  &lt;figure&gt;
    &lt;img src=&quot;/blog/images/dictionary-learning/setup.png&quot; width=&quot;800&quot; alt=&quot;Dictionary Learning Setup Table&quot; /&gt;
    &lt;figcaption&gt;Anthropic&apos;s Dictionary Learning Setup&lt;/figcaption&gt;
    &lt;/figure&gt;
&lt;/div&gt;

&lt;h3 id=&quot;anthropics-results&quot;&gt;Anthropic‚Äôs Results&lt;/h3&gt;

&lt;p&gt;The most surprising thing about this approach is that it works so well. Like
&lt;em&gt;really&lt;/em&gt; well.&lt;/p&gt;

&lt;p&gt;There are, broadly, two ways to think about features:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;strong&gt;Features as Results&lt;/strong&gt; - the feature activates when it sees particular
inputs. Looking at features can help us to understand the inputs to the
model.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Features as Actions&lt;/strong&gt; - ultimately features activating leads to differences
in the output logits. Features can be seen as up-weighting or down-weighting
certain output tokens. In some sense, this is the more fundamental part. If
we ask ‚Äúwhat is a feature for?‚Äù then the answer is ‚Äúto help the model in
predicting the next token.‚Äù&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;Anthropic find many features which activate strongly in a specific context (say
Arabic script or DNA base pairs) and also (mostly) only activate when that
context is present. In other words, the features have high
&lt;a href=&quot;https://en.wikipedia.org/wiki/Precision_and_recall&quot;&gt;precision and recall&lt;/a&gt;. This
suggests that these are ~monosemantic features! In terms of
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Features as Results&lt;/code&gt;, this captures what we would hope for - the features that
appear are mostly human-understandable.&lt;/p&gt;

&lt;p&gt;The authors also find that once a feature is activated, the result is an
increase in plausible next tokens given the input. In particular, to demonstrate
this counterfactually, we can add a large amount of a given feature to the
neuron activations. Theoretically, this should ‚Äústeer‚Äù the model to thinking
that context was present in the input, even if it wasn‚Äôt. This is a great test
for &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Features as Actions&lt;/code&gt;.&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;
  &lt;figure&gt;
    &lt;img src=&quot;/blog/images/dictionary-learning/steering.png&quot; width=&quot;800&quot; alt=&quot;Diagram of steering LLM&quot; /&gt;
    &lt;figcaption&gt;Once we steer the model by adding some of the Han Chinese feature, the model
starts outputting more Han Chinese. Similarly for other identified features.
Note: this is a very small transformer so the outputs might not completely make
sense; it‚Äôs mainly recognising the context that it‚Äôs in.&lt;/figcaption&gt;
    &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;Additionally, if we fully replace the MLP activations with the output of our
autoencoder&lt;sup id=&quot;fnref:8&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:8&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;8&lt;/a&gt;&lt;/sup&gt;, we get a model which explicitly uses our feature dictionary
instead of the learned MLP neurons. Here the resulting ‚Äúdictionary model‚Äù is
able to get 95% of the performance of the regular model. The dictionary model
achieves this despite, in the case of large autoencoders, the features being
extremely sparse. This performance is a great sign for &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Features as Actions&lt;/code&gt;; it
suggests that the sparse features capture most of the information that the model
is using for its prediction task! This also validates that our assumption that
features are approximately sparse seems to be a fairly good assumption&lt;sup id=&quot;fnref:9&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:9&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;9&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;other-phenomena&quot;&gt;Other Phenomena&lt;/h3&gt;

&lt;p&gt;They also note some other smaller results:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;As the number of features in the autoencoder increases, we capture more of the
true performance of the model. This correlation suggests that models are
probably implementing up to 100x or more features than they have neurons ü§Ø&lt;/li&gt;
  &lt;li&gt;The features are generally understandable by both humans and by other Machine
Learning models. To show this they ask Claude to do some interpretation too.&lt;/li&gt;
  &lt;li&gt;As the number of features increases, the features themselves ‚Äúsplit‚Äù. That is
even though a feature is monosemantic - it activates on a single concept -
there may be levels to concepts. For example, small autoencoders might have a
(monosemantic) feature for the concept ‚Äúdog‚Äù. But larger autoencoders have
features for corgis and poodles and large dogs etc. which break down the
concept of dog into smaller chunks. Scale helps with refining concepts.&lt;/li&gt;
&lt;/ul&gt;

&lt;div align=&quot;center&quot;&gt;
  &lt;figure&gt;
    &lt;img src=&quot;/blog/images/dictionary-learning/feature_splitting.png&quot; width=&quot;800&quot; alt=&quot;FEature Splitting in LLM&quot; /&gt;
    &lt;figcaption&gt;What was once a single feature representing the &quot;the&quot; in mathematical prose gets broken down into more specific concepts as we increase the number of features in the autoencoder&lt;/figcaption&gt;
    &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;whats-next&quot;&gt;What‚Äôs Next?&lt;/h2&gt;

&lt;h4 id=&quot;have-all-the-problems-in-mechanistic-interpretability-been-solved&quot;&gt;Have All The Problems in Mechanistic Interpretability Been Solved?&lt;/h4&gt;

&lt;p&gt;Certainly not. Although this approach is a breakthrough in approaching features
and converting regular networks into less polysemantic ones, some problems
remain:&lt;/p&gt;

&lt;h3 id=&quot;-scaling-sparse-autoencoders&quot;&gt;üìà Scaling Sparse Autoencoders&lt;/h3&gt;

&lt;p&gt;Large models are still, well ‚Ä¶ large. Dictionary learning mitigates the problem
since we don‚Äôt have to deal with polysemantic neurons anymore. But there‚Äôs still
a lot that could happen between doing this on a small 1-Layer model and a large
model. In particular, since there are many more features than neurons, Sparse
AutoEncoders for large models could be absolutely gigantic and may take as much
compute to train as the model‚Äôs pre-training. We will very likely need ways to
improve the efficiency of Sparse AutoEncoder training.&lt;/p&gt;

&lt;h3 id=&quot;-compositionality-and-interaction-effects&quot;&gt;ü§ù Compositionality and Interaction Effects&lt;/h3&gt;

&lt;p&gt;In Machine Learning, as in Physics,
&lt;a href=&quot;https://www.science.org/doi/10.1126/science.177.4047.393&quot;&gt;More Is Different&lt;/a&gt;.
That is, there may be qualitatively different behaviours for large models as
compared to smaller ones. One clear way this could occur is when features are
composed of many sub-features across different layers and form complex
interactions. This is an open problem to be explored.&lt;/p&gt;

&lt;h3 id=&quot;-universality&quot;&gt;üåê Universality&lt;/h3&gt;

&lt;p&gt;The &lt;a href=&quot;https://distill.pub/2020/circuits/zoom-in&quot;&gt;Universality Hypothesis&lt;/a&gt; from
Chris Olah states that sufficiently neural networks with different architectures
and trained on different data will learn the same high-level features and
concepts.&lt;/p&gt;

&lt;p&gt;The authors show that when two models are trained with the same architecture but
different random initialisations, they learn similar features. This is certainly
a step towards universality but doesn‚Äôt show the whole thesis. A strong form of
Universality would suggest that there are some high-level ‚Äúnatural‚Äù
features/concepts which lots of &lt;em&gt;different&lt;/em&gt; architectures for predictors
(silicon and human brains) all converge on. We‚Äôre quite a way from showing this
in the general case.&lt;/p&gt;

&lt;h3 id=&quot;-interpretability-metrics&quot;&gt;üìê Interpretability Metrics&lt;/h3&gt;

&lt;p&gt;Though there are some proxy measures for interpretability, currently the best
metric that we have is for a human to check and say ‚Äúyes I can interpret this
feature‚Äù or ‚Äúno I can‚Äôt‚Äù. This seems hard to operationalise at scale as a
concrete metric.&lt;/p&gt;

&lt;p&gt;To bridge this gap large models such as GPT-4 and Claude can also help with the
interpretability. In a process known as AutoInterpret, LLMs are given a prompt
and how much each feature activates. They then attempt to interpret the feature.
This works kinda okay at the moment but it feels like there should be a cleaner,
more principled approach.&lt;/p&gt;

&lt;h3 id=&quot;Ô∏é-steering&quot;&gt;‚ò∏Ô∏é Steering&lt;/h3&gt;

&lt;p&gt;The authors show that by adding more of a given feature vector in activation
space, you can influence a model‚Äôs behaviour. When, whether, and how steering
works reliably and efficiently are questions that could all be useful. We might
wish to steer models as a surgical needle to balance out the more coarse tool
that is RLHF. In the future, this may also be useful to reduce harmful behaviour
in increasingly powerful models.&lt;/p&gt;

&lt;h3 id=&quot;-modularity&quot;&gt;üî≥ Modularity&lt;/h3&gt;

&lt;p&gt;As mentioned above, there would be an embarrassingly large number of features
for a model like &lt;a href=&quot;https://openai.com/research/gpt-4&quot;&gt;GPT-4&lt;/a&gt; and so it looks like
it will be difficult to create succinct compelling stories which involve so many
moving parts. In some sense, this is the lowest level of interpretability. It‚Äôs
analogous to trying to understand a very complex computer program by looking
through it character by character, if the words were all jumbled up.&lt;/p&gt;

&lt;p&gt;What we would like is some slightly higher level concepts composed of multiple
features with which we can use to think. Splitting up the network into
macro-modules rather than the micro-level features seems like a promising path
forward.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;Anthropic are very positive about this approach and finish their blog post with
the line:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;For the first time, we feel that the next primary obstacle to interpreting
large language models is engineering rather than science.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;There is some truth to how exciting this development is. We might ask whether
the work ahead is purely
&lt;a href=&quot;https://www.reddit.com/r/ProgrammerHumor/comments/8c1i45/stack_more_layers/&quot;&gt;scaling up&lt;/a&gt;.
As we outlined in the problems for future work above, I do believe there are
still some Science of Deep Learning problems which Mechanistic Interpretability
can sink its teeth into. Only now, we also have a new tool which is incredibly
powerful to help us along the way.&lt;/p&gt;

&lt;p&gt;In light of the other problems that still remain to be solved, we might add the
final sentences of
&lt;a href=&quot;https://redirect.cs.umbc.edu/courses/471/papers/turing.pdf&quot;&gt;Turing‚Äôs 1950 paper&lt;/a&gt;,
as an addendum:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;We can only see a short distance ahead, but we can see plenty there that needs
to be done.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;!-- ---

_Note: I am working on a replication and extension of the Sparse Autoencoders
results supported by [Cavendish Labs](https://cavendishlabs.org/) - I will share
the results here in the next few days. _ --&gt;

&lt;!-- Footnotes themselves at the bottom. --&gt;
&lt;p&gt;&lt;br /&gt;
&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Thanks to Derik and Joe for comments on a draft of this post.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;br /&gt;&lt;/p&gt;

&lt;div class=&quot;footnotes&quot; role=&quot;doc-endnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:1&quot; role=&quot;doc-endnote&quot;&gt;

      &lt;p&gt;With both its weights and its activations on a series of input examples say¬†&lt;a href=&quot;#fnref:1&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:2&quot; role=&quot;doc-endnote&quot;&gt;

      &lt;p&gt;Of course, if it‚Äôs just a practical undertaking perhaps we would grit our
teeth and try to do this - it appears we at least have the tools to give it
a shot, even if it‚Äôs painfully slow. We have completed huge practical
undertakings before as a scientific community e.g. deciphering the human
genome or getting man to the moon. As we will see there is another concern
as well.¬†&lt;a href=&quot;#fnref:2&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:3&quot; role=&quot;doc-endnote&quot;&gt;

      &lt;p&gt;One theory of exactly how that might come about is found in the
&lt;a href=&quot;https://transformer-circuits.pub/2022/toy_model/index.html&quot;&gt;Superposition Hypothesis&lt;/a&gt;.¬†&lt;a href=&quot;#fnref:3&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:4&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Thanks to Robert Huben for this useful framing¬†&lt;a href=&quot;#fnref:4&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:5&quot; role=&quot;doc-endnote&quot;&gt;

      &lt;p&gt;The proof of this and the convergence properties are analogous to how you
can use fewer data points for linear regression if you know that the linear
map you‚Äôre trying to find is sparse e.g. with Lasso methods for sparse
linear regression. For this to work precisely, we add a bias and a ReLU
non-linearity.¬†&lt;a href=&quot;#fnref:5&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:6&quot; role=&quot;doc-endnote&quot;&gt;

      &lt;p&gt;This is similar to the intuition of the MoEification paper - MLPs naturally
learn some sparse/modular structure, which we might hope to exploit.¬†&lt;a href=&quot;#fnref:6&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:7&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;With the convention from GPT-2 that MLP_dim = 4 * embedding_dim¬†&lt;a href=&quot;#fnref:7&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:8&quot; role=&quot;doc-endnote&quot;&gt;

      &lt;p&gt;which, we recall, is trying to reconstruct the MLP activations through the
sparse bottleneck¬†&lt;a href=&quot;#fnref:8&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:9&quot; role=&quot;doc-endnote&quot;&gt;

      &lt;p&gt;To the extent that we don‚Äôt get 100% of the performance, there are a few
hypotheses. Firstly, we might not have the optimal autoencoder architecture
yet or the autoencoder might not be fully trained enough to saturation.
Secondly, altering the l1&lt;em&gt;loss coefficient hyperparameter adjusts how sparse
we want to make our features and there may be some tuning to do there.
Thirdly, the network might just not _fully&lt;/em&gt; sparse, this seems likely -
there are some early results showing that as the size of the model increases
(from the toy model we have to a large frontier model), we might expect more
sparsity - which suggests that Dictionary Learning may get better with
scale. The later &lt;a href=&quot;https://arxiv.org/pdf/2310.17230.pdf&quot;&gt;Cookbook Features&lt;/a&gt;
paper also suggests this.¬†&lt;a href=&quot;#fnref:9&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;</content><author><name></name></author><category term="machine-learning" /><category term="interpretability" /><category term="dictionary-learning" /><summary type="html">Taking Features Out of Superposition</summary></entry><entry><title type="html">An Analogy for Understanding Mixture of Expert Models</title><link href="http://localhost:4000/blog/2023/10/22/moe-analogy.html" rel="alternate" type="text/html" title="An Analogy for Understanding Mixture of Expert Models" /><published>2023-10-22T00:00:00+01:00</published><updated>2023-10-22T00:00:00+01:00</updated><id>http://localhost:4000/blog/2023/10/22/moe-analogy</id><content type="html" xml:base="http://localhost:4000/blog/2023/10/22/moe-analogy.html">&lt;!-- An Intuitive Way To Understand Mixture Of Expert Models (Sparse MoEs) --&gt;

&lt;h4 id=&quot;tldr-experts-are-doctors-routers-are-gps&quot;&gt;TL;DR: Experts are Doctors, Routers are GPs&lt;/h4&gt;

&lt;h2 id=&quot;motivation&quot;&gt;Motivation&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Foundation_models&quot;&gt;Foundation models&lt;/a&gt; aim to
solve a wide range of tasks. In the days of yore, we would build a supervised
model for every individual use case; foundation models promise a single unified
solution.&lt;/p&gt;

&lt;p&gt;There are challenges with this however. When two tasks need different skills,
trying to learn both can make you learn neither as well as if you had focused on
one&lt;sup id=&quot;fnref:neg&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:neg&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt;. Storing information for many tasks can also be a challenge, even for
large models.&lt;/p&gt;

&lt;p&gt;Moreover we might wonder if it make sense to use the same parameters for
computing the answer to a logic puzzle and for finding the perfect adjective to
describe the love interest in a romance fanfic.&lt;/p&gt;

&lt;p&gt;We would like our models to have modular functions. We could then select and
even combine abilities when needed.&lt;/p&gt;

&lt;h2 id=&quot;moes-for-scale&quot;&gt;MoEs For Scale&lt;/h2&gt;

&lt;p&gt;Scaling up models offers various advantages. There are three main quantities to
scale: the number of model &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;parameters&lt;/code&gt;, the amount of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;data&lt;/code&gt; and the amount of
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;compute&lt;/code&gt; applied at train time. With regular transformers, to scale up the
number of parameters, we must likewise scale the amount of compute applied.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Intuitively more parameters mean more &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;knowledge&lt;/code&gt;, and more compute represents
additional &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;intelligence&lt;/code&gt; &lt;sup id=&quot;fnref:1&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:1&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;2&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;There are some use cases where having more knowledge can be traded off with
being more cognitively able. For example, you may choose to memorise rather than
re-derive the laws of physics to use them in a specific problem. Similarly we
can trade off the opposite way as well - if you know you‚Äôll have access to a
textbook or Wikipedia then you might not want to memorise certain historical
facts. All you need to know is when and how to look up the facts you need.&lt;/p&gt;

&lt;p&gt;So, dependent on whether we need more knowledge or more cognitive ability, we
also want to scale parameters and compute separately&lt;sup id=&quot;fnref:2&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:2&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;3&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;

&lt;h2 id=&quot;sparse-mixture-of-experts-models&quot;&gt;Sparse Mixture of Experts Models&lt;/h2&gt;

&lt;p&gt;In a vanilla transformer, each Transformer Block contains an attention layer for
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;communication&lt;/code&gt; between tokens and an MLP layer for &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;computation&lt;/code&gt; within
tokens.¬†The MLP layer contains most of the parameters of a large transformer and
transforms the individual tokens.&lt;/p&gt;

&lt;p&gt;In &lt;a href=&quot;https://arxiv.org/pdf/2101.03961.pdf&quot;&gt;Sparse Mixture of Experts&lt;/a&gt; (MoEs), we
swap out the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;MLP layers&lt;/code&gt; of the vanilla transformer for an &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Expert Layer&lt;/code&gt;. The
Expert Layer is made up of multiple MLPs called ‚ÄúExperts‚Äù. For each input we
select one expert to send that input to. In this way, each token has different
parameters applied to it. A dynamic routing mechanism decides how to map tokens
to Experts&lt;sup id=&quot;fnref:3&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:3&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;4&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;
  &lt;figure&gt;
    &lt;img src=&quot;/blog/images/softmoe/moe.png&quot; width=&quot;800&quot; alt=&quot;Sparse MoE&quot; /&gt;
    &lt;figcaption&gt;Sparse Expert Layer (Switch Transformer) &lt;/figcaption&gt;
    &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;Sparse MoEs solve the problems we noted earlier:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;MoEs allow their internal ‚ÄúExperts‚Äù to specialise in certain domains rather
than having to be all things to all tokens &lt;sup id=&quot;fnref:0&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:0&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;5&lt;/a&gt;&lt;/sup&gt; &lt;sup id=&quot;fnref:m&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:m&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;6&lt;/a&gt;&lt;/sup&gt;.&lt;/li&gt;
  &lt;li&gt;And with MoEs, we are able to increase the number of parameters of models
without increasing how much training compute or inference time latency. This
decouples parameter scaling from compute scaling (i.e. we decouple knowledge
from intelligence)&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;the-analogy&quot;&gt;The Analogy&lt;/h2&gt;

&lt;p&gt;Imagine you‚Äôre feeling fatigued and you have no idea what‚Äôs causing this.
Suppose the problem is with your eyes but you don‚Äôt know this yet. Since your
friend is a cardiologist (doctor specialising in the heart), you ask them for
advice, which they freely give. You might ask yourself if you should follow
their advice blindly or if you should:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Approach 1: Get a second opinion from another cardiologist.&lt;/strong&gt;&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;
  &lt;figure&gt;
    &lt;img src=&quot;/blog/images/analogy-moe/two_cardiologists.png&quot; width=&quot;600&quot; alt=&quot;Two Cardiologists&quot; /&gt;
    &lt;!-- &lt;figcaption&gt;Sparse Expert Layer (Switch Transformer) &lt;/figcaption&gt; --&gt;
    &lt;/figure&gt;
&lt;/div&gt;

&lt;blockquote&gt;
  &lt;p&gt;Averaging over multiple doctors who were trained in the same way increases
robustness by reducing variance (maybe the first doctor was tired that day or
something). But it doesn‚Äôt help with bias &lt;sup id=&quot;fnref:stat&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:stat&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;7&lt;/a&gt;&lt;/sup&gt; - all the cardiologists are
likely to be wrong in the same way, if they are wrong at all.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Approach 2: Go to a generalist doctor that has no specialism.&lt;/strong&gt;&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;
  &lt;figure&gt;
    &lt;img src=&quot;/blog/images/analogy-moe/no_specialist.png&quot; width=&quot;600&quot; alt=&quot;One cardiologist and one doctor with no specialism&quot; /&gt;
    &lt;!-- &lt;figcaption&gt;Sparse Expert Layer (Switch Transformer) &lt;/figcaption&gt; --&gt;
    &lt;/figure&gt;
&lt;/div&gt;

&lt;blockquote&gt;
  &lt;p&gt;It‚Äôs not clear whether this is better than asking another cardiologist. Sure
they might have different knowledge to the cardiologist which might be useful
if your problem isn‚Äôt about the heart. But there‚Äôs an awful lot of medical
knowledge out there and we can‚Äôt reasonably expect this one generalist to know
everything about all of them. They probably have cursory knowledge at best. We
need someone who specialises in the area that we‚Äôre struggling with. Problem
is we don‚Äôt know which area of specialism we need!&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Approach 3: Ask multiple doctors who all specialise in different areas and do
the thing most of them suggest.&lt;/strong&gt;&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;
  &lt;figure&gt;
    &lt;img src=&quot;/blog/images/analogy-moe/all_doctors.png&quot; width=&quot;600&quot; alt=&quot;Multiple Doctors with Different Specialisms&quot; /&gt;
    &lt;!-- &lt;figcaption&gt;Sparse Expert Layer (Switch Transformer) &lt;/figcaption&gt; --&gt;
    &lt;/figure&gt;
&lt;/div&gt;

&lt;blockquote&gt;
  &lt;p&gt;This is much better. If you have a problem with your eyes, you know that the
eye doctor is being consulted so you have a much better chance of getting the
right treatment. But there are downsides here. Most notably, asking multiple
doctors is probably pretty inefficient. Now we have to see 50 specialists for
every problem even though most of them have no idea about our problem. What we
would prefer is to know which one specialist (or possibly couple of
specialists) we should see and only get advice from them.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Approach 4: Go to your GP, tell them about your ailment and ask them which
doctor you should go and see.&lt;/strong&gt;&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;
  &lt;figure&gt;
    &lt;img src=&quot;/blog/images/analogy-moe/gp.png&quot; width=&quot;600&quot; alt=&quot;GP-Doctor System&quot; /&gt;
    &lt;!-- &lt;figcaption&gt;Sparse Expert Layer (Switch Transformer) &lt;/figcaption&gt; --&gt;
    &lt;/figure&gt;
&lt;/div&gt;

&lt;blockquote&gt;
  &lt;p&gt;Here we get the benefits of getting advice from the most relevant specialised
doctor without having to ask every other doctor. This is both more accurate
and time-efficient.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;In approach 4, the GP is the routing function. They know the strengths of the
different doctors and send you to one of them depending on your problem.&lt;/p&gt;

&lt;p&gt;The Doctors are the Experts. We allow them to specialise knowing that the GP can
route us to the correct doctor for our problem.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;The GP-doctor system is exactly a Mixture of Experts layer.&lt;/strong&gt;&lt;/p&gt;

&lt;h3 id=&quot;what-are-moes-good-for&quot;&gt;What Are MoEs Good For?&lt;/h3&gt;

&lt;p&gt;Viewed this way we see that Mixture of Expert models will be effective whenever
we want a model to have access to large amounts of information - more than a
single Expert could hope to learn alone. Another use case is when our task can
be decomposed into one of a number of tasks.&lt;/p&gt;

&lt;p&gt;In general we might imagine MoEs which when faced with more difficult problems
can send the input to a more powerful expert which has access to more resources.
This starts to move us increasingly towards
&lt;a href=&quot;https://github.com/koayon/awesome-adaptive-computation&quot;&gt;Adaptive Computation&lt;/a&gt;.&lt;/p&gt;

&lt;div class=&quot;footnotes&quot; role=&quot;doc-endnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:neg&quot; role=&quot;doc-endnote&quot;&gt;

      &lt;p&gt;This phenomena is known as negative interference in learning. Jack of All
Trades, Master of None. For other tasks we can see positive interference
however, also known as Transfer Learning.¬†&lt;a href=&quot;#fnref:neg&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:1&quot; role=&quot;doc-endnote&quot;&gt;

      &lt;p&gt;For some vague definitions of ‚Äúintelligence‚Äù and ‚Äúknowledge‚Äù. This intuition
is courtesy of
&lt;a href=&quot;https://scholar.google.com/citations?user=wsGvgA8AAAAJ&amp;amp;hl=en&quot;&gt;Noam Shazeer&lt;/a&gt;.¬†&lt;a href=&quot;#fnref:1&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:2&quot; role=&quot;doc-endnote&quot;&gt;

      &lt;p&gt;In reality both knowledge and cognitive ability are hard to separate this
cleanly but hopefully the intuition still remains useful.¬†&lt;a href=&quot;#fnref:2&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:3&quot; role=&quot;doc-endnote&quot;&gt;

      &lt;p&gt;The experts ‚Äúcompete‚Äù to process the tokens and as in Natural Selection and
Economics, competition for niches makes them specialise.¬†&lt;a href=&quot;#fnref:3&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:0&quot; role=&quot;doc-endnote&quot;&gt;

      &lt;p&gt;In actuality Expert might not necessarily specialise strictly by task. It
might be beneficial for an expert to specialise in syntactic rather than
semantic features or to combine two tasks which are different enough to not
inference with each other.¬†&lt;a href=&quot;#fnref:0&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:m&quot; role=&quot;doc-endnote&quot;&gt;

      &lt;p&gt;This approach also has good biological precedent. Humans don‚Äôt use every
part of their brain for every stimulus they receive - when they receive, for
example a visual stimuli, they use only their visual cortex to process it.¬†&lt;a href=&quot;#fnref:m&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:stat&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;In the statistical sense¬†&lt;a href=&quot;#fnref:stat&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;</content><author><name></name></author><category term="machine-learning" /><category term="mixture-of-experts" /><category term="adaptive-computation" /><summary type="html"></summary></entry><entry><title type="html">From Sparse To Soft Mixtures of Experts</title><link href="http://localhost:4000/blog/2023/10/20/soft-moe.html" rel="alternate" type="text/html" title="From Sparse To Soft Mixtures of Experts" /><published>2023-10-20T00:00:00+01:00</published><updated>2023-10-20T00:00:00+01:00</updated><id>http://localhost:4000/blog/2023/10/20/soft-moe</id><content type="html" xml:base="http://localhost:4000/blog/2023/10/20/soft-moe.html">&lt;p&gt;Mixture of Expert (MoE) models have recently emerged as an ML architecture
offering efficient scaling and practicality in both training and inference &lt;sup id=&quot;fnref:1&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:1&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;

&lt;h3 id=&quot;what-are-sparse-moes&quot;&gt;What Are Sparse MoEs?&lt;/h3&gt;

&lt;p&gt;In traditional &lt;a href=&quot;https://arxiv.org/pdf/2101.03961.pdf&quot;&gt;Sparse MoEs&lt;/a&gt;, we swap out
the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;MLP layers&lt;/code&gt; of the vanilla transformer for an &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Expert Layer&lt;/code&gt;. The Expert
Layer is made up of multiple MLPs referred to as Experts. For each input one
expert is selected to send that input to. A dynamic routing mechanism decides
how to map tokens to Experts. Importantly, though this is less mentioned, MoEs
are more modular and hence more naturally interpretable than vanilla
transformers.&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;
  &lt;figure&gt;
    &lt;img src=&quot;/blog/images/softmoe/moe.png&quot; width=&quot;800&quot; alt=&quot;Sparse MoE&quot; /&gt;
    &lt;figcaption&gt;Sparse Expert Layer (Switch Transformer) &lt;/figcaption&gt;
    &lt;/figure&gt;
&lt;/div&gt;

&lt;h3 id=&quot;introducing-soft-moes&quot;&gt;Introducing Soft MoEs&lt;/h3&gt;

&lt;p&gt;The Soft MoE paradigm was introduced by Google researchers in the paper
&lt;a href=&quot;https://arxiv.org/pdf/2308.00951.pdf&quot;&gt;From Sparse To Soft Mixtures of Experts&lt;/a&gt;.
Unlike Sparse MoEs, Soft MoEs don‚Äôt send a &lt;em&gt;subset&lt;/em&gt; of the input tokens to
experts. Instead, each expert receives a &lt;em&gt;linear combination&lt;/em&gt; of all the input
tokens. The weights for these combinations are determined by the same dynamic
routing mechanism as in Sparse MoEs.&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;
  &lt;figure&gt;
    &lt;img src=&quot;/blog/images/softmoe/duck.png&quot; width=&quot;500&quot; alt=&quot;Soft MoE&quot; /&gt;
    &lt;figcaption&gt;In Soft MoEs each expert processes linear combinations of image patches. &lt;/figcaption&gt;
    &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;The discrete routing that makes Sparse MoEs so effective also makes them not
inherently fully differentiable and can cause training issues. The Soft MoE
approach solves these issues, are better suited to GPU hardware and in general
outperform Sparse MoEs.&lt;/p&gt;

&lt;p&gt;The paper abstract reads:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Sparse mixture of expert architectures (MoEs) scale model capacity without
large increases in training or inference costs. Despite their success, MoEs
suffer from a number of issues: training instability, token dropping,
inability to scale the number of experts, or ineffective finetuning. In this
work, we propose Soft MoE, a fully-differentiable sparse Transformer that
addresses these challenges, while maintaining the benefits of MoEs. Soft MoE
performs an implicit soft assignment by passing different weighted
combinations of all input tokens to each expert. As in other MoE works,
experts in Soft MoE only process a subset of the (combined) tokens, enabling
larger model capacity at lower inference cost. In the context of visual
recognition, Soft MoE greatly outperforms standard Transformers (ViTs) and
popular MoE variants (Tokens Choice and Experts Choice). For example, Soft
MoE-Base/16 requires 10.5√ó lower inference cost (5.7√ó lower wall-clock time)
than ViT-Huge/14 while matching its performance after similar training. Soft
MoE also scales well: Soft MoE Huge/14 with 128 experts in 16 MoE layers has
over 40√ó more parameters than ViT Huge/14, while inference time cost grows by
only 2%, and it performs substantially better.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h3 id=&quot;links-to-talk-and-slides&quot;&gt;Links to Talk and Slides&lt;/h3&gt;

&lt;p&gt;I recently gave a talk at &lt;a href=&quot;https://www.eleuther.ai&quot;&gt;EleutherAI&lt;/a&gt;, the open-source
AI research lab, about Soft MoEs.&lt;/p&gt;

&lt;p&gt;You can watch the talk back on YouTube
&lt;a href=&quot;https://youtu.be/xCKdBC5dh_g?si=uDH8vLVII7l_X8_L&quot;&gt;here&lt;/a&gt; &lt;sup id=&quot;fnref:2&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:2&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;2&lt;/a&gt;&lt;/sup&gt; or view the slides
&lt;a href=&quot;https://docs.google.com/presentation/d/12Sw4wRQJr3sxcJR91_UM_dlYgYxeAbf9t8es54bAYUM/edit#slide=id.p&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;I‚Äôm very excited about research ideas working on expanding the SoftMoE paradigm
to autoregressive (GPT-style) models, which is currently an open problem
described in the above talk. Feel free to reach out if you‚Äôre interested in or
are currently researching in this area. &lt;br /&gt; &lt;br /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;div class=&quot;footnotes&quot; role=&quot;doc-endnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:1&quot; role=&quot;doc-endnote&quot;&gt;

      &lt;p&gt;For more details on MoE models see the
&lt;a href=&quot;https://github.com/koayon/awesome-adaptive-computation&quot;&gt;Awesome Adaptive Computation&lt;/a&gt;
repo.¬†&lt;a href=&quot;#fnref:1&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:2&quot; role=&quot;doc-endnote&quot;&gt;

      &lt;p&gt;Unfortunately the video‚Äôs audio quality isn‚Äôt as great as it could be, I may
look at cleaning this up.¬†&lt;a href=&quot;#fnref:2&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;</content><author><name></name></author><category term="machine-learning" /><category term="mixture-of-experts" /><category term="adaptive-computation" /><summary type="html">Mixture of Expert (MoE) models have recently emerged as an ML architecture offering efficient scaling and practicality in both training and inference [^1].</summary></entry><entry><title type="html">DeepSpeed‚Äôs Bag of Tricks for Speed &amp;amp; Scale</title><link href="http://localhost:4000/blog/2023/07/14/deepspeed-train.html" rel="alternate" type="text/html" title="DeepSpeed‚Äôs Bag of Tricks for Speed &amp;amp; Scale" /><published>2023-07-14T00:00:00+01:00</published><updated>2023-07-14T00:00:00+01:00</updated><id>http://localhost:4000/blog/2023/07/14/deepspeed-train</id><content type="html" xml:base="http://localhost:4000/blog/2023/07/14/deepspeed-train.html">&lt;!-- # DeepSpeed&apos;s Bag of Tricks for Speed &amp; Scale --&gt;

&lt;h2 id=&quot;an-introduction-to-deepspeed-for-training&quot;&gt;An Introduction to DeepSpeed for Training&lt;/h2&gt;

&lt;p&gt;In the literature and the public conversation around Natural Language
Processing, lots has been made of the results of scaling up data, compute and
model size. For example we have the &lt;a href=&quot;https://arxiv.org/abs/2001.08361&quot;&gt;original&lt;/a&gt;
and &lt;a href=&quot;https://arxiv.org/abs/2203.15556&quot;&gt;updated&lt;/a&gt; transformer scaling laws.&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;
  &lt;figure&gt;
    &lt;img src=&quot;/blog/images/deepspeed/stack_more_layers.webp&quot; width=&quot;500&quot; alt=&quot;Layers&quot; /&gt;
    &lt;figcaption&gt;Keep it stacking&lt;/figcaption&gt;
    &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;One sometimes overlooked point is the vital role of engineering breakthroughs in
enabling large models to be trained and served on current hardware.&lt;/p&gt;

&lt;p&gt;This post is about the engineering tricks that bring the research to life.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;em&gt;Note: This post assumes some basic familiarity with PyTorch/Tensorflow and
transformers. If you‚Äôve never used these before check out the
&lt;a href=&quot;https://pytorch.org/docs/stable/index.html&quot;&gt;PyTorch docs&lt;/a&gt; and the
&lt;a href=&quot;https://jalammar.github.io/illustrated-transformer/&quot;&gt;Illustrated Transformer&lt;/a&gt;.
Some background on backpropagation works will also be useful - check out
&lt;a href=&quot;https://www.youtube.com/watch?v=Ilg3gGewQ5U&quot;&gt;this video&lt;/a&gt; if you want a
refresher!&lt;/em&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;!-- {:toc}  --&gt;

&lt;hr /&gt;

&lt;details&gt;
  &lt;summary&gt;Table of Contents&lt;/summary&gt;
  &lt;ul&gt;
    &lt;li&gt;&lt;a href=&quot;#&quot;&gt;0. Introduction&lt;/a&gt;
      &lt;ul&gt;
        &lt;li&gt;&lt;a href=&quot;#01-deepspeed-s-three-innovation-pillars&quot;&gt;0.1 DeepSpeed&apos;s Three Innovation Pillars&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&quot;#02-problems-training-large-models&quot;&gt;0.2 Problems Training Large Models&lt;/a&gt;&lt;/li&gt;
      &lt;/ul&gt;
    &lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;#1-partial-solutions&quot;&gt;1. Partial Solutions&lt;/a&gt;
      &lt;ul&gt;
        &lt;li&gt;&lt;a href=&quot;#11-naive-data-parallelism&quot;&gt;1.1 Naive Data Parallelism&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&quot;#12-naive-model-parallelism&quot;&gt;1.2 Naive Model Parallelism&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&quot;#13-a-better-way--deepspeed&quot;&gt;1.3 A Better Way: DeepSpeed&lt;/a&gt;&lt;/li&gt;
      &lt;/ul&gt;
    &lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;#2-deepspeed-deep-dive--key-ideas&quot;&gt;2. DeepSpeed Deep Dive: Key Ideas&lt;/a&gt;
      &lt;ul&gt;
        &lt;li&gt;&lt;a href=&quot;#20-mixed-precision-training&quot;&gt;2.0 Mixed Precision Training&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&quot;#21-delaying-weight-updates&quot;&gt;2.1 Delaying Weight Updates&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&quot;#22-storing-optimiser-states-without-redundancy--zero-stage-1-&quot;&gt;2.2 Storing Optimiser States Without Redundancy (ZeRO stage 1)&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&quot;#23-storing-gradients-and-parameters-without-redundancy--zero-stages-2---3-&quot;&gt;2.3 Storing Gradients and Parameters Without Redundancy (ZeRO stages 2 &amp;amp; 3)&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&quot;#24-tensor-slicing&quot;&gt;2.4 Tensor Slicing&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&quot;#25-gradient-checkpointing&quot;&gt;2.5 Gradient Checkpointing&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&quot;#26-profiling-etc&quot;&gt;2.6 Profiling etc&lt;/a&gt;&lt;/li&gt;
      &lt;/ul&gt;
    &lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;#3-in-pictures&quot;&gt;3. In Pictures&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;#4-in-code&quot;&gt;4. In Code&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;#5-using-deepspeed&quot;&gt;5. Using DeepSpeed&lt;/a&gt;&lt;/li&gt;
  &lt;/ul&gt;
&lt;/details&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;01-deepspeeds-three-innovation-pillars&quot;&gt;0.1 DeepSpeed‚Äôs Three Innovation Pillars&lt;/h3&gt;

&lt;p&gt;&lt;a href=&quot;https://www.deepspeed.ai&quot;&gt;DeepSpeed&lt;/a&gt; has four main use cases: enabling large
training runs, decreasing inference latency, model compression and enabling ML
science.&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;
  &lt;figure&gt;
  &lt;img src=&quot;/blog/images/deepspeed/deepspeed-pillars.png&quot; width=&quot;700&quot; alt=&quot;&quot; /&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;This post covers training optimizations.&lt;/p&gt;

&lt;h3 id=&quot;02-problems-training-large-models&quot;&gt;0.2 Problems Training Large Models&lt;/h3&gt;

&lt;p&gt;Training large models (e.g. LLMs) on huge datasets can be can be prohibitively
slow, expensive, or even impossible with available hardware.&lt;/p&gt;

&lt;p&gt;In particular, very large models generally do not fit into the memory of a
single GPU/TPU node. Compared to CPUs, GPUs are generally higher throughput but
lower memory capacity. (A typical GPU may have 32GB memory versus 1TB+ for
CPUs).&lt;/p&gt;

&lt;p&gt;Our aims are:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;To train models too large for a single device&lt;/li&gt;
  &lt;li&gt;Efficiently distribute computation across devices&lt;/li&gt;
  &lt;li&gt;Fully utilize all devices as much as possible&lt;/li&gt;
  &lt;li&gt;Minimize communication bottlenecks &lt;em&gt;between&lt;/em&gt; devices&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;DeepSpeed reduces compute and time to train by &amp;gt;100x for large models.&lt;/p&gt;

&lt;p&gt;If you just want to see how to implement DeepSpeed in your code, see the
&lt;a href=&quot;#5-using-deepspeed&quot;&gt;Using DeepSpeed&lt;/a&gt; section below.&lt;/p&gt;

&lt;h2 id=&quot;1-partial-solutions&quot;&gt;1. Partial Solutions&lt;/h2&gt;

&lt;h3 id=&quot;11-naive-data-parallelism&quot;&gt;1.1 Naive Data Parallelism&lt;/h3&gt;

&lt;p&gt;Without any data parallelism, we get this sorry sight:&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;
  &lt;figure&gt;
  &lt;img src=&quot;/blog/images/deepspeed/gpu_unused.png&quot; width=&quot;700&quot; alt=&quot;Unused GPU potential&quot; /&gt;
  &lt;figcaption&gt;Oh dear&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;We‚Äôve spent a lot of money on GPU cores for them all to sit there idle apart
from one! Unless you‚Äôre single-handedly trying to prop up the NVIDIA share
price, this is a terrible idea!&lt;/p&gt;

&lt;p&gt;One thing that we might try is splitting up the data, parallelising across
devices. Here we copy the entire model onto each worker, each of which process
different subsets of the training dataset.&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;
  &lt;figure&gt;
  &lt;img src=&quot;/blog/images/deepspeed/data_parallel.png&quot; width=&quot;700&quot; alt=&quot;Data Parallelisation&quot; /&gt;
  &lt;figcaption&gt;Data Parallelisation&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;Each device compute its own gradients and then we average out the gradients
across all the nodes to update our parameters with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;all_reduce&lt;/code&gt;. This approach
is pretty straightforward to implement and works for any model type.&lt;/p&gt;

&lt;p&gt;We‚Äôve turned more GPUs into more speed - great!&lt;/p&gt;

&lt;p&gt;In addition we also increase effective batch size, reducing costly parameter
updates. Since with larger batch sizes there is more signal in each gradient
update, this also improves convergence (up to a point).&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;
  &lt;figure&gt;
  &lt;img src=&quot;/blog/images/deepspeed/whats_the_catch.gif&quot; alt=&quot;What&apos;s The Catch&quot; /&gt;
  &lt;figcaption&gt;I thought you&apos;d never ask&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;Unfortunately the memory bottleneck still remains. For Data Parallelism to work,
the entire model has to fit on every device, which just isn‚Äôt going to happen
for large models.&lt;/p&gt;

&lt;h3 id=&quot;12-naive-model-parallelism&quot;&gt;1.2 Naive Model Parallelism&lt;/h3&gt;

&lt;p&gt;Another thing we could try is splitting up the computation of the model itself,
putting different layers (transformer blocks) on different devices. With this
model parallelism approach we aren‚Äôt limited by the size of a memory of a single
GPU, but instead by all the GPUs that we have.&lt;/p&gt;

&lt;p&gt;However two problems remain. Firstly how to split up a model efficiently is very
dependant on the specific model architecture (for example the number of layers
and attention heads). And secondly communicating &lt;em&gt;between&lt;/em&gt; nodes now bottlenecks
training.&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;
  &lt;figure&gt;
  &lt;img src=&quot;/blog/images/deepspeed/model_parallel.png&quot; width=&quot;600&quot; alt=&quot;Model parallelisation&quot; /&gt;
  &lt;figcaption&gt;One batch moving through the parallelised model. In model parallelisation, one forward and backward pass requires all the devices, most of which are idle at any one time&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;Since each layer requires the input to the previous layer in each pass, workers
spend most of their time waiting. What a waste of GPU time! Here it looks like
the model takes the same amount of time as if we had a GPU to fit it on but it‚Äôs
even worse. The communication overhead of getting data between nodes makes it
even &lt;em&gt;slower&lt;/em&gt; than a single GPU.&lt;/p&gt;

&lt;p&gt;Can we do better than this?&lt;/p&gt;

&lt;h3 id=&quot;13-a-better-way-deepspeed&quot;&gt;1.3 A Better Way: DeepSpeed&lt;/h3&gt;

&lt;p&gt;Data Parallelism gave speedups but couldn‚Äôt handle models too large for a single
machine. Model Parallelism allowed us to train large models but it‚Äôs slow.&lt;/p&gt;

&lt;p&gt;We really want a marriage of the ideas of both data and model parallelism -
speed and scale together.&lt;/p&gt;

&lt;p&gt;We don‚Äôt always get what we want, but in this case we do. With DeepSpeed,
Microsoft packaged up a bag of tricks to allow ML engineers to train larger
models more efficiently. All in, DeepSpeed enables &amp;gt;100x lower training time and
cost with minimal code changes - just 4 changed lines of PyTorch code. Let‚Äôs
walk through how.&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;
  &lt;figure&gt;
  &lt;img src=&quot;/blog/images/deepspeed/dp_vs_mp.png&quot; width=&quot;700&quot; alt=&quot;DP vs MP&quot; /&gt;
  &lt;figcaption&gt;Data Parallelisation vs Model Parallelism&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h2 id=&quot;2-deepspeed-deep-dive-key-ideas&quot;&gt;2. DeepSpeed Deep Dive: Key Ideas&lt;/h2&gt;

&lt;p&gt;&lt;del&gt;One&lt;/del&gt; Seven Weird Tricks to Train Large Models:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Mixed precision training&lt;/li&gt;
  &lt;li&gt;Delaying Weight Updates&lt;/li&gt;
  &lt;li&gt;Storing the optimiser states without redundancy (ZeRO stage 1)&lt;/li&gt;
  &lt;li&gt;Storing gradients and parameters without redundancy (ZeRO stages 2 &amp;amp; 3)&lt;/li&gt;
  &lt;li&gt;Tensor Slicing&lt;/li&gt;
  &lt;li&gt;Gradient Checkpointing&lt;/li&gt;
  &lt;li&gt;Quality of Life Improvements and Profiling&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;20-mixed-precision-training&quot;&gt;2.0 Mixed Precision Training&lt;/h3&gt;

&lt;p&gt;Ordinarily mathematical operations are performed with 32 bit floats (fp32).
Using half precision (fp16) vs full precision (fp32) halves memory and speeds up
computation.&lt;/p&gt;

&lt;p&gt;We forward/backward pass in fp16 for speed, keeping copies of fp32 optimizer
states (momentum, first order gradient etc.) for accuracy. The high precision
fp32 maintains the high dynamic range so that we can still represent very slight
updates.&lt;/p&gt;

&lt;h3 id=&quot;21-delaying-weight-updates&quot;&gt;2.1 Delaying Weight Updates&lt;/h3&gt;

&lt;p&gt;A simple training loop might contain something like:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;batch&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;enumerate&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train_loader&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;

    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;j&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;minibatch&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;enumerate&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;batch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;

        &lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;minibatch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;local_gradients&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;gradients&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;batch_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;average_gradients&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;distributed&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;all_reduce&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;local_gradients&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# reduce INSIDE inner loop
&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;optimizer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;step&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;average_gradients&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Note here that within every loop we‚Äôre calculating not only the local gradients
but also synchronizing gradients which requires communicating with all the other
nodes.&lt;/p&gt;

&lt;p&gt;Delaying synchronization improves throughput e.g:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;batch&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;enumerate&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train_loader&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;

    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;j&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;minibatch&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;enumerate&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;batch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;

        &lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;minibatch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;local_gradients&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;gradients&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;batch_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

    &lt;span class=&quot;n&quot;&gt;average_gradients&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;distributed&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;all_reduce&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;local_gradients&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# reduce OUTSIDE inner loop
&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;optimizer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;step&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;average_gradients&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;22-storing-optimiser-states-without-redundancy-zero-stage-1&quot;&gt;2.2 Storing Optimiser States Without Redundancy (ZeRO stage 1)&lt;/h3&gt;

&lt;p&gt;Suppose we have a GPU with 50GB of memory and our model weights are 10GB of
memory. That‚Äôs all great right?&lt;/p&gt;

&lt;p&gt;For inference we feed in our input data and get out activations at each step.
Then once we pass each layer, we can throw away activations from prior layers.
Our model fits on the single GPU.&lt;/p&gt;

&lt;p&gt;For training however, it‚Äôs a different story. Each GPU needs its intermediate
activations, gradients and the fp32 optimiser states for backpropagation. Pretty
soon we‚Äôre overflowing the GPU with our model‚Äôs memory footprint üòû&lt;/p&gt;

&lt;p&gt;The biggest memory drain on our memory is the optimisation states.&lt;/p&gt;

&lt;p&gt;We know that we‚Äôre going to need to get multiple GPUs and do some model
parallelisation here. Eventually we want to partition the whole model but a good
first move would be to at least remove optimisation state redundancy.&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;
  &lt;figure&gt;
  &lt;img src=&quot;/blog/images/deepspeed/zero_stages.png&quot; width=&quot;800&quot; alt=&quot;The Stages of ZeRO&quot; /&gt;
  &lt;figcaption&gt;The Stages of Zero Redundancy Optimisation (ZeRO)&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;For ZeRO stage 1, in the backward pass, each device calculates the (first order)
gradients for the final section of the model. The final device &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;gathers&lt;/code&gt; all
these gradients, averages them and then computes the Adam optimised gradient
with the optimisation states. It then &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;broadcasts&lt;/code&gt; back the new parameter states
for the final section of the model to all devices. Then the penultimate device
will do the same and so on until we reach the first device.&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;
  &lt;figure&gt;
  &lt;img src=&quot;/blog/images/deepspeed/zero1-t1.gif&quot; width=&quot;800&quot; alt=&quot;ZeRO Stage 1&quot; /&gt;
  &lt;figcaption&gt;
    ZeRO Stage 1
&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;
&lt;p&gt;We can think of this as a 5 step process:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;All nodes calculate gradients from their loss (note they all did a forward
pass on different data so their losses will be different!)&lt;/li&gt;
  &lt;li&gt;Final node collects and averages the gradients from all nodes via &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;reduce&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;Final node calculates gradient update using optimiser states&lt;/li&gt;
  &lt;li&gt;Final node &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;broadcasts&lt;/code&gt; the new gradients to all of the nodes.&lt;/li&gt;
  &lt;li&gt;Repeat for penultimate section and so on to complete the gradient updates.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;ZeRO stage 1 typically reduces our memory footprint by ~4x.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;üîÑ Fun Fact: The name DeepSpeed is a palindrome! How cute ü§ó
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;23-storing-gradients-and-parameters-without-redundancy-zero-stages-2--3&quot;&gt;2.3 Storing Gradients and Parameters Without Redundancy (ZeRO stages 2 &amp;amp; 3)&lt;/h3&gt;

&lt;p&gt;We can take the partitioning idea further and do it for parameters and gradients
as well as optimisation states.&lt;/p&gt;

&lt;h4 id=&quot;in-the-forward-pass&quot;&gt;In the forward pass:&lt;/h4&gt;

&lt;div align=&quot;center&quot;&gt;
  &lt;figure&gt;
  &lt;img src=&quot;/blog/images/deepspeed/zero3_forward.gif&quot; width=&quot;800&quot; alt=&quot;ZeRO Stage 3 (Forward)&quot; /&gt;
  &lt;figcaption&gt;
    ZeRO Stage 3: forward pass
&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;
&lt;ol&gt;
  &lt;li&gt;The first node &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;broadcasts&lt;/code&gt; the parameters for the first section of the model.&lt;/li&gt;
  &lt;li&gt;All nodes complete the forward pass for their data for the first section of the model.&lt;/li&gt;
  &lt;li&gt;They then throw away the parameters for first section of the model.&lt;/li&gt;
  &lt;li&gt;Repeat for second section and so on to get the loss.&lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;and-the-backward-pass&quot;&gt;And the backward pass:&lt;/h4&gt;

&lt;div align=&quot;center&quot;&gt;
  &lt;figure&gt;
  &lt;img src=&quot;/blog/images/deepspeed/zero3_backward.gif&quot; width=&quot;800&quot; alt=&quot;ZeRO Stage 3 (Backward)&quot; /&gt;
  &lt;figcaption&gt;
    Zero Stage 3: backward pass
&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;
&lt;ol&gt;
  &lt;li&gt;The final node &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;broadcasts&lt;/code&gt; its section gradients.&lt;/li&gt;
  &lt;li&gt;Each backpropagate their own loss to get the next gradients.&lt;/li&gt;
  &lt;li&gt;As before, final node accumulates and averages all gradients (&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;reduce&lt;/code&gt;), calculates gradient update with optimiser and then &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;broadcasts&lt;/code&gt; the results, which can be used for the next section.&lt;/li&gt;
  &lt;li&gt;Once used, all gradients are thrown away by nodes which are not responsible for that section.&lt;/li&gt;
  &lt;li&gt;Repeat for penultimate section and so on to complete the gradient updates.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;If we have &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;N&lt;/code&gt; cores, we now have an &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;N&lt;/code&gt;x memory footprint reduction from ZeRO.&lt;/p&gt;

&lt;h4 id=&quot;a-breather&quot;&gt;A breather&lt;/h4&gt;

&lt;p&gt;That was the most complex part so feel free to check out these resources to make
sure you understand what‚Äôs going on:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=y4_bCiAsIAk&amp;amp;list=PLDEUW02OCkqGZ5_8jVQUK0dRJx8Um-hpc&amp;amp;index=1&amp;amp;t=20s&quot;&gt;DeepSpeed founder at MLOps community&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.microsoft.com/en-us/research/blog/ZeRO-deepspeed-new-system-optimizations-enable-training-models-with-over-100-billion-parameters/&quot;&gt;Microsoft blog post&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;It‚Äôs all downhill from here!&lt;/p&gt;

&lt;h4 id=&quot;benefits-of-zero&quot;&gt;Benefits of ZeRO&lt;/h4&gt;

&lt;p&gt;Overall, ZeRO removes the redundancy across data parallel process by
partitioning optimizer states, gradients and parameters across nodes. Look at
how much memory footprint we‚Äôve saved!&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;
  &lt;figure&gt;
  &lt;img src=&quot;/blog/images/deepspeed/deepspeed_benefits.png&quot; width=&quot;800&quot; alt=&quot;DeepSpeed Benefits&quot; /&gt;
  &lt;figcaption&gt;Benefits of DeepSpeed&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;One surprising thing about this approach is that it scales superlinearly. That
is, when we double the number of GPUs that we‚Äôre using, we &lt;em&gt;more than&lt;/em&gt; double
the throughput of the system! In splitting up the model across more GPUs, we
leave more space per node for activations which allows for higher batch sizes.&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;
  &lt;figure&gt;
  &lt;img src=&quot;/blog/images/deepspeed/superlinear_scale.png&quot; alt=&quot;Superlinear Scale&quot; /&gt;
  &lt;figcaption&gt;Superlinear Scale of DeepSpeed vs Perfect Scaling&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h3 id=&quot;24-tensor-slicing&quot;&gt;2.4 Tensor Slicing&lt;/h3&gt;

&lt;p&gt;Most of the operations in a large ML model are matrix multiplications followed
by non-linearities. Matrix multiplication can be thought of as dot products
between pairs of matrix rows and columns. So we can compute independent dot
products on different GPUs and then combine the results afterwards.&lt;/p&gt;

&lt;p&gt;Another way to think about this is that if we want to parallelise matrix
multiplication across GPUs, we can slice up huge tensors into smaller ones and
then combine the results at the end.&lt;/p&gt;

&lt;p&gt;For matrices $$ X = \begin{bmatrix} X_1 &amp;amp; X_2 \end{bmatrix} $$ and $$ A =
\begin{bmatrix} A_1 \\ A_2 \end{bmatrix} $$, we note that:&lt;/p&gt;

&lt;p&gt;$$
XA = \begin{bmatrix} X_1 &amp;amp; X_2 \end{bmatrix} \begin{bmatrix} A_1 \\ A_2 \end{bmatrix}
$$&lt;/p&gt;

&lt;p&gt;For example:&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;
  &lt;figure&gt;
  &lt;img src=&quot;/blog/images/deepspeed/row_slicing_numbers.png&quot; width=&quot;700&quot; alt=&quot;Row Slicing&quot; /&gt;
  &lt;figcaption&gt;Row Slicing&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;However if there is a non-linear map after the M e.g. if $$ Y = \text{ReLU}(XA)
$$, this slicing isn‚Äôt going to work. $$ \text{ReLU}(X_1A_1 + X_2A_2) \neq
\text{ReLU}(X_1A_1) + \text{ReLU}(X_2A_2) $$ in general by non-linearity. So we
should instead split up X by columns and duplicate M across both nodes such that
we have:&lt;/p&gt;

&lt;p&gt;$$ Y = [Y_1, Y_2] = [\text{ReLU}(X A_1), \text{ReLU}(X A_2)] = XA $$&lt;/p&gt;

&lt;p&gt;For example:&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;
  &lt;figure&gt;
  &lt;img src=&quot;/blog/images/deepspeed/column_slicing_numbers.png&quot; width=&quot;700&quot; alt=&quot;Column Slicing&quot; /&gt;
  &lt;figcaption&gt;Column Slicing&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;Note: normally we think of A acting on X by left multiplication. In this case X
is our data and A is the weights which we want to parallelise. Through taking
transposes we can swap the order of the geometric interpretation so we can think
of the above as linear map A acting on our data X and still retain the slicing.&lt;/p&gt;

&lt;h3 id=&quot;25-gradient-checkpointing&quot;&gt;2.5 Gradient Checkpointing&lt;/h3&gt;

&lt;p&gt;In our description of ZeRO each core cached (held in memory) the activations for
it‚Äôs part of the model.&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;
  &lt;figure&gt;
  &lt;img src=&quot;https://github.com/cybertronai/gradient-checkpointing/raw/master/img/output.gif&quot; alt=&quot;Regular backprop&quot; /&gt;
  &lt;figcaption&gt;The top layer represents the activations in the model populating during the forward pass and the lower layer, the gradients populated in the backward pass. The first circle is the input data and the bottom right is the loss.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;Suppose we had extremely limited memory but were flush with compute. An
alternative approach to storing all the activations would be to simply recompute
them when we need in the backward pass. We can always recompute the activations
by running the same input data through a forward pass.&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;
  &lt;figure&gt;
  &lt;img src=&quot;https://github.com/cybertronai/gradient-checkpointing/raw/master/img/output_poor.gif&quot; alt=&quot;Memory poor backprop&quot; /&gt;
  &lt;figcaption&gt;Here each activation is computed just before it&apos;s needed using forward passes.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;This recomputing approach saves lots of memory but is quite compute wasteful,
incurring &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;m&lt;/code&gt; extra forward passes for an &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;m-layer&lt;/code&gt; transformer.&lt;/p&gt;

&lt;p&gt;A middle ground approach to trading off compute and memory is
&lt;a href=&quot;https://github.com/cybertronai/gradient-checkpointing&quot;&gt;gradient checkpointing&lt;/a&gt;
(sometimes known as activation checkpointing). Here we store some intermediate
activations with $$\sqrt m$$ of the memory for the cost of one forward pass.&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;
  &lt;figure&gt;
  &lt;img src=&quot;https://github.com/cybertronai/gradient-checkpointing/raw/master/img/output2.gif&quot; alt=&quot;Gradient Checkpointing&quot; /&gt;
  &lt;figcaption&gt;Here the only the second layer activations are cached as a &quot;checkpoint&quot;. Now for activations after the checkpoint instead of computing from the input data, we can compute from the checkpoint. This approach trades off memory and compute.&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h3 id=&quot;26-profiling-etc&quot;&gt;2.6 Profiling etc&lt;/h3&gt;

&lt;p&gt;While not strictly causing any code optimisations, DeepSpeed provides developer
friendly features like convenient profiling and monitoring to track latency and
performance. We also have model checkpointing so you can recover a model from
different points in training. Developer happiness matters almost as much as
loss!&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;
  &lt;figure&gt;
  &lt;img src=&quot;https://i.imgflip.com/7s8ojc.jpg&quot; width=&quot;500&quot; alt=&quot;Happy&quot; /&gt;
  &lt;figcaption&gt;Happy engineers write happy code&lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;Check out the &lt;a href=&quot;https://deepspeed.readthedocs.io/en/latest/&quot;&gt;docs&lt;/a&gt; for more info!&lt;/p&gt;

&lt;h2 id=&quot;3-in-pictures&quot;&gt;3. In Pictures&lt;/h2&gt;

&lt;video controls=&quot;&quot; width=&quot;700&quot;&gt;
  &lt;source src=&quot;/blog/images/deepspeed/Turing-Animation.mp4&quot; type=&quot;video/mp4&quot; /&gt;
&lt;/video&gt;

&lt;p&gt;&lt;em&gt;Animated Video from Microsoft: warning, it‚Äôs a little slow.&lt;/em&gt;&lt;/p&gt;

&lt;h2 id=&quot;4-in-code&quot;&gt;4. In Code&lt;/h2&gt;

&lt;p&gt;The full DeepSpeed library, with all the hardware level optimisations, is
open-sourced. See the &lt;a href=&quot;https://github.com/microsoft/DeepSpeed/&quot;&gt;core library&lt;/a&gt;,
the &lt;a href=&quot;https://www.deepspeed.ai/training/&quot;&gt;docs&lt;/a&gt; and
&lt;a href=&quot;https://github.com/microsoft/DeepSpeedExamples&quot;&gt;examples&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;For an annotated and easier to follow implementation see
&lt;a href=&quot;https://nn.labml.ai/scaling/zero3/index.html&quot;&gt;Lab ML‚Äôs version&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;5-using-deepspeed&quot;&gt;5. Using DeepSpeed&lt;/h2&gt;

&lt;p&gt;DeepSpeed integrates with PyTorch and TensorFlow to optimize training.&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;
  &lt;figure&gt;
  &lt;img src=&quot;/blog/images/deepspeed/stack.png&quot; width=&quot;600&quot; alt=&quot;Stack&quot; /&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;In PyTorch we only need to change 4 lines of code to apply DeepSpeed such that
our code is optimised for training on a single GPU machine, a single machine
with multiple GPUs, or on multiple machines in a distributed fashion.&lt;/p&gt;

&lt;p&gt;First we swap out:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;to&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;device&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;optimizer&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;optim&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Adam&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;parameters&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;lr&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;1e-4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;with initialising DeepSpeed by writing:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;ds_config&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
  &lt;span class=&quot;s&quot;&gt;&quot;train_micro_batch_size_per_gpu&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;batch_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
  &lt;span class=&quot;s&quot;&gt;&quot;optimizer&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
      &lt;span class=&quot;s&quot;&gt;&quot;type&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;Adam&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
      &lt;span class=&quot;s&quot;&gt;&quot;params&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
          &lt;span class=&quot;s&quot;&gt;&quot;lr&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;1e-4&lt;/span&gt;
      &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
  &lt;span class=&quot;p&quot;&gt;},&lt;/span&gt;
  &lt;span class=&quot;s&quot;&gt;&quot;fp16&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
      &lt;span class=&quot;s&quot;&gt;&quot;enabled&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;
  &lt;span class=&quot;p&quot;&gt;},&lt;/span&gt;
  &lt;span class=&quot;s&quot;&gt;&quot;zero_optimization&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
      &lt;span class=&quot;s&quot;&gt;&quot;stage&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
      &lt;span class=&quot;s&quot;&gt;&quot;offload_optimizer&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
         &lt;span class=&quot;s&quot;&gt;&quot;device&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;cpu&quot;&lt;/span&gt;
      &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
  &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;model_engine&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;initialize&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;model_architecture&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                       &lt;span class=&quot;n&quot;&gt;model_parameters&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;params&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                       &lt;span class=&quot;n&quot;&gt;config&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ds_config&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Then in our training loop we change out the original PyTorch‚Ä¶&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;step&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;batch&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;enumerate&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data_loader&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;# Calculate loss using model e.g.
&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;output&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;batch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;criterion&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;output&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;target&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

    &lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;backward&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;optimizer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;step&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;for:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;step&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;batch&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;enumerate&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data_loader&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;# Forward propagation method to get loss
&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;...&lt;/span&gt;

    &lt;span class=&quot;c1&quot;&gt;# Runs backpropagation
&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;model_engine&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;backward&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

    &lt;span class=&quot;c1&quot;&gt;# Weights update
&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;model_engine&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;step&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;That‚Äôs all it takes! In addition, DeepSpeed‚Äôs backend has also been integrated
with HuggingFace via the
&lt;a href=&quot;https://huggingface.co/docs/accelerate/index&quot;&gt;Accelerate library&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;thats-all-folks&quot;&gt;That‚Äôs All Folks!&lt;/h2&gt;

&lt;p&gt;There‚Äôs a lot of clever improvements that go into the special sauce for training
large models. And for users, with just a few simple code changes, DeepSpeed
works its magic to unleash the power of all your hardware for fast, efficient
model training.&lt;/p&gt;

&lt;p&gt;Happy training!&lt;/p&gt;</content><author><name></name></author><category term="machine-learning" /><category term="deepspeed" /><category term="training" /><summary type="html"></summary></entry></feed>