<!DOCTYPE html>
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
<!-- Begin Jekyll SEO tag v2.8.0 -->
<title>From Sparse To Soft Mixtures of Experts | Kola Ayonrinde</title>
<meta name="generator" content="Jekyll v3.9.3">
<meta property="og:title" content="From Sparse To Soft Mixtures of Experts">
<meta property="og:locale" content="en_US">
<meta name="description" content="Mixture of Expert (MoE) models have recently emerged as an ML architecture offering efficient scaling and practicality in both training and inference [^1].">
<meta property="og:description" content="Mixture of Expert (MoE) models have recently emerged as an ML architecture offering efficient scaling and practicality in both training and inference [^1].">
<link rel="canonical" href="http://localhost:4000/blog/2023/10/20/soft-moe.html">
<meta property="og:url" content="http://localhost:4000/blog/2023/10/20/soft-moe.html">
<meta property="og:site_name" content="Kola Ayonrinde">
<meta property="og:type" content="article">
<meta property="article:published_time" content="2023-10-20T00:00:00-04:00">
<meta name="twitter:card" content="summary">
<meta property="twitter:title" content="From Sparse To Soft Mixtures of Experts">
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2023-10-20T00:00:00-04:00","datePublished":"2023-10-20T00:00:00-04:00","description":"Mixture of Expert (MoE) models have recently emerged as an ML architecture offering efficient scaling and practicality in both training and inference [^1].","headline":"From Sparse To Soft Mixtures of Experts","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/blog/2023/10/20/soft-moe.html"},"url":"http://localhost:4000/blog/2023/10/20/soft-moe.html"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/blog/assets/main.css">
<link type="application/atom+xml" rel="alternate" href="http://localhost:4000/blog/feed.xml" title="Kola Ayonrinde">
</head>
<body>
<header class="site-header" role="banner">

  <div class="wrapper">
<a class="site-title" rel="author" href="/blog/">Kola Ayonrinde</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger">
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewbox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"></path>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/blog/about/">About</a></div>
      </nav>
</div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">
  <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
  <!-- Google tag (gtag.js) -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-Q7175JLSX4"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag() {
      dataLayer.push(arguments);
    }
    gtag("js", new Date());

    gtag("config", "G-Q7175JLSX4");
  </script>
  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">
      From Sparse To Soft Mixtures of Experts
    </h1>
    <p class="post-meta">
      <time class="dt-published" datetime="2023-10-20T00:00:00-04:00" itemprop="datePublished">Oct 20, 2023
      </time></p>
  </header>

  <div class="post-content e-content" itemprop="articleBody">
<p>Mixture of Expert (MoE) models have recently emerged as an ML architecture
offering efficient scaling and practicality in both training and inference <sup id="fnref:1" role="doc-noteref"><a href="#fn:1" class="footnote" rel="footnote">1</a></sup>.</p>

<h3 id="what-are-sparse-moes">What Are Sparse MoEs?</h3>

<p>In traditional <a href="https://arxiv.org/pdf/2101.03961.pdf">Sparse MoEs</a>, we swap out
the <code class="language-plaintext highlighter-rouge">MLP layers</code> of the vanilla transformer for an <code class="language-plaintext highlighter-rouge">Expert Layer</code>. The Expert
Layer is made up of multiple MLPs referred to as Experts. For each input one
expert is selected to send that input to. A dynamic routing mechanism decides
how to map tokens to Experts. Importantly, though this is less mentioned, MoEs
are more modular and hence more naturally interpretable than vanilla
transformers.</p>

<div align="center">
  <figure>
    <img src="/blog/images/softmoe/moe.png" width="800" alt="Sparse MoE">
    <figcaption>Sparse Expert Layer (Switch Transformer) </figcaption>
    </figure>
</div>

<h3 id="introducing-soft-moes">Introducing Soft MoEs</h3>

<p>The Soft MoE paradigm was introduced by Google researchers in the paper
<a href="https://arxiv.org/pdf/2308.00951.pdf">From Sparse To Soft Mixtures of Experts</a>.
Unlike Sparse MoEs, Soft MoEs don’t send a <em>subset</em> of the input tokens to
experts. Instead, each expert receives a <em>linear combination</em> of all the input
tokens. The weights for these combinations are determined by the same dynamic
routing mechanism as in Sparse MoEs.</p>

<div align="center">
  <figure>
    <img src="/blog/images/softmoe/duck.png" width="500" alt="Soft MoE">
    <figcaption>In Soft MoEs each expert processes linear combinations of image patches. </figcaption>
    </figure>
</div>

<p>The discrete routing that makes Sparse MoEs so effective also makes them not
inherently fully differentiable and can cause training issues. The Soft MoE
approach solves these issues, are better suited to GPU hardware and in general
outperform Sparse MoEs.</p>

<p>The paper abstract reads:</p>

<blockquote>
  <p>Sparse mixture of expert architectures (MoEs) scale model capacity without
large increases in training or inference costs. Despite their success, MoEs
suffer from a number of issues: training instability, token dropping,
inability to scale the number of experts, or ineffective finetuning. In this
work, we propose Soft MoE, a fully-differentiable sparse Transformer that
addresses these challenges, while maintaining the benefits of MoEs. Soft MoE
performs an implicit soft assignment by passing different weighted
combinations of all input tokens to each expert. As in other MoE works,
experts in Soft MoE only process a subset of the (combined) tokens, enabling
larger model capacity at lower inference cost. In the context of visual
recognition, Soft MoE greatly outperforms standard Transformers (ViTs) and
popular MoE variants (Tokens Choice and Experts Choice). For example, Soft
MoE-Base/16 requires 10.5× lower inference cost (5.7× lower wall-clock time)
than ViT-Huge/14 while matching its performance after similar training. Soft
MoE also scales well: Soft MoE Huge/14 with 128 experts in 16 MoE layers has
over 40× more parameters than ViT Huge/14, while inference time cost grows by
only 2%, and it performs substantially better.</p>
</blockquote>

<h3 id="links-to-talk-and-slides">Links to Talk and Slides</h3>

<p>I recently gave a talk at <a href="https://www.eleuther.ai">EleutherAI</a>, the open-source
AI research lab, about Soft MoEs.</p>

<p>You can watch the talk back on YouTube
<a href="https://youtu.be/xCKdBC5dh_g?si=uDH8vLVII7l_X8_L">here</a> <sup id="fnref:2" role="doc-noteref"><a href="#fn:2" class="footnote" rel="footnote">2</a></sup> or view the slides
<a href="https://docs.google.com/presentation/d/12Sw4wRQJr3sxcJR91_UM_dlYgYxeAbf9t8es54bAYUM/edit#slide=id.p">here</a>.</p>

<p>I’m very excited about research ideas working on expanding the SoftMoE paradigm
to autoregressive (GPT-style) models, which is currently an open problem
described in the above talk. Feel free to reach out if you’re interested in or
are currently researching in this area. <br> <br></p>

<hr>

<p><br></p>

<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:1" role="doc-endnote">

      <p>For more details on MoE models see the
<a href="https://github.com/koayon/awesome-adaptive-computation">Awesome Adaptive Computation</a>
repo. <a href="#fnref:1" class="reversefootnote" role="doc-backlink">↩</a></p>
    </li>
    <li id="fn:2" role="doc-endnote">

      <p>Unfortunately the video’s audio quality isn’t as great as it could be, I may
look at cleaning this up. <a href="#fnref:2" class="reversefootnote" role="doc-backlink">↩</a></p>
    </li>
  </ol>
</div>
</div>

  <!-- Citations -->
  <!-- Add the citation section here -->
  <div class="citation-section">
    <h3>If you'd like to cite this article, please use:</h3>
    <pre>
  @misc{kayonrinde2023softmoe,
    author = "Kola Ayonrinde",
    title = "From Sparse To Soft Mixtures of Experts",
    year = 2023,
    howpublished = "Blog post",
    url = "http://localhost:4000/2023/10/20/soft-moe.html"
  }
    </pre>
  </div>
<a class="u-url" href="/blog/2023/10/20/soft-moe.html" hidden></a>
</article>

      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/blog/"></data>

  <div class="wrapper">

    <h2 class="footer-heading">Kola Ayonrinde</h2>

    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <li class="p-name">Kola Ayonrinde</li>
<li><a class="u-email" href="mailto:koayon@gmail.com">koayon@gmail.com</a></li>
</ul>
      </div>

      <div class="footer-col footer-col-2">
<ul class="social-media-list"><li><a href="https://github.com/koayon"><svg class="svg-icon"><use xlink:href="/blog/assets/minima-social-icons.svg#github"></use></svg> <span class="username">koayon</span></a></li></ul>
</div>

      <div class="footer-col footer-col-3">
        <p>The technical blog of Kola Ayonrinde: ML Engineer/Data Scientist</p>
      </div>
    </div>

  </div>

</footer>
</body>

</html>
