<!DOCTYPE html>
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
<!-- Begin Jekyll SEO tag v2.8.0 -->
<title>An Analogy for Understanding Mixture of Expert Models | Kola Ayonrinde</title>
<meta name="generator" content="Jekyll v3.9.3">
<meta property="og:title" content="An Analogy for Understanding Mixture of Expert Models">
<meta property="og:locale" content="en_US">
<meta name="description" content="The technical blog of Kola Ayonrinde: ML Engineer/Data Scientist">
<meta property="og:description" content="The technical blog of Kola Ayonrinde: ML Engineer/Data Scientist">
<link rel="canonical" href="http://localhost:4000/blog/2023/10/22/moe-intuition.html">
<meta property="og:url" content="http://localhost:4000/blog/2023/10/22/moe-intuition.html">
<meta property="og:site_name" content="Kola Ayonrinde">
<meta property="og:type" content="article">
<meta property="article:published_time" content="2023-10-22T00:00:00-04:00">
<meta name="twitter:card" content="summary">
<meta property="twitter:title" content="An Analogy for Understanding Mixture of Expert Models">
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2023-10-22T00:00:00-04:00","datePublished":"2023-10-22T00:00:00-04:00","description":"The technical blog of Kola Ayonrinde: ML Engineer/Data Scientist","headline":"An Analogy for Understanding Mixture of Expert Models","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/blog/2023/10/22/moe-intuition.html"},"url":"http://localhost:4000/blog/2023/10/22/moe-intuition.html"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/blog/assets/main.css">
<link type="application/atom+xml" rel="alternate" href="http://localhost:4000/blog/feed.xml" title="Kola Ayonrinde">
</head>
<body>
<header class="site-header" role="banner">

  <div class="wrapper">
<a class="site-title" rel="author" href="/blog/">Kola Ayonrinde</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger">
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewbox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"></path>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/blog/about/">About</a></div>
      </nav>
</div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">
  <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
  <!-- Google tag (gtag.js) -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-Q7175JLSX4"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag() {
      dataLayer.push(arguments);
    }
    gtag("js", new Date());

    gtag("config", "G-Q7175JLSX4");
  </script>
  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">
      An Analogy for Understanding Mixture of Expert Models
    </h1>
    <p class="post-meta">
      <time class="dt-published" datetime="2023-10-22T00:00:00-04:00" itemprop="datePublished">Oct 22, 2023
      </time></p>
  </header>

  <div class="post-content e-content" itemprop="articleBody">
<!-- An Intuitive Way To Understand Mixture Of Expert Models (Sparse MoEs) -->

<h3 id="tldr-experts-are-doctors-routers-are-gps">TL;DR: Experts are Doctors, Routers are GPs</h3>

<h2 id="motivation">Motivation</h2>

<p>Ideally, we would like foundation models to solve multiple types of tasks. Ins
the days of yore, we would build a supervised model for every individual use
case; foundation models promise a single model which can solve a wide variety of
general tasks.</p>

<p>There are challenges with this however. When two tasks require different skills,
trying to learn both can cause negative interference and make you less good at
both of them. And similarly storing all the information for all the different
tasks might be difficult even for large models.</p>

<p>Moreover we might wonder if it make senses to have the same parameters used to
compute the answer to a logic puzzle also be used to find the perfect adjective
to describe the love interest in a romance fanfic.</p>

<p>Ideally we would like to split up our model to have different independent
functions which we can select when they are relevant (and possibly compose
together different abilities).</p>

<p>Sparse Mixture of Expert models allow their internal “Experts” to specialise in
certain domains rather than having to be all things to all tokens <sup id="fnref:0" role="doc-noteref"><a href="#fn:0" class="footnote" rel="footnote">1</a></sup> <sup id="fnref:m" role="doc-noteref"><a href="#fn:m" class="footnote" rel="footnote">2</a></sup>.</p>

<h2 id="moes-for-scale">MoEs For Scale</h2>

<p>Another motivation for Mixture of Expert models is that are large benefits to
scaling up models. We can scale the number of model parameters, the amount of
data and the amount of compute applied at train time. With regular transformers,
in order to scale up the number of parameters, you must likewise scale the
amount of compute applied.</p>

<p>Intuitively the number of parameters represent the amount of <code class="language-plaintext highlighter-rouge">knowledge</code> a model
has and compute applied represents the <code class="language-plaintext highlighter-rouge">intelligence</code><sup id="fnref:1" role="doc-noteref"><a href="#fn:1" class="footnote" rel="footnote">3</a></sup>.</p>

<p>There are some use cases where having more knowledge can be traded off with
being more cognitively able. For example you may choose to memorise rather than
re-derive the laws of physics in order to use them in a specific problem.
Similarly we can trade off the opposite way as well - if you know you have
access to a textbook or Wikipedia then you might not want to memorise certain
historical facts, all you need to know is when and how to look up the facts you
need.</p>

<p>So we would like to be able to scale the parameters and compute separately
depending on whether our use case requires more knowledge or more cognitive
ability<sup id="fnref:2" role="doc-noteref"><a href="#fn:2" class="footnote" rel="footnote">4</a></sup>.</p>

<p>One approach to decoupling the two of these is the Mixture of Experts (MoE)
paradigm. With MoEs, we are able to increase the number of parameters of models
without increasing how much training compute or inference time latency.</p>

<h2 id="sparse-mixture-of-experts-models">Sparse Mixture of Experts Models</h2>

<p>In a vanilla transformer, each Transformer Block contains an attention layer for
<code class="language-plaintext highlighter-rouge">communication</code> between tokens and an MLP layer for <code class="language-plaintext highlighter-rouge">computation</code> within
tokens. The MLP layer contains most of the parameters of a large transformer and
transforms the individual tokens.</p>

<p>In <a href="https://arxiv.org/pdf/2101.03961.pdf">Sparse MoEs</a>, we swap out the
<code class="language-plaintext highlighter-rouge">MLP layers</code> of the vanilla transformer for an <code class="language-plaintext highlighter-rouge">Expert Layer</code>. The Expert Layer
is made up of multiple MLPs referred to as Experts. For each input one expert is
selected to send that input to. In this way, each token it has different
parameters applied to it. A dynamic routing mechanism decides how to map tokens
to Experts<sup id="fnref:3" role="doc-noteref"><a href="#fn:3" class="footnote" rel="footnote">5</a></sup>.</p>

<div align="center">
  <figure>
    <img src="/blog/images/softmoe/moe.png" width="800" alt="Sparse MoE">
    <figcaption>Sparse Expert Layer (Switch Transformer) </figcaption>
    </figure>
</div>

<h2 id="the-analogy">The Analogy</h2>

<p>Imagine you’re not feeling well and you have no idea what kind of problem it is.
Since your friend is a cardiologist (doctor specialising in the heart) you ask
them for advice. They might give you some advice for treatments. The question
that you then ask yourself is if you should follow their advice. If you weren’t
too sure there are a few things you could try:</p>

<ol>
  <li>Get a second opinion from another cardiologist.</li>
</ol>

<div align="center">
  <figure>
    <img src="/blog/images/analogy-moe/two_cardiologists.png" width="600" alt="Two Cardiologists">
    <!-- <figcaption>Sparse Expert Layer (Switch Transformer) </figcaption> -->
    </figure>
</div>

<blockquote>
  <p>Averaging over multiple doctors who were all trained in the same way increases
robustness by reducing variance (maybe the first doctor was tired that day or
something). It doesn’t help bias (in the technical statistical sense) - all
the cardiologists are likely to be wrong in the same way, if they are wrong.</p>
</blockquote>

<ol>
  <li>Go to a generalist doctor that has no specialism.</li>
</ol>

<div align="center">
  <figure>
    <img src="/blog/images/analogy-moe/no_specialist.png" width="600" alt="One cardiologist and one doctor with no specialism">
    <!-- <figcaption>Sparse Expert Layer (Switch Transformer) </figcaption> -->
    </figure>
</div>

<blockquote>
  <p>It’s not clear whether this is better than asking another cardiologist. Sure
they might have different knowledge to the cardiologist which might be useful
if your problem isn’t about the heart. But there’s an awful lot of medical
knowledge out there and we can’t reasonably expect this one generalist to know
everything about all of them. They probably have cursory knowledge at best. We
need someone who specialises in the area that we’re struggling with. Problem
is we don’t know which area of specialism we need!</p>
</blockquote>

<ol>
  <li>Ask multiple doctors who all specialise in different areas and do the thing
most of them suggest.</li>
</ol>

<div align="center">
  <figure>
    <img src="/blog/images/analogy-moe/all_doctors.png" width="600" alt="Multiple Doctors with Different Specialisms">
    <!-- <figcaption>Sparse Expert Layer (Switch Transformer) </figcaption> -->
    </figure>
</div>

<blockquote>
  <p>This is much better. If you have a problem with your eyes, you know that the
eye doctor is being consulted so you have a much better chance of getting the
right treatment. But there are downsides here. Most notably, asking multiple
doctors is probably pretty inefficient. Now we have to see 50 specialists for
every problem even though most of them have no idea about our problem. What we
would prefer is to know which one specialist (or possibly couple of
specialists) we should see and only get advice from them.</p>
</blockquote>

<ol>
  <li>Go to your GP, tell them about your ailment and ask them which doctor you
should go and see.</li>
</ol>

<div align="center">
  <figure>
    <img src="/blog/images/analogy-moe/gp.png" width="600" alt="GP-Doctor System">
    <!-- <figcaption>Sparse Expert Layer (Switch Transformer) </figcaption> -->
    </figure>
</div>

<blockquote>
  <p>This is of course what we do in real life and it’s better because we get the
benefits of getting advice from the most relevant specialised doctor</p>
</blockquote>

<p>In approach 4, the GP is the routing function. They know the strengths of the
different doctors and send you to one of them depending on your problem.</p>

<p>The Doctors are the Experts. We allow them to specialise knowing that the GP can
route us to the correct doctor for our problem.</p>

<p><strong>The GP-doctor system is exactly a Mixture of Experts layer.</strong></p>

<h3 id="what-are-moes-good-for">What Are MoEs Good For?</h3>

<p>Viewed this way we see that Mixture of Expert models will be effective whenever
we want a model to large amounts of information - more than a single Expert
could hope to learn alone. Another use case is when our task can be decomposed
into one of a number of tasks.</p>

<p>In general we might imagine Mixture of Expert models which when faced with more
difficult problems can send the input to a more powerful expert which has access
to more resources. This starts to move us towards increasingly
<a href="https://github.com/koayon/awesome-adaptive-computation">Adaptive Computation</a>.</p>

<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:0" role="doc-endnote">

      <p>In actuality Expert might not necessarily specialise strictly by task. It
might be beneficial for an expert to specialise in syntactic rather than
semantic features or to combine two tasks which are different enough to not
inference with each other. <a href="#fnref:0" class="reversefootnote" role="doc-backlink">↩</a></p>
    </li>
    <li id="fn:m" role="doc-endnote">

      <p>This approach also has good biological precedent. Humans don’t use every
part of their brain for every stimulus they receive - when they receive, for
example a visual stimuli, they use only their visual cortex to process it. <a href="#fnref:m" class="reversefootnote" role="doc-backlink">↩</a></p>
    </li>
    <li id="fn:1" role="doc-endnote">

      <p>For some vague definitions of “intelligence” and “knowledge”. This intuition
is courtesy of
<a href="https://scholar.google.com/citations?user=wsGvgA8AAAAJ&hl=en">Noam Shazeer</a>. <a href="#fnref:1" class="reversefootnote" role="doc-backlink">↩</a></p>
    </li>
    <li id="fn:2" role="doc-endnote">

      <p>In reality both knowledge and cognitive ability are hard to separate this
cleanly but hopefully the intuition still remains useful. <a href="#fnref:2" class="reversefootnote" role="doc-backlink">↩</a></p>
    </li>
    <li id="fn:3" role="doc-endnote">

      <p>The experts “compete” to process the tokens and as in Natural Selection and
Economics, competition for niches makes them specialise. <a href="#fnref:3" class="reversefootnote" role="doc-backlink">↩</a></p>
    </li>
  </ol>
</div>
</div>
<a class="u-url" href="/blog/2023/10/22/moe-intuition.html" hidden></a>
</article>

      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/blog/"></data>

  <div class="wrapper">

    <h2 class="footer-heading">Kola Ayonrinde</h2>

    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <li class="p-name">Kola Ayonrinde</li>
<li><a class="u-email" href="mailto:koayon@gmail.com">koayon@gmail.com</a></li>
</ul>
      </div>

      <div class="footer-col footer-col-2">
<ul class="social-media-list"><li><a href="https://github.com/koayon"><svg class="svg-icon"><use xlink:href="/blog/assets/minima-social-icons.svg#github"></use></svg> <span class="username">koayon</span></a></li></ul>
</div>

      <div class="footer-col footer-col-3">
        <p>The technical blog of Kola Ayonrinde: ML Engineer/Data Scientist</p>
      </div>
    </div>

  </div>

</footer>
</body>

</html>
