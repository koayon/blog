---
layout: post
title: "The Impact of Mixtral"
stub: mixtral
tags: machine-learning mixture-of-experts adaptive-computation
toc: true
---

### Can You Feel The MoE?

<div align="center">
  <figure>
    <img src="/blog/images/mixtral/mistral_logo.png" width="800" alt="Mistral Logo">
    <figcaption>Love a little WordArt throwback</figcaption>
    </figure>
</div>

<br>

Since the infamous
[BitTorrent link launch](https://x.com/MistralAI/status/1706877320844509405?s=20)
of Mixtral, Mistral's Mixture of Expert (MoE) model, there's been renewed
attention[^1] paid to MoE models.

[^1]: If I may

This week, Mistral released the [paper](https://arxiv.org/pdf/2401.04088.pdf)
accompanying the [model](https://huggingface.co/mistralai/Mixtral-8x7B-v0.1).
This feels like a great time to dig into the details of the Mixtral model and
the impact that it‚Äôs having on the MoE and LLM communities so far.

## Mixtral and the MoE paradigm

We discussed the intuition behind MoE models in
[An Analogy for Understanding Mixture of Expert Models](https://www.kolaayonrinde.com/blog/2023/10/22/moe-analogy.html):

> In [Sparse Mixture of Experts](https://arxiv.org/pdf/2101.03961.pdf) (MoEs),
> we swap out the `MLP layers` of the vanilla transformer for an `Expert Layer`.
> The Expert Layer is made up of multiple MLPs called ‚ÄúExperts‚Äù. For each input
> we select one expert to send that input to. In this way, each token has
> different parameters applied to it. A dynamic routing mechanism decides how to
> map tokens to Experts.

<div align="center">
  <figure>
    <img src="/blog/images/mixtral/switch_transformers.png" width="800" alt="Switch Transformer">
    <figcaption>Sparse Expert Layer
<a href = "https://arxiv.org/pdf/2101.03961.pdf"> Switch Transformer</a> </figcaption>
    </figure>
</div>

<br>

This approach gives models more parameters [^2] without requiring more compute
or latency for each forward pass. MoE models also typically have better sample
efficiency - that is, their performance improves much faster than dense
transformers in training, when given the same amount of compute. This isn‚Äôt
quite a free lunch because it requires more memory to store the model for
inference, but, if you have enough memory, it‚Äôs pretty great.

[^2]: which is more knowledge in some sense

Mixtral 8x7B has the backbone of Mistral-7B (their previous model). As with
Mistral-7B, Mixtral uses Group Query Attention and Sliding Window Attention. The
main changes are a 32k context window out of the box and replacing the Feed
Forward Networks (FFNs) with Mixture of Expert (MoE) layers.

Mixtral opts for an MoE layer with `8 FFN experts` which are sparsely activated
by choosing the `top 2` at each layer. [^3]

[^3]:
    This is a slight reversal of recent work as many papers had followed the
    Switch Transformer in only choosing the top 1 expert per layer. We expect
    that choosing 2 experts allows for more expressivity, more stable training
    and better gradient flow which is traded off against increased parallel
    computation in each forward pass.

Having `8 experts` means that where the original Mistral had a single FFN per
transformer block, Mixtral has 8 separate FFNs. [^4]

[^4]:
    Early MoEs like the Switch Transformer were using 100s of experts per layer.
    This always seemed a little excessive and working on these models, a good
    heuristic for choosing the expert number hyperparameter is either the number
    of experts that will fit into your single GPU memory if you‚Äôre mostly doing
    single batch inference or the number of GPUs that you could do expert
    parallelism on at inference time if you‚Äôre running a high bandwidth API.
    With this in mind 8 experts seems like a nice middle ground right now for
    users of an open-source product.

Rather than each token rather than being processed by all the parameters, a
routing network dynamically selects the `top 2` experts for each token depending
on the content of the token itself. Hence, though the total parameter count is
47B, the ‚Äúactive‚Äù parameter count (i.e. the number of parameters used for each
forward pass) comes in at 13B. [^params]

[^params]:
    This is slightly less than 8x7 =56 total parameters as because attention and
    embedding parameters are _not_ duplicated).

Succinctly an MoE layer is given as:

$$
\displaystyle \sum_{i=0}^{n-1}G(x)_i \cdot E_i(x),
$$

where G is a gating function which is 0 everywhere except at 2 indices and where
each $E_i$ is a single expert FFN. Note that in the above formula since most of
the entries of the sum are zeros (as G(x) is zero for most i), we only have to
compute some of the $E_i$s rather than all of them. This is where MoEs have
computational efficiency advantages over using bigger models or using an
ensemble of models.

There is an MoE layer in each of the transformer blocks (32 in this case) and
hence we do this routing procedure 32 times for each forward pass. In a
traditional ensemble model, N (8 in this case) models have their predictions
averaged, so there are 8 token paths. We can compare the number of possible
paths that each token could take in an MoE to these ensemble methods:

> At each layer we choose 2 of the 8 experts to process our token. There are
> $\binom{8}{2}$ = 28 ways to do this. And this happens at each of the 32 layers
> giving $28^32$ possible paths overall, which is huge[^5]! ü§Ø The variety of
> possible paths here points towards increasingly Adaptive Computation in
> models. In Adaptive Computation, we consider models which handle different
> tokens with different parameters and different amounts of compute.

[^5]:
    In fact this is quite the understatement, all of these paths are weighted
    according to the router logits so there's even more nuance than this in the
    possible paths that tokens can take.

<div align="center">
  <figure>
    <img src="/blog/images/mixtral/moe_layers_sketch.png" width="800" alt="MoE layers">
    <figcaption>A diagram showing an example path that a token could take for the first two layers</figcaption>
    </figure>
</div>

<br>

Up until now there have been a two barriers to truly performant and stably
trainable MoEs:

#### Problem 1: Training MoEs properly from a mathematical perspective

MoE models have an inherently discrete step, the hard routing, and this
typically harms the gradient flow. Typically we want fully differentiable
functions for backprop and MoEs aren‚Äôt even continuous! Considering
mathematically plausible approximations to the true gradient can hugely improve
MoE training. Recent approaches like
[Sparse Backpropagation](https://arxiv.org/pdf/2310.00811.pdf) and
[Soft MoE for encoders](https://arxiv.org/abs/2308.00951) provide better
gradient flow and hence more performant models.

#### Problem 2: Training MoEs efficiently

Compared to their FLOP-class, MoEs are larger models. Their size means that
there are real benefits to effective parallelisation and minimising
communication costs. Many frameworks such as
[DeepSpeed MoE](https://arxiv.org/pdf/2201.05596.pdf) now support MoE training
in a fairly hardware efficient way. <br>

Having overcome both of these issues, we're now ready to use MoEs more in
practise.

<br>
<br>

## Notes on Mixtral‚Äôs paper

### Evals

The Mixtral base model outperforms popular (and larger) models like Llama 2 70B,
Gemini Pro and GPT-3.5 on most benchmarks. Note that these models are not only
larger in total parameter count but are also larger in active parameter count
too!

At the time of writing, Mixtral is the best open-source model and the 3rd best
Chat model, only beaten by GPT-4 and Claude 2.0 variants.

<div align="center">
  <figure>
    <img src="/blog/images/mixtral/llm_leaderboard.png" width="800" alt="LLM Leaderboard">
    <figcaption></figcaption>
    </figure>
</div>

### Context Window

Mixtral shows impressive use of its whole 32k context window. The model has
relatively good recall even for mid-context tokens.

<div align="center">
  <figure>
    <img src="/blog/images/mixtral/context_window.png" width="800" alt="Graph of Long Context Performance">
    <!-- <figcaption>Mixtral maintains good performance across its context window and seems to effectively use all of the context</figcaption> -->
    </figure>
</div>

### Instruction Fine-Tuning

Along with the base model, Mistral also released Instruction Fine-Tuned Chat and
Assistant models. For alignment, they opted for Direct Preference Optimisation
(DPO) which is proving to be a powerful and less finicky alternative to the
traditional RLHF. [^lambert]

[^lambert]:
    Nathan Lambert has a great explainer on DPO
    [here](https://www.interconnects.ai/p/the-dpo-debate)

### Interpretability

One hypothesis about MoEs is that some experts might specialise in a particular
domain (e.g. mathematics, biology, code, poetry etc.). This hypothesis is an old
one which has consistently been shown to be mistaken in the literature. Here,
the authors confirm, as in previous MoE papers, there is little difference in
the distribution of experts used for different domains (although they report
being surprised by this finding!). Often experts seem to specialise
syntactically (e.g. an expert for punctuation or whitespace), rather than
semantically (an expert for neuroscience). [^specialisation]

[^specialisation]:
    It may also be the case that their natural semantic specialisation is not
    very clear to human researchers

Although the distribution of experts is fairly uniform overall, interestingly
two adjacent tokens are much more likely to be processed by the same expert,
than we might naively predict. In other words, once an expert sees one token,
it‚Äôs quite likely to also see the next one - experts like to alley-oop
themselves! üèÄ

This recent [paper](https://arxiv.org/pdf/2312.17238.pdf) details ways to
exploit this alley-oop property by caching the recently used expert weights in
fast memory.

_As I've noted
[previously](https://www.kolaayonrinde.com/blog/2023/11/03/dictionary-learning.html#whatsnext:~:text=%F0%9F%94%B3-,Modularity,-As%20mentioned%20above),
I‚Äôm excited about the explicit modularity in MoE models for increased
interpretability_.

### Notable omissions

There's little information in the paper about expert balancing techniques. Many
different auxiliary losses have been proposed for expert balancing and it would
be cool to see which loss function Mistral found to work well at this scale.

The authors are also quite hush about the pretrain, instruction or feedback
datasets used to train the model. Given the impressive performance, it‚Äôs quite
likely that there‚Äôs some secret sauce in the dataset compilation and filtering.
It seems increasingly likely that data will be a moat for Foundation Model
providers [^moat].

[^moat]:
    at least for companies that don‚Äôt produce applications built on top of the
    models.

<br>

## Mixtral in the wild

### Impact for On Device LLMs

MoEs win by having increased performance with faster inference. Founder Sharif
Shameem [writes](https://x.com/sharifshameem/status/1734470299314459108?s=20),
‚ÄúThe Mixtral MoE model genuinely feels like an inflection point ‚Äî a true GPT-3.5
level model that can run at 30 tokens/sec on an M1 MacBook Pro. Imagine all the
products now possible when inference is 100% free and your data stays on your
device!‚Äù

Indeed since the launch of Mixtral, it‚Äôs been used in many applications from
_the enterprise_ to _local chatbots_ to _DIY Home Assistants √† la Siri_.

---

<br>

As many people use MoE models on-device for the first time, I expect that we
will start to see more methods which speed up MoE inference. The
[Fast MoE Inference paper](https://arxiv.org/pdf/2312.17238.pdf) and MoE
specific quantisation like [QMoE](https://arxiv.org/pdf/2310.16795.pdf) are all
great steps in this direction.

In particular, Quantization can be thought of as storing a model compressed like
we do for audio in MP3s. We degrade the quality model slightly and get massive
decreases in the memory that it requires. We can typically quantise MoEs even
more aggressively than dense models and retain strong performance.

### Impact for Foundation Model Companies

Mistral was only started a matter of months ago with a super lean team and is
already SOTA for Open Source models. This is impressive from their team but it
may also suggest that Foundation Models are being commodified real quick.

Originally Mistral were offering Mixtral behind their API for \\$1.96 per
million tokens. Considering GPT-4 is \$10-30 at the time of writing, this seemed
fair for a hosted API. Within days different inference providers undercut
Mistral
[significantly](https://twitter.com/JosephJacks_/status/1735756308496667101):

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Last week <a href="https://twitter.com/MistralAI?ref_src=twsrc%5Etfw">@MistralAI</a> launched pricing for the Mixtral MoE: $2.00~ / 1M tokens.<br><br>Hours later <a href="https://twitter.com/togethercompute?ref_src=twsrc%5Etfw">@togethercompute</a> took the weights and dropped pricing by 70% to $0.60 / 1M.<br><br>Days later <a href="https://twitter.com/abacusai?ref_src=twsrc%5Etfw">@abacusai</a> cut 50% deeper to $0.30 / 1M.<br><br>Yesterday <a href="https://twitter.com/DeepInfra?ref_src=twsrc%5Etfw">@DeepInfra</a> went to $0.27 / 1M.<br><br>Who‚Äôs next ??? üìâ</p>&mdash; JJ ‚Äî oss/acc (@JosephJacks_) <a href="https://twitter.com/JosephJacks_/status/1735756308496667101?ref_src=twsrc%5Etfw">December 15, 2023</a></blockquote> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

There was even one provider who was giving away tokens _for free_. I know a race
to the bottom when I see one‚Ä¶

The consumer/developer is truly winning here but it reiterates the point that
Foundation Model companies should expect the value of tokens to fall
_dramatically_.
[Competition is for Losers](https://www.amazon.co.uk/Zero-One-Notes-Start-Future/dp/0753555204),
as Peter Thiel might say; it‚Äôs very possible to compete away all the profits to
zero. [^altman]. It increasingly looks like most of the value captured from an
LLM business perspective will likely be in the application layer (e.g.
Perplexity, Copilot) and the infrastructure layer (e.g. AWS/Azure).

[^altman]:
    Sam Altman has colourfully referred to this as the marginal cost of
    intelligence going to zero

### Impact for the Scientific Community

Mixtral is a huge win for the scientific and interpretability communities. We
now finally have a model which is comfortably better than GPT3.5 and whose
weights are freely available to researchers.

In addition, given Mixtral shares the same backbone as the previous Mistral 7B,
it seems plausible some weights were re-used as initialisations for Mixtral.
This approach is known in the literature as
[Sparse Upcycling](https://arxiv.org/pdf/2212.05055.pdf). If Sparse Upcycling
works, this suggests that the compute required to make great MoE models might be
much less than previous thought. Researchers can take advantage of existing
models like Llama 2 etc. rather than having to pretrain entirely from scratch,
which completely changes which projects are feasible for academics and the
GPU-poor.

#### Open Source ML in the Age of Adaptive Computation

‚ÄúIn 2012 we were detecting cats and dogs and in 2022 we were writing human-like
pottery, generating beautiful and novel imagery, solving the protein folding
problem and writing code. Why is that?‚Äù

Arthur Mensch, Mistral co-founder, suggests most of the reason is ‚Äúthe free flow
of information. You had academic labs [and] very big industry labs communicating
all the time about their results and building on top of others‚Äô results. That‚Äôs
the way we [significantly improved] the architecture and training techniques. We
made everything work as a community‚Äù.

We‚Äôre not at the end of the ML story just yet. There‚Äôs still science to be done
and inventions to be discovered so we still need the free flow of information.

> In this house we love Open Source models and papers. ü§ó

Expect MoEs to become even more important for 2024. The age of
[Adaptive Computation](https://github.com/koayon/awesome-adaptive-computation)
is here.

<br>
<br>
