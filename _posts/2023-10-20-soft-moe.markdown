---
layout: post
title: "From Sparse To Soft Mixtures of Experts"
# date: 2023-07-14 20:29:35 +0100
tags: machine-learning training mixture-of-experts adaptive-computation
---

<!-- # From Sparse to Soft Mixtures of Experts -->

An existing development in Mixture of Expert (MoE) models has been Google
Research's Soft MoE.

In traditional (Sparse) MoEs, we swap out the MLP layers of the transformer for
an Expert Layer containing multiple MLPs. We also have a dynamic routing
mechanism to decide which Expert to send each input to.

The Soft MoE paradigm differs by instead sending each expert a linear
combination of the tokens decided by the routing mechanism.

<div align="center">
  <figure>
    <img src="/blog/images/softmoe/duck.png" width="500" alt="Layers">
    <figcaption>Soft MoE</figcaption>
    </figure>
</div>

The Soft MoE approach solves lots of issues with instability and
discontinuities. Soft MoEs are also better suited to GPU hardware and in general
outperform Sparse MoEs where applicable.

The paper abstract reads:

> Sparse mixture of expert architectures (MoEs) scale model capacity without
> large increases in training or inference costs. Despite their success, MoEs
> suffer from a number of issues: training instability, token dropping,
> inability to scale the number of experts, or ineffective finetuning. In this
> work, we propose Soft MoE, a fully-differentiable sparse Transformer that
> addresses these challenges, while maintaining the benefits of MoEs. Soft MoE
> performs an implicit soft assignment by passing different weighted
> combinations of all input tokens to each expert. As in other MoE works,
> experts in Soft MoE only process a subset of the (combined) tokens, enabling
> larger model capacity at lower inference cost. In the context of visual
> recognition, Soft MoE greatly outperforms standard Transformers (ViTs) and
> popular MoE variants (Tokens Choice and Experts Choice). For example, Soft
> MoE-Base/16 requires 10.5× lower inference cost (5.7× lower wall-clock time)
> than ViT-Huge/14 while matching its performance after similar training. Soft
> MoE also scales well: Soft MoE Huge/14 with 128 experts in 16 MoE layers has
> over 40× more parameters than ViT Huge/14, while inference time cost grows by
> only 2%, and it performs substantially better.

I recently gave a talk at EleutherAI, the open-source AI research lab, about
Soft MoEs.

You can watch the talk back on YouTube
[here](https://youtu.be/xCKdBC5dh_g?si=uDH8vLVII7l_X8_L)[^1] or see the slides
[here](https://docs.google.com/presentation/d/12Sw4wRQJr3sxcJR91_UM_dlYgYxeAbf9t8es54bAYUM/edit#slide=id.p).

I'm very excited about people working on expanding the SoftMoE approach to the
autoregressive regime. Feel free to reach out if you're interested in or are
currently researching in this area.

[^1] - Unfortunately on the video the audio quality isn't as great as it could
be, I may look at cleaning this up.
